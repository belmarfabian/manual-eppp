# Enfoques y metodologías de evaluación

## Tipología de evaluaciones según momento

Las evaluaciones se clasifican primariamente según el momento del ciclo de política en que se conducen. Las **evaluaciones ex-ante** se realizan antes de la implementación, analizando el diseño propuesto de la intervención. Su propósito es valorar si el problema está correctamente diagnosticado, si la teoría de cambio es plausible, si los objetivos son realistas y si los recursos previstos resultan apropiados. Como señala DIPRES (2015), evaluaciones ex-ante rigurosas previenen inversiones en intervenciones mal concebidas cuyo fracaso sería predecible desde el diseño.

Las **evaluaciones ex-post** ocurren durante o después de la implementación, examinando resultados efectivos y aprendizajes operacionales. Se subdividen según foco temporal: evaluaciones de procesos durante la implementación, evaluaciones de resultados al cierre o en momentos intermedios, y evaluaciones de impacto que requieren suficiente tiempo transcurrido para que efectos sustantivos se materialicen.

La distinción ex-ante/ex-post no es puramente temporal sino sustantiva. Evaluaciones ex-ante trabajan con modelos y proyecciones, juzgando plausibilidad teórica y coherencia lógica. Evaluaciones ex-post trabajan con evidencia empírica de implementación efectiva, juzgando desempeño observado contra objetivos, benchmarks o contrafactuales. Metodológicamente, las primeras privilegian análisis lógico y prospectivo; las segundas, contrastación empírica retrospectiva.

## Evaluación por objeto de análisis

Una segunda tipología clasifica evaluaciones según su objeto de análisis específico. La **evaluación de diseño** examina la arquitectura conceptual de la intervención: coherencia entre diagnóstico del problema, objetivos, teoría de cambio, población objetivo, componentes y recursos. Utiliza herramientas como el marco lógico y el árbol de problemas para identificar inconsistencias o debilidades que comprometerían la efectividad incluso con implementación impecable.

La **evaluación de procesos** analiza la calidad, eficiencia y fidelidad de la implementación. Interroga si las actividades se ejecutan según lo planificado, si los recursos se utilizan eficientemente, si existen cuellos de botella operacionales y si los beneficiarios efectivamente reciben los bienes o servicios contemplados. Como documenta Pignatta (2015), muchos programas con diseños sólidos fracasan por deficiencias de implementación que evaluaciones de procesos habrían detectado tempranamente.

La **evaluación de resultados** examina el logro de productos y efectos inmediatos en beneficiarios. ¿Se entregaron los bienes o servicios comprometidos? ¿Los beneficiarios modificaron comportamientos, adquirieron competencias o mejoraron condiciones según lo esperado? Estas evaluaciones típicamente comparan indicadores con líneas base o metas, sin necesariamente establecer atribución causal rigurosa.

La **evaluación de impacto** constituye el estándar más demandante, buscando determinar efectos causalmente atribuibles a la intervención mediante diseños que controlan por factores confundentes. Requiere contrafactuales creíbles —qué habría ocurrido a los beneficiarios sin el programa— típicamente mediante grupos de control o comparación, diseños experimentales, cuasi-experimentales o métodos econométricos sofisticados.

## Enfoques metodológicos principales

El campo evaluativo ha generado múltiples enfoques metodológicos con supuestos, fortalezas y limitaciones distintivas. El **enfoque experimental** constituye el estándar de oro para estimación de impacto causal. Asigna aleatoriamente individuos o unidades a tratamiento y control, garantizando que diferencias observadas ex-post sean atribuibles al programa. Sin embargo, requiere condiciones raramente satisfechas en contextos reales: factibilidad ética y política de aleatorización, escalas suficientes para potencia estadística, capacidad de mantener la asignación aleatoria durante implementación.

Los **enfoques cuasi-experimentales** aproximan inferencia causal sin aleatorización. Técnicas como diferencias en diferencias, regresión discontinua, variables instrumentales o matching estadístico explotan variación natural en exposición al programa para construir contrafactuales plausibles. Si bien menos robustos que experimentos, resultan frecuentemente más viables y, bajo supuestos apropiados, producen estimaciones creíbles de efectos causales.

El **enfoque de teoría del cambio** privilegia la explicitación y contrastación de los mecanismos causales postulados entre actividades e impactos. En lugar de reducir la evaluación a estimación de efectos netos, mapea la cadena causal completa, interrogando cada eslabón. ¿Las actividades generaron los productos esperados? ¿Los productos indujeron cambios intermedios anticipados? ¿Esos cambios contribuyeron a impactos finales? Este enfoque resulta especialmente valioso para intervenciones complejas con múltiples vías causales o donde la atribución causal rigurosa es impracticable.

Los **enfoques de estudios de caso** examinan intensivamente un número reducido de instancias de implementación. Sacrifican generalización estadística por comprensión profunda de procesos, contextos y mecanismos. Mediante combinación de fuentes documentales, entrevistas y observación, reconstruyen trayectorias de implementación identificando facilitadores, obstáculos y factores contextuales críticos. Como argumenta Nagel (2002), casos bien seleccionados generan aprendizajes teóricos y prácticos que estudios de muestra grande frecuentemente pierden.

Los **enfoques participativos** involucran activamente a beneficiarios y otros stakeholders en el diseño, conducción e interpretación de la evaluación. Parten de reconocer que distintos actores poseen conocimientos parciales pero complementarios sobre la intervención. Metodologías participativas no solo enriquecen la comprensión evaluativa sino que, al generar apropiación de hallazgos, incrementan la probabilidad de que recomendaciones se implementen efectivamente.

## Criterios de evaluación OECD-DAC

El Comité de Ayuda al Desarrollo de la OCDE ha consolidado seis criterios como marco de referencia para evaluación de políticas y programas. **Pertinencia** interroga si los objetivos de la intervención corresponden a las necesidades, prioridades y políticas de beneficiarios y contexto. Una intervención puede implementarse exitosamente y lograr objetivos, pero ser irrelevante si esos objetivos no responden a necesidades reales o prioridades legítimas de la población objetivo.

**Coherencia** examina la compatibilidad de la intervención con otras intervenciones en el mismo contexto. ¿Complementa, duplica o contradice otros programas? ¿Se alinea con políticas y estrategias sectoriales? Falta de coherencia genera desperdicio de recursos, confusión para beneficiarios e incluso efectos contraproducentes cuando intervenciones trabajan en direcciones opuestas.

**Eficacia** mide el grado de logro de los objetivos, considerando su importancia relativa. No se reduce a cumplimiento de metas, sino que pondera diferencialmente el logro de distintos objetivos según su relevancia para resolver el problema central. Un programa que cumple 100% metas secundarias pero fracasa en objetivos primarios no es eficaz, incluso si formalmente "cumple" porcentajes agregados de metas.

**Eficiencia** relaciona resultados con recursos empleados. ¿Se lograron resultados óptimos con recursos dados, o resultados dados con recursos mínimos? Evaluación de eficiencia requiere benchmarking: comparación con intervenciones similares, estándares técnicos o análisis de alternativas. Un programa puede ser eficaz pero ineficiente si resultados equivalentes podrían lograrse con menores costos.

**Impacto** refiere a efectos de largo plazo, amplios y potencialmente indirectos atribuibles a la intervención. Trasciende resultados inmediatos para examinar transformaciones sostenidas en condiciones de vida, capacidades, instituciones o prácticas. Evaluación rigurosa de impacto demanda horizontes temporales extendidos y metodologías que establezcan atribución causal, distinguiendo efectos del programa de tendencias contextuales.

**Sostenibilidad** pregunta si beneficios perdurarán tras finalizar la intervención. ¿Se fortalecieron capacidades locales? ¿Se modificaron instituciones de modo perdurable? ¿Existen recursos y compromisos para continuar actividades clave? Muchos programas generan beneficios durante implementación que se evaporan rápidamente al cesar el apoyo externo, revelando fragilidad de los cambios inducidos.

## Métodos cuantitativos y cualitativos

La evaluación puede emplear métodos cuantitativos, cualitativos o mixtos según preguntas evaluativas y disponibilidad de datos. Métodos cuantitativos operan con datos numéricos, privilegian muestras grandes, análisis estadístico y generalización. Incluyen encuestas, análisis de registros administrativos, experimentos, y técnicas econométricas diversas. Su fortaleza radica en medición precisa, identificación de patrones sistemáticos y capacidad de establecer magnitud de efectos. Sus limitaciones incluyen reduccionismo de fenómenos complejos a variables medibles, dificultad para capturar procesos y mecanismos, y dependencia de disponibilidad de datos estructurados.

Métodos cualitativos trabajan con datos textuales o visuales, privilegian profundidad sobre amplitud, comprensión contextualizada sobre generalización estadística. Incluyen entrevistas semi-estructuradas, grupos focales, observación participante, análisis documental y etnografía. Su fortaleza está en capturar perspectivas de actores, comprender procesos y contextos, identificar mecanismos causales y generar hallazgos inesperados que métodos estructurados habrían omitido. Sus limitaciones incluyen potencial falta de representatividad, dificultad de generalización, riesgos de sesgo interpretativo del evaluador y intensividad de recursos.

Diseños mixtos combinan ambos enfoques aprovechando sus complementariedades. Encuestas cuantitativas establecen patrones agregados mientras entrevistas cualitativas explican mecanismos. Experimentos miden efectos netos mientras estudios de caso reconstruyen procesos de implementación. Como señala García Moreno y García López (2014), evaluaciones comprehensivas de intervenciones complejas típicamente requieren triangulación de múltiples fuentes y métodos.

## Selección de enfoque metodológico

La selección de enfoque metodológico debe considerar múltiples factores. Primero, **el propósito de la evaluación**: si el objetivo primario es atribución causal rigurosa de impactos, enfoques experimentales o cuasi-experimentales resultan apropiados; si el foco está en comprensión de procesos o factores contextuales, enfoques cualitativos o de teoría del cambio tienen prioridad.

Segundo, **las características de la intervención**: programas estandarizados con poblaciones grandes y procesos uniformes facilitan evaluaciones cuantitativas de impacto; intervenciones complejas, multifacéticas y adaptativas requieren enfoques que capturen heterogeneidad y contingencia. Tercero, **la disponibilidad de datos**: existencia de registros administrativos completos, líneas base o datos de panel habilita diseños cuantitativos sofisticados; ausencia de datos secundarios apropiados requiere recolección primaria cualitativa o cuantitativa.

Cuarto, **restricciones de tiempo y recursos**: evaluaciones experimentales o recolección de datos primarios a gran escala demandan tiempos y presupuestos que frecuentemente exceden disponibilidades; métodos basados en fuentes secundarias, entrevistas focalizadas o análisis documental pueden producir hallazgos útiles con recursos acotados.

Quinto, **consideraciones éticas y políticas**: aleatorización puede ser inaceptable cuando implica negar deliberadamente beneficios a grupos de control; evaluaciones que requieren acceso a información sensible enfrentan restricciones de confidencialidad; contextos políticamente polarizados pueden demandar metodologías de validez incuestionable.

## Preguntas de reflexión

1. Considere el Programa Acompañamiento Psicosocial (del Subsistema Seguridades y Oportunidades). ¿Qué tipo de evaluación sería prioritaria y por qué?

2. ¿En qué circunstancias una evaluación de procesos podría ser más valiosa que una evaluación de impacto? Piense en ejemplos concretos.

3. Analice un programa social chileno usando los seis criterios OECD-DAC. ¿Qué criterio presentaría mayores desafíos de evaluación y por qué?

4. ¿Qué ventajas y limitaciones tendría un enfoque puramente cuantitativo versus uno puramente cualitativo para evaluar el programa PACE?

## Referencias del capítulo

- DIPRES (2015). *Evaluación Ex-Post: Conceptos y Metodologías*. Ministerio de Hacienda.
- García Moreno, M., & García López, R. (2014). *La gestión para resultados en el desarrollo: avances y desafíos en América Latina y el Caribe*. Banco Interamericano de Desarrollo.
- Nagel, S.S. (2002). *Handbook of public policy evaluation*. Sage Publications.
- Pignatta, M.A. (2015). Monitoreo y evaluación de políticas públicas en América Latina: Brechas por cerrar. *Perspectivas de Políticas Públicas*.
