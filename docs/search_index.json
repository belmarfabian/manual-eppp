[["index.html", "Evaluación de Políticas Públicas Fundamentos, Metodologías y Aplicaciones Prefacio Propósito del libro Enfoque pedagógico Estructura del libro Agradecimientos Sobre el autor", " Evaluación de Políticas Públicas Fundamentos, Metodologías y Aplicaciones Fabián Belmar 2025-11-17 Prefacio Este libro constituye el material base para el curso de Evaluación de Políticas Públicas impartido en la carrera de Administración Pública de la Universidad de Talca. Su objetivo es proporcionar a estudiantes de pregrado las herramientas conceptuales, metodológicas y prácticas necesarias para comprender, diseñar e implementar evaluaciones de políticas y programas públicos. Propósito del libro La evaluación de políticas públicas se ha consolidado como una función esencial del Estado moderno. En un contexto de recursos limitados y crecientes demandas ciudadanas, la capacidad de determinar si las intervenciones públicas logran sus objetivos, a qué costo y con qué efectos, resulta fundamental para la mejora continua de la gestión pública y la rendición de cuentas democrática. Este texto busca formar profesionales capaces de: Comprender los fundamentos teóricos y metodológicos de la evaluación de políticas públicas Aplicar herramientas de diagnóstico, diseño y evaluación de intervenciones públicas Utilizar el marco lógico y otras metodologías estándar de evaluación Analizar críticamente políticas y programas existentes Proponer mejoras fundamentadas en evidencia Enfoque pedagógico El libro adopta un enfoque aplicado, privilegiando el aprendizaje basado en casos reales del contexto chileno. Cada capítulo integra: Fundamentos conceptuales respaldados por literatura especializada Metodologías paso a paso con ejemplos concretos Casos de estudio de políticas y programas chilenos Ejercicios prácticos y preguntas de reflexión Referencias a fuentes oficiales y bases de datos públicas Estructura del libro El texto se organiza en cuatro partes: Parte I: Fundamentos introduce los conceptos esenciales de la evaluación de políticas públicas, sus enfoques metodológicos, el marco institucional chileno y el rol del gobierno abierto. Parte II: Herramientas de Diagnóstico y Diseño desarrolla las metodologías fundamentales para el diagnóstico de problemas públicos y el diseño de intervenciones, incluyendo el árbol de problemas, el marco lógico y el diseño de indicadores. Parte III: Evaluación de Resultados e Impacto profundiza en las metodologías de evaluación ex-post, abarcando evaluación de procesos, resultados e impacto con sus respectivas técnicas cuantitativas y cualitativas. Parte IV: Aplicaciones integra los contenidos previos mediante casos de evaluación en Chile, la elaboración de informes y las consideraciones éticas de la práctica evaluativa. Agradecimientos Este libro se nutre de la experiencia de múltiples actores del sistema de evaluación chileno, así como de las contribuciones académicas de instituciones como CEPAL, DIPRES y la Comisión Nacional de Evaluación y Productividad. Agradezco especialmente a los estudiantes de Administración Pública de la Universidad de Talca, cuyas preguntas y trabajos han enriquecido continuamente este material. Sobre el autor Fabián Belmar es profesor de Administración Pública en la Universidad de Talca, Chile. "],["introducción-a-la-evaluación-de-políticas-públicas.html", "Capítulo 1 Introducción a la evaluación de políticas públicas 1.1 ¿Qué es la evaluación de políticas públicas? 1.2 Elementos esenciales de la evaluación 1.3 Principios rectores 1.4 Funciones de la evaluación en el sector público 1.5 Desafíos de la evaluación en Chile 1.6 Preguntas de reflexión 1.7 Referencias del capítulo", " Capítulo 1 Introducción a la evaluación de políticas públicas 1.1 ¿Qué es la evaluación de políticas públicas? La evaluación de políticas públicas constituye una función sistemática de análisis destinada a determinar el mérito, valor y utilidad de las intervenciones gubernamentales. Siguiendo a Ortegón, Pacheco y Prieto (2005), la evaluación implica un proceso de recopilación y análisis de información que permite emitir juicios fundamentados sobre el diseño, implementación, efectos e impactos de políticas y programas. No se trata simplemente de un ejercicio técnico de medición, sino de una práctica que articula consideraciones metodológicas, políticas, éticas y presupuestarias para informar la toma de decisiones públicas. En el contexto contemporáneo, la evaluación ha evolucionado desde una función marginal y episódica hacia un componente integral de la gestión pública moderna. Como señala la Comisión Nacional de Evaluación y Productividad (CNEP, 2023), los sistemas de evaluación contribuyen simultáneamente a tres objetivos fundamentales: mejorar la efectividad de las intervenciones públicas mediante el aprendizaje organizacional, optimizar la asignación de recursos escasos y fortalecer la rendición de cuentas democrática ante la ciudadanía. La evaluación se diferencia de otras funciones analíticas del sector público —como la auditoría, el monitoreo o la investigación académica— por su foco específico en determinar el valor de las intervenciones. Mientras el monitoreo rastrea el cumplimiento de metas y el uso de recursos, la evaluación interroga si las metas mismas son apropiadas, si la teoría de cambio subyacente es válida y si los resultados justifican los costos incurridos. Esta distinción resulta crucial para comprender el rol distintivo que la evaluación cumple en el ciclo de las políticas públicas. 1.2 Elementos esenciales de la evaluación Toda evaluación rigurosa se estructura en torno a cuatro elementos constitutivos. Primero, requiere preguntas evaluativas claras que definan qué aspectos específicos del programa se someterán a escrutinio. Estas preguntas pueden interrogar sobre la pertinencia del diseño, la calidad de la implementación, la eficiencia en el uso de recursos, la efectividad en el logro de objetivos o el impacto atribuible a la intervención. Segundo, demanda criterios de evaluación explícitos que establezcan los estándares contra los cuales se juzgará el desempeño. Los criterios del Comité de Ayuda al Desarrollo de la OCDE (OECD-DAC) —pertinencia, coherencia, eficacia, eficiencia, impacto y sostenibilidad— se han consolidado como marco de referencia internacional, aunque cada evaluación debe adaptar y priorizar criterios según su propósito específico. Tercero, requiere evidencia sistemática recopilada mediante métodos apropiados al objeto y preguntas de la evaluación. Esta evidencia puede provenir de fuentes cuantitativas (indicadores administrativos, encuestas, datos experimentales) o cualitativas (entrevistas, grupos focales, análisis documental), frecuentemente combinadas en diseños mixtos que aprovechan las fortalezas complementarias de ambos enfoques. Cuarto, culmina en juicios valorativos fundamentados que sintetizan los hallazgos en conclusiones y recomendaciones accionables. A diferencia de la investigación académica que busca producir conocimiento generalizable, la evaluación apunta a generar aprendizajes utilizables para mejorar intervenciones específicas o informar decisiones de política. 1.3 Principios rectores La práctica evaluativa se guía por principios que resguardan su calidad y utilidad. El principio de utilidad establece que las evaluaciones deben diseñarse y ejecutarse respondiendo a las necesidades de información de usuarios específicos, evitando ejercicios puramente ceremoniales o burocráticos. Como señala Bonnefoy y Armijo (2005), evaluaciones técnicamente sofisticadas pero desconectadas de procesos decisionales reales desperdician recursos y erosionan la credibilidad del sistema evaluativo. El principio de viabilidad reconoce que las evaluaciones operan bajo restricciones de tiempo, presupuesto, acceso a información y capacidades técnicas. Diseños metodológicamente ideales pero impracticables resultan contraproducentes; la evaluación efectiva balancea rigor y factibilidad, maximizando la validez de los hallazgos dentro de las restricciones existentes. El principio de propiedad ética exige que las evaluaciones respeten los derechos de los actores involucrados, protejan la confidencialidad de la información sensible, eviten consecuencias negativas no anticipadas y comuniquen hallazgos con transparencia y honestidad. Esto incluye considerar cuidadosamente cómo los hallazgos evaluativos pueden afectar a beneficiarios, implementadores y otros stakeholders. El principio de precisión técnica demanda que las evaluaciones empleen métodos apropiados, documenten procedimientos transparentemente y fundamenten conclusiones en evidencia sólida. Sin embargo, precisión no equivale a complejidad técnica innecesaria; frecuentemente, diseños relativamente simples pero bien ejecutados producen hallazgos más útiles que ejercicios metodológicamente elaborados pero opacos para usuarios no especializados. 1.4 Funciones de la evaluación en el sector público La evaluación cumple múltiples funciones en el ecosistema de políticas públicas. Su función instrumental consiste en proporcionar información que mejore directamente el diseño o la implementación de intervenciones específicas. Evaluaciones de procesos, por ejemplo, identifican cuellos de botella operacionales cuya corrección incrementa la efectividad del programa sin requerir recursos adicionales. La función conceptual opera a mayor plazo, contribuyendo a transformar gradualmente la comprensión de problemas públicos y las teorías causales que sustentan las intervenciones. Incluso cuando hallazgos específicos no generan cambios inmediatos, la acumulación de evidencia evaluativa modifica paradigmas de política. La consolidación del enfoque de determinantes sociales de la salud, por ejemplo, se nutrió sustancialmente de evaluaciones que documentaron los límites de intervenciones puramente clínicas. La función simbólica o legitimadora reconoce que las evaluaciones operan en contextos políticos donde su mera existencia —independientemente de hallazgos específicos— puede cumplir propósitos de legitimación, señalización de compromiso con transparencia o clausura de controversias. Si bien esta función puede prestarse a usos puramente ceremoniales, también puede servir propósitos democráticos genuinos al dotar de credibilidad técnica a decisiones políticamente complejas. Finalmente, la función de accountability vincula evaluación con rendición de cuentas democrática. Evaluaciones independientes y públicamente accesibles permiten a ciudadanos, legisladores y organizaciones de la sociedad civil escrutar el desempeño gubernamental con información técnicamente sólida, trascendiendo narrativas oficiales no contrastadas. 1.5 Desafíos de la evaluación en Chile El sistema chileno de evaluación enfrenta desafíos estructurales que condicionan la práctica evaluativa. Primero, la fragmentación institucional implica que diferentes servicios públicos operan con capacidades evaluativas heterogéneas, dificultando evaluaciones comprehensivas de políticas que atraviesan múltiples sectores. El Plan Nacional de Cuidados, por ejemplo, requeriría coordinar evaluaciones de MINSAL, MINEDUC, Ministerio de la Mujer y otros servicios, coordinación que el marco institucional actual dificulta. Segundo, las limitaciones de datos restringen el universo de preguntas evaluativas abordables mediante fuentes secundarias. Si bien Chile cuenta con registros administrativos e instrumentos de recopilación de datos relativamente robustos en perspectiva regional, persisten brechas significativas en información de procesos, calidad de servicios y trayectorias individuales de beneficiarios que limitan especialmente evaluaciones cualitativas rigurosas. Tercero, existen tensiones entre independencia y utilidad. Evaluaciones conducidas por unidades independientes maximizan credibilidad técnica pero pueden desconectarse de las necesidades operacionales de los gestores. Evaluaciones conducidas por las propias unidades ejecutoras aseguran relevancia pero enfrentan cuestionamientos sobre objetividad. El sistema chileno ha privilegiado el primer modelo, relegando evaluaciones internas y favoreciendo estudios externos, con consecuencias mixtas para el aprendizaje organizacional. Cuarto, la discontinuidad política genera incentivos débiles para el uso sistemático de evidencia evaluativa. Horizontes de gestión cortos y alta rotación de equipos directivos desincentivan inversiones en evaluaciones cuyos resultados madurarán bajo administraciones sucesoras. Como documenta el Banco Mundial (2005), evaluaciones del sistema chileno frecuentemente se producen cuando las decisiones más relevantes ya se han tomado, limitando su influencia efectiva. 1.6 Preguntas de reflexión ¿En qué se diferencia evaluar una política pública de simplemente medir si cumplió sus metas? Piense en un programa específico. Considere un programa social que conozca. ¿Qué función de la evaluación —instrumental, conceptual, simbólica o de accountability— sería más relevante en ese caso y por qué? El principio de viabilidad sugiere que diseños metodológicamente ideales pero impracticables deben ajustarse. ¿Qué ejemplos concretos de tensiones entre rigor y viabilidad puede identificar? ¿Cómo podrían los desafíos del sistema chileno de evaluación afectar el tipo de preguntas que se pueden responder confiablemente sobre políticas públicas? 1.7 Referencias del capítulo Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. CNEP (2023). Actualización y reenfoque al sistema de evaluación de programas públicos. Comisión Nacional de Evaluación y Productividad. Banco Mundial (2005). Chile: estudio de evaluación de impacto del Programa de Evaluación de Programas. Unidad de Reducción de la Pobreza y Gestión Económica América Latina y el Caribe. Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico para la planificación, el seguimiento y la evaluación de proyectos y programas. CEPAL. "],["enfoques-y-metodologías-de-evaluación.html", "Capítulo 2 Enfoques y metodologías de evaluación 2.1 Tipología de evaluaciones según momento 2.2 Evaluación por objeto de análisis 2.3 Enfoques metodológicos principales 2.4 Criterios de evaluación OECD-DAC 2.5 Métodos cuantitativos y cualitativos 2.6 Selección de enfoque metodológico 2.7 Preguntas de reflexión 2.8 Referencias del capítulo", " Capítulo 2 Enfoques y metodologías de evaluación 2.1 Tipología de evaluaciones según momento Las evaluaciones se clasifican primariamente según el momento del ciclo de política en que se conducen. Las evaluaciones ex-ante se realizan antes de la implementación, analizando el diseño propuesto de la intervención. Su propósito es valorar si el problema está correctamente diagnosticado, si la teoría de cambio es plausible, si los objetivos son realistas y si los recursos previstos resultan apropiados. Como señala DIPRES (2015), evaluaciones ex-ante rigurosas previenen inversiones en intervenciones mal concebidas cuyo fracaso sería predecible desde el diseño. Las evaluaciones ex-post ocurren durante o después de la implementación, examinando resultados efectivos y aprendizajes operacionales. Se subdividen según foco temporal: evaluaciones de procesos durante la implementación, evaluaciones de resultados al cierre o en momentos intermedios, y evaluaciones de impacto que requieren suficiente tiempo transcurrido para que efectos sustantivos se materialicen. La distinción ex-ante/ex-post no es puramente temporal sino sustantiva. Evaluaciones ex-ante trabajan con modelos y proyecciones, juzgando plausibilidad teórica y coherencia lógica. Evaluaciones ex-post trabajan con evidencia empírica de implementación efectiva, juzgando desempeño observado contra objetivos, benchmarks o contrafactuales. Metodológicamente, las primeras privilegian análisis lógico y prospectivo; las segundas, contrastación empírica retrospectiva. 2.2 Evaluación por objeto de análisis Una segunda tipología clasifica evaluaciones según su objeto de análisis específico. La evaluación de diseño examina la arquitectura conceptual de la intervención: coherencia entre diagnóstico del problema, objetivos, teoría de cambio, población objetivo, componentes y recursos. Utiliza herramientas como el marco lógico y el árbol de problemas para identificar inconsistencias o debilidades que comprometerían la efectividad incluso con implementación impecable. La evaluación de procesos analiza la calidad, eficiencia y fidelidad de la implementación. Interroga si las actividades se ejecutan según lo planificado, si los recursos se utilizan eficientemente, si existen cuellos de botella operacionales y si los beneficiarios efectivamente reciben los bienes o servicios contemplados. Como documenta Pignatta (2015), muchos programas con diseños sólidos fracasan por deficiencias de implementación que evaluaciones de procesos habrían detectado tempranamente. La evaluación de resultados examina el logro de productos y efectos inmediatos en beneficiarios. ¿Se entregaron los bienes o servicios comprometidos? ¿Los beneficiarios modificaron comportamientos, adquirieron competencias o mejoraron condiciones según lo esperado? Estas evaluaciones típicamente comparan indicadores con líneas base o metas, sin necesariamente establecer atribución causal rigurosa. La evaluación de impacto constituye el estándar más demandante, buscando determinar efectos causalmente atribuibles a la intervención mediante diseños que controlan por factores confundentes. Requiere contrafactuales creíbles —qué habría ocurrido a los beneficiarios sin el programa— típicamente mediante grupos de control o comparación, diseños experimentales, cuasi-experimentales o métodos econométricos sofisticados. 2.3 Enfoques metodológicos principales El campo evaluativo ha generado múltiples enfoques metodológicos con supuestos, fortalezas y limitaciones distintivas. El enfoque experimental constituye el estándar de oro para estimación de impacto causal. Asigna aleatoriamente individuos o unidades a tratamiento y control, garantizando que diferencias observadas ex-post sean atribuibles al programa. Sin embargo, requiere condiciones raramente satisfechas en contextos reales: factibilidad ética y política de aleatorización, escalas suficientes para potencia estadística, capacidad de mantener la asignación aleatoria durante implementación. Los enfoques cuasi-experimentales aproximan inferencia causal sin aleatorización. Técnicas como diferencias en diferencias, regresión discontinua, variables instrumentales o matching estadístico explotan variación natural en exposición al programa para construir contrafactuales plausibles. Si bien menos robustos que experimentos, resultan frecuentemente más viables y, bajo supuestos apropiados, producen estimaciones creíbles de efectos causales. El enfoque de teoría del cambio privilegia la explicitación y contrastación de los mecanismos causales postulados entre actividades e impactos. En lugar de reducir la evaluación a estimación de efectos netos, mapea la cadena causal completa, interrogando cada eslabón. ¿Las actividades generaron los productos esperados? ¿Los productos indujeron cambios intermedios anticipados? ¿Esos cambios contribuyeron a impactos finales? Este enfoque resulta especialmente valioso para intervenciones complejas con múltiples vías causales o donde la atribución causal rigurosa es impracticable. Los enfoques de estudios de caso examinan intensivamente un número reducido de instancias de implementación. Sacrifican generalización estadística por comprensión profunda de procesos, contextos y mecanismos. Mediante combinación de fuentes documentales, entrevistas y observación, reconstruyen trayectorias de implementación identificando facilitadores, obstáculos y factores contextuales críticos. Como argumenta Nagel (2002), casos bien seleccionados generan aprendizajes teóricos y prácticos que estudios de muestra grande frecuentemente pierden. Los enfoques participativos involucran activamente a beneficiarios y otros stakeholders en el diseño, conducción e interpretación de la evaluación. Parten de reconocer que distintos actores poseen conocimientos parciales pero complementarios sobre la intervención. Metodologías participativas no solo enriquecen la comprensión evaluativa sino que, al generar apropiación de hallazgos, incrementan la probabilidad de que recomendaciones se implementen efectivamente. 2.4 Criterios de evaluación OECD-DAC El Comité de Ayuda al Desarrollo de la OCDE ha consolidado seis criterios como marco de referencia para evaluación de políticas y programas. Pertinencia interroga si los objetivos de la intervención corresponden a las necesidades, prioridades y políticas de beneficiarios y contexto. Una intervención puede implementarse exitosamente y lograr objetivos, pero ser irrelevante si esos objetivos no responden a necesidades reales o prioridades legítimas de la población objetivo. Coherencia examina la compatibilidad de la intervención con otras intervenciones en el mismo contexto. ¿Complementa, duplica o contradice otros programas? ¿Se alinea con políticas y estrategias sectoriales? Falta de coherencia genera desperdicio de recursos, confusión para beneficiarios e incluso efectos contraproducentes cuando intervenciones trabajan en direcciones opuestas. Eficacia mide el grado de logro de los objetivos, considerando su importancia relativa. No se reduce a cumplimiento de metas, sino que pondera diferencialmente el logro de distintos objetivos según su relevancia para resolver el problema central. Un programa que cumple 100% metas secundarias pero fracasa en objetivos primarios no es eficaz, incluso si formalmente “cumple” porcentajes agregados de metas. Eficiencia relaciona resultados con recursos empleados. ¿Se lograron resultados óptimos con recursos dados, o resultados dados con recursos mínimos? Evaluación de eficiencia requiere benchmarking: comparación con intervenciones similares, estándares técnicos o análisis de alternativas. Un programa puede ser eficaz pero ineficiente si resultados equivalentes podrían lograrse con menores costos. Impacto refiere a efectos de largo plazo, amplios y potencialmente indirectos atribuibles a la intervención. Trasciende resultados inmediatos para examinar transformaciones sostenidas en condiciones de vida, capacidades, instituciones o prácticas. Evaluación rigurosa de impacto demanda horizontes temporales extendidos y metodologías que establezcan atribución causal, distinguiendo efectos del programa de tendencias contextuales. Sostenibilidad pregunta si beneficios perdurarán tras finalizar la intervención. ¿Se fortalecieron capacidades locales? ¿Se modificaron instituciones de modo perdurable? ¿Existen recursos y compromisos para continuar actividades clave? Muchos programas generan beneficios durante implementación que se evaporan rápidamente al cesar el apoyo externo, revelando fragilidad de los cambios inducidos. 2.5 Métodos cuantitativos y cualitativos La evaluación puede emplear métodos cuantitativos, cualitativos o mixtos según preguntas evaluativas y disponibilidad de datos. Métodos cuantitativos operan con datos numéricos, privilegian muestras grandes, análisis estadístico y generalización. Incluyen encuestas, análisis de registros administrativos, experimentos, y técnicas econométricas diversas. Su fortaleza radica en medición precisa, identificación de patrones sistemáticos y capacidad de establecer magnitud de efectos. Sus limitaciones incluyen reduccionismo de fenómenos complejos a variables medibles, dificultad para capturar procesos y mecanismos, y dependencia de disponibilidad de datos estructurados. Métodos cualitativos trabajan con datos textuales o visuales, privilegian profundidad sobre amplitud, comprensión contextualizada sobre generalización estadística. Incluyen entrevistas semi-estructuradas, grupos focales, observación participante, análisis documental y etnografía. Su fortaleza está en capturar perspectivas de actores, comprender procesos y contextos, identificar mecanismos causales y generar hallazgos inesperados que métodos estructurados habrían omitido. Sus limitaciones incluyen potencial falta de representatividad, dificultad de generalización, riesgos de sesgo interpretativo del evaluador y intensividad de recursos. Diseños mixtos combinan ambos enfoques aprovechando sus complementariedades. Encuestas cuantitativas establecen patrones agregados mientras entrevistas cualitativas explican mecanismos. Experimentos miden efectos netos mientras estudios de caso reconstruyen procesos de implementación. Como señala García Moreno y García López (2014), evaluaciones comprehensivas de intervenciones complejas típicamente requieren triangulación de múltiples fuentes y métodos. 2.6 Selección de enfoque metodológico La selección de enfoque metodológico debe considerar múltiples factores. Primero, el propósito de la evaluación: si el objetivo primario es atribución causal rigurosa de impactos, enfoques experimentales o cuasi-experimentales resultan apropiados; si el foco está en comprensión de procesos o factores contextuales, enfoques cualitativos o de teoría del cambio tienen prioridad. Segundo, las características de la intervención: programas estandarizados con poblaciones grandes y procesos uniformes facilitan evaluaciones cuantitativas de impacto; intervenciones complejas, multifacéticas y adaptativas requieren enfoques que capturen heterogeneidad y contingencia. Tercero, la disponibilidad de datos: existencia de registros administrativos completos, líneas base o datos de panel habilita diseños cuantitativos sofisticados; ausencia de datos secundarios apropiados requiere recolección primaria cualitativa o cuantitativa. Cuarto, restricciones de tiempo y recursos: evaluaciones experimentales o recolección de datos primarios a gran escala demandan tiempos y presupuestos que frecuentemente exceden disponibilidades; métodos basados en fuentes secundarias, entrevistas focalizadas o análisis documental pueden producir hallazgos útiles con recursos acotados. Quinto, consideraciones éticas y políticas: aleatorización puede ser inaceptable cuando implica negar deliberadamente beneficios a grupos de control; evaluaciones que requieren acceso a información sensible enfrentan restricciones de confidencialidad; contextos políticamente polarizados pueden demandar metodologías de validez incuestionable. 2.7 Preguntas de reflexión Considere el Programa Acompañamiento Psicosocial (del Subsistema Seguridades y Oportunidades). ¿Qué tipo de evaluación sería prioritaria y por qué? ¿En qué circunstancias una evaluación de procesos podría ser más valiosa que una evaluación de impacto? Piense en ejemplos concretos. Analice un programa social chileno usando los seis criterios OECD-DAC. ¿Qué criterio presentaría mayores desafíos de evaluación y por qué? ¿Qué ventajas y limitaciones tendría un enfoque puramente cuantitativo versus uno puramente cualitativo para evaluar el programa PACE? 2.8 Referencias del capítulo DIPRES (2015). Evaluación Ex-Post: Conceptos y Metodologías. Ministerio de Hacienda. García Moreno, M., &amp; García López, R. (2014). La gestión para resultados en el desarrollo: avances y desafíos en América Latina y el Caribe. Banco Interamericano de Desarrollo. Nagel, S.S. (2002). Handbook of public policy evaluation. Sage Publications. Pignatta, M.A. (2015). Monitoreo y evaluación de políticas públicas en América Latina: Brechas por cerrar. Perspectivas de Políticas Públicas. "],["marco-institucional-de-la-evaluación-en-chile.html", "Capítulo 3 Marco institucional de la evaluación en Chile 3.1 Evolución histórica del sistema de evaluación 3.2 Dirección de Presupuestos (DIPRES) 3.3 Comisión Nacional de Evaluación y Productividad (CNEP) 3.4 Contraloría General de la República 3.5 Unidades evaluativas sectoriales 3.6 Acceso público a evaluaciones 3.7 Uso de evaluaciones en decisiones presupuestarias 3.8 Preguntas de reflexión 3.9 Referencias del capítulo", " Capítulo 3 Marco institucional de la evaluación en Chile 3.1 Evolución histórica del sistema de evaluación El sistema chileno de evaluación de programas públicos se consolida gradualmente a partir de la década de 1990, en el contexto de modernización del Estado post-dictadura. Su génesis responde a múltiples factores convergentes: democratización que demanda transparencia y rendición de cuentas, restricciones fiscales que exigen optimización del gasto, y paradigmas internacionales de Nueva Gestión Pública que priorizan resultados sobre procedimientos (Banco Mundial, 2005). En 1994 se institucionaliza el Sistema de Control de Gestión, requiriendo que servicios públicos formulen objetivos, metas e indicadores de desempeño asociados a programas presupuestarios. Esto marca un giro desde contabilidad de insumos hacia gestión por resultados, aunque inicialmente con desarrollo metodológico incipiente y uso limitado en asignación presupuestaria. En 1997 se crea el Programa de Evaluación de Programas Gubernamentales, institucionalizado en la Dirección de Presupuestos (DIPRES) del Ministerio de Hacienda. Este programa establece un ciclo anual de evaluaciones externas, independientes y públicas de programas seleccionados mediante negociación entre DIPRES y ministerios sectoriales. Las evaluaciones, conducidas por consultores externos mediante licitación pública, siguen metodologías estandarizadas y sus resultados informan las negociaciones presupuestarias subsecuentes. En 2001 se introducen los Programas de Mejoramiento de la Gestión (PMG), que vinculan incentivos monetarios colectivos al cumplimiento de metas institucionales. Posteriormente, la Ley de Presupuestos incorpora compromisos de gestión vinculados a evaluación. En 2012 se crea la Comisión Nacional de Evaluación y Productividad (CNEP, inicialmente llamada Comisión Nacional de Productividad), otorgando a la función evaluativa una instancia consultiva autónoma con mandato de asesorar al Presidente en materia de evaluación de políticas públicas. 3.2 Dirección de Presupuestos (DIPRES) DIPRES constituye el actor central del sistema de evaluación chileno. Como entidad dependiente del Ministerio de Hacienda, concentra responsabilidades normativas, coordinación del ciclo evaluativo anual y utilización de hallazgos en la negociación presupuestaria. Su División de Control de Gestión administra el Programa de Evaluación de Programas, definiendo metodologías, términos de referencia, supervisión de evaluaciones externas y seguimiento de recomendaciones. El modelo institucional privilegia evaluación externa. Consultoras independientes, seleccionadas mediante licitación pública, conducen los estudios siguiendo pautas metodológicas de DIPRES. Esto resguarda independencia técnica y credibilidad, evitando autoevaluaciones que enfrentarían conflictos de interés. Sin embargo, genera tensiones: evaluadores externos carecen de conocimiento institucional profundo que gestores poseen, y gestores pueden percibir las evaluaciones como ejercicios fiscalizadores antes que aprendizaje organizacional. DIPRES administra también el Sistema de Información para la Gestión Financiera del Estado (SIGFE), el Banco Integrado de Proyectos (BIP) y el Sistema de Evaluación y Control de Gestión, produciendo información integrada sobre presupuestos, inversiones y desempeño. Esta infraestructura informacional facilita evaluaciones al proporcionar datos longitudinales sobre recursos, metas y resultados, aunque con limitaciones en información de procesos y calidad de servicios. El sistema prioriza dos tipos de evaluación: Evaluaciones de Programas Gubernamentales (EPG), que examinan comprehensivamente diseño, gestión y resultados de programas específicos mediante metodologías mixtas; y Evaluaciones de Impacto, que estiman efectos causales de intervenciones mediante diseños cuantitativos. Adicionalmente, DIPRES conduce Evaluaciones Comprehensivas del Gasto que analizan sectores completos, y Análisis de Focalización que examinan si beneficios alcanzan poblaciones objetivo. 3.3 Comisión Nacional de Evaluación y Productividad (CNEP) Creada en 2012 como organismo asesor técnico autónomo del Presidente, CNEP tiene mandato de proponer políticas para mejorar la evaluación de programas públicos y la productividad de la economía chilena. A diferencia de DIPRES, CNEP no ejecuta evaluaciones sino que genera análisis transversales, propone reformas al sistema evaluativo y elabora metodologías. Su composición —siete profesionales de reconocido prestigio designados por el Presidente previo proceso de selección pública— busca asegurar independencia técnica y pluralismo. Sin capacidad de implementar directamente recomendaciones, CNEP opera mediante incidencia: produce estudios técnicamente sólidos que informan debate público y generan presión para reforma. El informe CNEP (2023) constituye el diagnóstico más comprehensivo del sistema chileno. Identifica debilidades estructurales: énfasis excesivo en evaluaciones individuales de programas versus análisis de políticas integrales; desconexión entre evaluación y procesos de diseño de políticas; debilidades de coordinación intersectorial; limitaciones de capacidades evaluativas en servicios públicos; y uso insuficiente de hallazgos para mejora continua. Propone reorientar el sistema desde fiscalización de programas hacia aprendizaje sistemático para mejora de políticas. 3.4 Contraloría General de la República Si bien su función primaria es auditoría de legalidad y regularidad del gasto, Contraloría realiza auditorías de desempeño que se solapan parcialmente con evaluación. Estas auditorías examinan economía, eficiencia y eficacia de programas desde perspectiva de cumplimiento de normativa y uso apropiado de recursos públicos. A diferencia de evaluaciones de DIPRES que interrogan pertinencia de objetivos y teorías de cambio, auditorías de Contraloría típicamente aceptan objetivos como dados y examinan su logro. La coexistencia de evaluación en DIPRES y auditoría de desempeño en Contraloría genera complementariedades pero también potenciales redundancias y confusión. Evaluaciones enfatizan aprendizaje y mejora; auditorías, accountability y corrección. Los dos enfoques requieren coordinación para evitar sobrelapamiento ineficiente de estudios sobre programas similares. 3.5 Unidades evaluativas sectoriales Ministerios y servicios públicos cuentan con capacidades evaluativas heterogéneas. Algunos sectores —salud, educación, desarrollo social— han desarrollado unidades de estudios o departamentos de evaluación relativamente robustos. Otros sectores mantienen capacidades mínimas, dependiendo críticamente del ciclo DIPRES para generar evidencia evaluativa. Esta heterogeneidad refleja diferencias en tradición de uso de evidencia, disponibilidad de profesionales especializados y culturas organizacionales. Sectores con mayor sofisticación técnica y recursos (MINSAL, MINEDUC) producen evaluaciones internas, estudios de costo-efectividad y proyecciones de impacto que complementan el ciclo DIPRES. Servicios con capacidades limitadas dependen exclusivamente de evaluaciones externas, restringiendo su autonomía analítica. DIPRES ha implementado programas de fortalecimiento de capacidades evaluativas sectoriales, reconociendo que sistemas descentralizados con competencias distribuidas producen mejor evidencia que modelos centralizados. Sin embargo, restricciones presupuestarias y rotación de personal limitan sostenibilidad de estas inversiones. 3.6 Acceso público a evaluaciones Chile destaca internacionalmente por transparencia de su sistema evaluativo. Todas las evaluaciones del programa DIPRES se publican íntegramente en el Portal de Evaluación y Control de Gestión (www.dipres.gob.cl), incluyendo informes finales, anexos metodológicos y bases de datos cuando aplicable. Esta apertura contrasta con sistemas de otros países donde evaluaciones circulan restringidamente entre tomadores de decisión. La transparencia cumple múltiples funciones. Permite escrutinio de calidad técnica por académicos y sociedad civil, incentivando rigor. Habilita uso secundario de hallazgos por investigadores, periodistas y organizaciones. Fortalece accountability al exponer desempeño gubernamental a evaluación pública. Sin embargo, accesibilidad no garantiza acceso efectivo: informes extensos, técnicamente complejos y sin síntesis amigables limitan utilización por audiencias no especializadas. El Banco Integrado de Programas Sociales (BIPS) complementa el portal de evaluaciones, proporcionando información estandarizada sobre diseño, cobertura, presupuesto y población objetivo de programas sociales. Su arquitectura facilita comparaciones entre programas y seguimiento longitudinal, aunque con limitaciones en actualización y completitud. 3.7 Uso de evaluaciones en decisiones presupuestarias El sistema chileno vincula formalmente evaluación con presupuesto: resultados de EPG informan la Ley de Presupuestos del año subsiguiente, y programas evaluados negativamente enfrentan riesgo de reducción o eliminación presupuestaria. Esta vinculación institucionalizada distingue a Chile de sistemas donde evaluaciones se producen pero su uso permanece discrecional. Sin embargo, la relación entre hallazgos evaluativos y asignaciones presupuestarias resulta menos mecánica que sugiere el diseño formal. Como documenta el Banco Mundial (2005), decisiones presupuestarias responden simultáneamente a múltiples factores: prioridades políticas del gobierno, presiones legislativas, compromisos previos y restricciones fiscales. Evaluaciones constituyen un insumo relevante pero no determinante. La implementación de recomendaciones evaluativas enfrenta desafíos. DIPRES realiza seguimiento formal de compromisos, requiriendo que servicios reporten avances en implementación de recomendaciones. No obstante, incentivos para implementación efectiva son débiles cuando recomendaciones demandan cambios organizacionales complejos, coordinación inter-ministerial o recursos adicionales en contextos de restricción fiscal. 3.8 Preguntas de reflexión ¿Qué ventajas y desventajas tiene que DIPRES —la entidad responsable del presupuesto— lidere el sistema de evaluación? Compare el rol de CNEP versus DIPRES. ¿Qué valor agregado aporta tener una comisión consultiva independiente adicional? Revise una evaluación publicada en el portal DIPRES. ¿Qué tan accesibles encuentra los hallazgos para un ciudadano sin formación técnica? ¿Cómo explicaría que evaluaciones puedan ser técnicamente rigurosas pero tener impacto limitado en decisiones de política? 3.9 Referencias del capítulo Banco Mundial (2005). Chile: estudio de evaluación de impacto del Programa de Evaluación de Programas. Unidad de Reducción de la Pobreza y Gestión Económica América Latina y el Caribe. CNEP (2023). Actualización y reenfoque al sistema de evaluación de programas públicos: una propuesta para un Estado más moderno y efectivo. Comisión Nacional de Evaluación y Productividad. CEP (2017). Un Estado para la ciudadanía. Informe de la Comisión de Modernización del Estado. Centro de Estudios Públicos. Pérez, G., &amp; Maldonado, C. (Eds.) (2015). Panorama de los sistemas nacionales de monitoreo y evaluación en América Latina. CIDE/Clear LAC. "],["gobierno-abierto-y-transparencia-en-evaluación.html", "Capítulo 4 Gobierno abierto y transparencia en evaluación 4.1 El paradigma de gobierno abierto 4.2 Transparencia evaluativa en Chile: avances y limitaciones 4.3 Participación ciudadana en procesos evaluativos 4.4 Uso de evaluaciones por organizaciones de la sociedad civil 4.5 Estrategias de comunicación de hallazgos evaluativos 4.6 Desafíos de implementación 4.7 Preguntas de reflexión 4.8 Referencias del capítulo", " Capítulo 4 Gobierno abierto y transparencia en evaluación 4.1 El paradigma de gobierno abierto El gobierno abierto constituye un modelo de gestión pública fundamentado en tres pilares interdependientes: transparencia, participación ciudadana y colaboración entre actores. Como señala el Banco Mundial (2010), este paradigma trasciende la simple apertura de información gubernamental para configurar una arquitectura relacional donde Estado y sociedad civil interactúan horizontalmente en la producción, implementación y evaluación de políticas públicas. La transparencia implica acceso ciudadano a información gubernamental comprensible, oportuna y utilizable. No se reduce a disponibilidad formal de datos sino a inteligibilidad efectiva: información técnicamente accesible pero incomprensible para audiencias no especializadas no satisface estándares de transparencia sustantiva. La participación refiere a involucramiento activo de ciudadanos y organizaciones en procesos de diseño, implementación y evaluación de políticas, reconociendo que distintos actores poseen conocimientos complementarios sobre problemas públicos y efectividad de intervenciones. La colaboración establece que gobierno, sector privado, academia y sociedad civil pueden co-producir soluciones más efectivas que las generadas unilateralmente por el Estado. Este modelo cobra especial relevancia para evaluación de políticas públicas. Como argumenta el informe CEP (2017) sobre modernización del Estado chileno, sistemas evaluativos transparentes, participativos y colaborativos generan simultáneamente mayor calidad técnica —al incorporar perspectivas diversas—, mayor legitimidad —al permitir escrutinio público— y mayor utilización de hallazgos —al generar apropiación entre actores que participaron en la evaluación. 4.2 Transparencia evaluativa en Chile: avances y limitaciones Chile destaca regionalmente por su transparencia evaluativa formal. Desde 1997, todas las evaluaciones del Programa de Evaluación de Programas Gubernamentales se publican íntegramente en el Portal de Evaluación y Control de Gestión de DIPRES (www.dipres.gob.cl). Esta apertura institucionalizada incluye informes finales completos, anexos metodológicos, bases de datos anonimizadas cuando aplicable, compromisos de implementación de recomendaciones y reportes de seguimiento de dichos compromisos. El portal organiza evaluaciones por año, ministerio, tipo de evaluación y estado de implementación de recomendaciones. Usuarios pueden descargar libremente todos los documentos sin registro previo. Adicionalmente, DIPRES publica metodologías estándar, términos de referencia de licitaciones, y reportes anuales que sintetizan el ciclo evaluativo. Esta infraestructura de transparencia contrasta marcadamente con sistemas de otros países donde evaluaciones circulan como documentos internos de gobierno con acceso restringido a tomadores de decisión. Sin embargo, accesibilidad formal no garantiza acceso efectivo. Como documenta Irarrázaval et al. (2020), la mayoría de evaluaciones DIPRES constituyen documentos técnicamente complejos, extensos (frecuentemente 100+ páginas) y redactados en lenguaje especializado que presupone familiaridad con metodologías evaluativas. Ciudadanos sin formación técnica, periodistas generalistas e incluso funcionarios públicos de áreas no especializadas enfrentan barreras significativas para interpretar y utilizar estos documentos. La ausencia sistemática de resúmenes ejecutivos amigables —síntesis de 2-3 páginas en lenguaje accesible destacando hallazgos prioritarios y recomendaciones accionables— limita severamente la democratización efectiva del acceso a evidencia evaluativa. Mientras especialistas —académicos, consultores, analistas de think tanks— utilizan extensivamente el portal DIPRES, audiencias más amplias permanecen efectivamente excluidas del acceso a esta información pública. El Banco Integrado de Programas Sociales (BIPS), administrado por el Ministerio de Desarrollo Social, complementa parcialmente el portal DIPRES al proporcionar información estandarizada sobre diseño, población objetivo, cobertura y presupuesto de programas sociales. Su interfaz más amigable y fichas sintéticas facilitan comparaciones entre programas. Sin embargo, el BIPS enfrenta limitaciones de actualización y completitud, con programas descritos de modo heterogéneo y datos frecuentemente desactualizados. 4.3 Participación ciudadana en procesos evaluativos La participación puede operar en múltiples momentos del ciclo evaluativo con diferentes niveles de intensidad. Arnstein (1969) propone una escalera de participación que distingue entre: (1) información —comunicación unidireccional de decisiones ya tomadas—, (2) consulta —recopilación de opiniones sin compromiso vinculante de incorporarlas—, (3) participación colaborativa —influencia real en decisiones—, y (4) delegación —transferencia de poder decisional a ciudadanos. En evaluación de políticas, estas modalidades se traducen en prácticas diferenciadas. En la definición de preguntas evaluativas, participación implica involucrar a beneficiarios, implementadores y otros stakeholders en identificar qué aspectos del programa resultan más críticos de examinar. Gestores pueden priorizar eficiencia operacional; beneficiarios, calidad de atención y pertinencia de prestaciones; implementadores de primera línea, viabilidad de procedimientos. Cada perspectiva enriquece el diseño evaluativo al revelar dimensiones que evaluadores externos podrían ignorar. En recolección de datos, participación opera mediante entrevistas semi-estructuradas, grupos focales, talleres participativos o encuestas que capturan experiencias, valoraciones y recomendaciones de quienes interactúan cotidianamente con el programa. Métodos participativos como cartografía social, fotovoz o relatos de vida permiten que beneficiarios documenten sus experiencias en formatos no extractivos donde ellos controlan narrativas. En interpretación de hallazgos, participación implica contrastar evidencia cuantitativa con conocimiento experiencial de implementadores y beneficiarios. Talleres de validación donde evaluadores presentan hallazgos preliminares y reciben retroalimentación permiten identificar interpretaciones erróneas, matizar conclusiones y generar recomendaciones más viables. La literatura internacional documenta que evaluaciones participativas —cuando bien diseñadas— generan múltiples beneficios: mayor validez al incorporar conocimiento local que evaluadores externos carecen, mayor legitimidad al incluir voces de afectados, mayor utilización al generar apropiación de hallazgos entre actores que participaron, y empoderamiento de beneficiarios al reconocer su expertise sobre el programa. El caso chileno muestra participación limitada y selectiva. Evaluaciones DIPRES típicamente se diseñan mediante negociación entre la División de Control de Gestión y los ministerios sectoriales responsables del programa evaluado, con involucramiento acotado incluso de gestores operacionales y prácticamente nulo de beneficiarios en la definición de preguntas evaluativas. Durante la recolección de datos, consultoras externas conducen entrevistas con directivos y funcionarios, y ocasionalmente grupos focales con beneficiarios, pero en modalidades extractivas donde ciudadanos proveen información sin participar en análisis o interpretación. Este déficit participativo refleja tensiones estructurales del sistema chileno. La priorización de independencia técnica y rigor metodológico —valores centrales del modelo— genera recelos hacia participación que podría comprometer objetividad. El énfasis en evaluación como instrumento de fiscalización presupuestaria incentiva diseños tecnocráticos sobre enfoques participativos. Restricciones de tiempo y presupuesto limitan la viabilidad de procesos participativos genuinos que demandan inversión sostenida de recursos. 4.4 Uso de evaluaciones por organizaciones de la sociedad civil Más allá de acceso gubernamental a evaluaciones para decisiones presupuestarias, sistemas de gobierno abierto aspiran a que múltiples actores —organizaciones de la sociedad civil, medios de comunicación, academia, ciudadanos individuales— utilicen evidencia evaluativa para advocacy, escrutinio de políticas, investigación o periodismo. Esta diversificación de usuarios democratiza el sistema evaluativo y genera presión para uso efectivo de hallazgos por parte de gobierno. En Chile, organizaciones especializadas —think tanks como Centro de Estudios Públicos, Fundación Sol, CIPER, Espacio Público— utilizan intensivamente evaluaciones DIPRES para análisis de políticas, reportes de investigación y columnas de opinión. Estas organizaciones cuentan con capacidades técnicas para interpretar metodologías complejas y extraer implicancias de política. Sus productos —policy briefs, estudios, artículos de prensa— traducen hallazgos técnicos a formatos más accesibles para audiencias amplias. Académicos emplean evaluaciones gubernamentales como fuentes secundarias para investigación, contrastando hallazgos con estudios propios, replicando análisis con metodologías alternativas o utilizando datos públicos para nuevas preguntas de investigación. Este uso académico genera escrutinio de calidad técnica del sistema evaluativo, identifica limitaciones metodológicas y produce conocimiento que enriquece debates de política. Medios de comunicación utilizan evaluaciones ocasionalmente, típicamente cuando hallazgos son políticamente controversiales (programas emblemáticos evaluados negativamente, casos de ineficiencia severa, denuncias de mala focalización). Sin embargo, cobertura mediática es episódica y frecuentemente superficial, reproduciendo titulares sin análisis profundo de metodologías o matices de hallazgos. Organizaciones sectoriales —sindicatos de profesores, colegios profesionales, federaciones de funcionarios— utilizan selectivamente evaluaciones cuando hallazgos respaldan sus demandas. Esto genera uso estratégico donde actores citan evidencia favorable e ignoran hallazgos inconvenientes, fenómeno que refleja naturaleza política del uso de evidencia. Ciudadanos individuales muestran uso marginal de evaluaciones, limitado a casos donde programas les afectan directamente y disponen de capital educacional para navegar documentos técnicos. Esta brecha de acceso efectivo sugiere que transparencia formal beneficia primariamente a élites técnicas y políticas, reproduciendo desigualdades epistémicas. 4.5 Estrategias de comunicación de hallazgos evaluativos Comunicación efectiva de evaluaciones requiere reconocer que distintas audiencias necesitan diferentes productos con niveles de detalle, complejidad técnica y formato adaptados a sus usos previstos. Una evaluación comprehensiva debe generar múltiples productos comunicacionales, no un único informe técnico. Para tomadores de decisión políticos y directivos superiores: Resúmenes ejecutivos de 2-3 páginas que sinteticen hallazgos priorizados según relevancia para decisiones, expliciten implicancias de política con claridad, y presenten recomendaciones accionables jerarquizadas por urgencia e impacto potencial. Lenguaje debe ser directo, evitar jerga técnica innecesaria y privilegiar visualizaciones sobre tablas densas. Para gestores operacionales del programa evaluado: Informes detallados que profundicen en dimensiones operacionales —cuellos de botella específicos, variaciones territoriales de implementación, análisis de costos unitarios, comparaciones con benchmarks sectoriales— con recomendaciones concretas sobre ajustes de procedimientos, capacitación de personal o reasignación de recursos. Estos usuarios valoran granularidad sobre síntesis. Para ciudadanía general: Síntesis visuales de 1-2 páginas con infografías, gráficos simples y lenguaje completamente desprovisto de tecnicismos. Estos productos deben responder preguntas básicas: ¿El programa cumplió sus objetivos? ¿Los recursos se usaron bien? ¿Qué mejoras se proponen? Videos cortos o podcasts pueden complementar productos escritos para audiencias con preferencias por formatos audiovisuales. Para comunidad académica y técnica: Informes completos con anexos metodológicos exhaustivos, bases de datos, especificaciones de modelos estadísticos, discusión de limitaciones y comparación con literatura especializada. Estos usuarios priorizan rigor y replicabilidad sobre accesibilidad. Para medios de comunicación: Notas de prensa sintéticas que destaquen hallazgos noticiables, contextualicen con datos accesibles, incluyan citas atribuibles y proporcionen contactos para ampliar información. Kits de prensa con gráficos descargables facilitan cobertura. Estrategias exitosas incluyen lanzamientos públicos de evaluaciones con presentaciones a múltiples audiencias, conferencias de prensa para evaluaciones de alto perfil, webinars para actores especializados, y repositorios digitales organizados temáticamente que faciliten búsqueda. Redes sociales institucionales pueden difundir hallazgos en formatos breves que dirijan a documentos completos. El desafío radica en balancear transparencia comprehensiva —publicación de informes técnicos completos— con accesibilidad diferenciada —productos adaptados a audiencias diversas. Sistemas evaluativos maduros invierten recursos significativos en comunicación, reconociendo que evaluaciones técnicamente impecables pero no comunicadas efectivamente desperdician su potencial de mejora de políticas y rendición de cuentas. 4.6 Desafíos de implementación La agenda de gobierno abierto en evaluación enfrenta obstáculos estructurales. Primero, tensiones entre independencia técnica y participación: evaluadores externos independientes maximizan credibilidad pero carecen de conocimiento contextual que implementadores y beneficiarios poseen; participación puede enriquecer comprensión pero genera riesgos de captura donde actores con intereses en el programa influencian sesgadamente hallazgos. Segundo, restricciones de recursos y tiempo: procesos participativos genuinos demandan inversión sostenida en convocatoria, facilitación, devolución de resultados e incorporación efectiva de insumos. Evaluaciones con plazos ajustados y presupuestos limitados difícilmente pueden implementar participación robusta sin comprometer calidad técnica. Tercero, capacidades diferenciadas de actores: participación efectiva requiere que ciudadanos y organizaciones cuenten con conocimientos mínimos sobre evaluación para contribuir constructivamente. Ausencia de capacidades genera participación ceremonial donde actores proveen opiniones desconectadas de evidencia o desconocimiento de trade-offs inevitables en política pública. Cuarto, cultura organizacional resistente: instituciones públicas habituadas a operar opacamente enfrentan costos de transición hacia transparencia y rendición de cuentas. Funcionarios pueden percibir evaluaciones participativas como amenazas antes que oportunidades de mejora, generando resistencias que obstaculizan acceso a información o sesgan interacciones con evaluadores. 4.7 Preguntas de reflexión Revise el portal DIPRES (www.dipres.gob.cl). Descargue una evaluación reciente. ¿Qué tan accesible encuentra el documento para un ciudadano sin formación en evaluación? ¿Qué elementos comunicacionales agregaría? Considere el Programa Acompañamiento Psicosocial. ¿En qué momentos del ciclo evaluativo involucraría a familias beneficiarias? ¿Qué metodologías participativas emplearía sin comprometer rigor técnico? Un think tank de oposición política utiliza selectivamente hallazgos de una evaluación gubernamental para criticar un programa emblemático del gobierno, ignorando matices metodológicos. ¿Cómo debería responder el evaluador? ¿Qué responsabilidades tiene sobre uso de sus hallazgos? ¿Qué ventajas y riesgos tiene que evaluaciones gubernamentales sean completamente públicas versus circulación restringida entre tomadores de decisión? 4.8 Referencias del capítulo Banco Mundial (2010). La formulación de políticas en la OCDE: Ideas para América Latina. Banco Mundial LAC. CEP (2017). Un Estado para la ciudadanía. Informe de la Comisión de Modernización del Estado. Centro de Estudios Públicos. Irarrázaval, I., Larrañaga, O., Rodríguez, J., &amp; Valdés, R. (2020). Propuestas para una mejor calidad del gasto y las políticas públicas en Chile. Centro de Políticas Públicas UC. Pérez, G., &amp; Maldonado, C. (Eds.) (2015). Panorama de los sistemas nacionales de monitoreo y evaluación en América Latina. CIDE/Clear LAC. "],["evaluación-de-diseño-de-políticas-y-programas.html", "Capítulo 5 Evaluación de diseño de políticas y programas 5.1 Concepto y propósito de la evaluación de diseño 5.2 Componentes esenciales de la evaluación de diseño 5.3 Metodología de evaluación de diseño 5.4 Criterios de juicio en evaluación de diseño 5.5 Caso aplicado: Evaluación de diseño del Programa de Acompañamiento y Acceso Efectivo (PACE) 5.6 Limitaciones y complementariedades 5.7 Preguntas de reflexión 5.8 Referencias del capítulo", " Capítulo 5 Evaluación de diseño de políticas y programas 5.1 Concepto y propósito de la evaluación de diseño La evaluación de diseño constituye una valoración sistemática de la arquitectura conceptual y operacional de una intervención, realizada típicamente en fase ex-ante —previa a implementación completa— aunque también aplicable a programas en operación para identificar debilidades estructurales que explican desempeño deficiente. Como establece DIPRES (2015), su propósito es determinar si el programa cuenta con fundamentos técnicos sólidos que hagan razonable esperar que, bajo implementación apropiada, logre sus objetivos. A diferencia de evaluaciones de resultados o impacto que interrogan desempeño observado empíricamente, la evaluación de diseño opera mediante análisis lógico de coherencia, consistencia y viabilidad. No pregunta “¿funcionó el programa?” sino “¿debería funcionar si se implementa adecuadamente?”. Esta distinción resulta crucial: programas con diseños defectuosos fracasarán independientemente de calidad de gestión, mientras programas bien diseñados pueden fracasar por deficiencias de implementación. Ortegón, Pacheco y Prieto (2005) argumentan que evaluaciones ex-ante rigurosas previenen desperdicio de recursos en intervenciones cuyo fracaso era predecible desde el diseño. En contextos de restricción fiscal, identificar tempranamente programas con fundamentos técnicos débiles —antes de invertir recursos sustantivos en implementación— constituye contribución mayor a eficiencia del gasto público que evaluar ex-post programas ya ejecutados. La evaluación de diseño cumple múltiples funciones. Para tomadores de decisión, proporciona evidencia técnica para decidir si proceder con implementación, modificar el diseño o abandonar la iniciativa. Para gestores, identifica ajustes necesarios antes que deficiencias se materialicen en fracasos operacionales. Para beneficiarios potenciales, resguarda que intervenciones efectivamente respondan a sus necesidades. Para el sistema público, fortalece cultura de planificación basada en evidencia versus improvisación. 5.2 Componentes esenciales de la evaluación de diseño 5.2.1 1. Diagnóstico del problema público El primer componente examina si el problema que motiva la intervención está correctamente identificado, caracterizado y cuantificado. Un diagnóstico robusto especifica: (a) manifestaciones observables del problema, (b) población afectada con estimaciones cuantitativas, (c) distribución territorial y temporal, (d) causas fundamentales versus síntomas, (e) tendencias históricas y proyecciones, (f) experiencias previas de abordaje. Errores comunes incluyen confundir síntomas con problemas (tratar “bajo rendimiento escolar” sin identificar sus causas), definir problemas como ausencia de soluciones (“falta de programa X”), diagnósticos sin respaldo empírico que reproducen supuestos no contrastados, y sub-estimación o sobre-estimación de magnitud que genera programas desproporcionados. Un buen diagnóstico debe ser contrastable: afirmaciones sobre magnitud, distribución y causas del problema deben respaldar en datos verificables de fuentes confiables. El estándar chileno privilegia uso de encuestas nacionales (CASEN, ENE, CENSO), registros administrativos sectoriales (MINSAL, MINEDUC, Superintendencias) y estudios académicos publicados, evitando diagnósticos basados exclusivamente en percepciones de expertos o narrativas políticas sin sustento empírico. 5.2.2 2. Pertinencia de objetivos Este componente evalúa si los objetivos del programa responden efectivamente al problema diagnosticado y se alinean con políticas superiores. Pertinencia implica correspondencia lógica: objetivos deben apuntar a resolver el problema o sus causas fundamentales, no a síntomas secundarios. Un programa cuyo diagnóstico identifica “deserción escolar por embarazo adolescente y trabajo infantil” pero cuyos objetivos se limitan a “mejorar infraestructura de establecimientos” sufre de incoherencia problema-objetivos. Adicionalmente, objetivos deben alinearse con políticas sectoriales, compromisos internacionales y prioridades gubernamentales. Un programa de primera infancia que ignore compromisos del Subsistema Chile Crece Contigo o estándares de la Convención de Derechos del Niño enfrenta cuestionamientos de coherencia sistémica. Los objetivos deben ser SMART: específicos (no ambiguos), medibles (verificables), alcanzables (realistas dado recursos y horizonte temporal), relevantes (importantes para resolver el problema) y temporizados (con plazos definidos). Objetivos vagos como “mejorar la calidad de vida” sin especificar dimensiones, magnitud y plazo resultan técnicamente deficientes. 5.2.3 3. Población objetivo y focalización La evaluación de diseño interroga si la población objetivo está correctamente definida, cuantificada y es alcanzable. Definición precisa especifica criterios de elegibilidad verificables objetivamente. Cuantificación estima tamaño de población elegible mediante fuentes estadísticas confiables. Alcanzabilidad considera si el programa cuenta con mecanismos efectivos para identificar y contactar a potenciales beneficiarios. Problemas frecuentes incluyen: (a) definiciones difusas que generan ambigüedad sobre quién califica (“familias vulnerables” sin especificar umbral operacional), (b) estimaciones de población objetivo desactualizadas o metodológicamente débiles, (c) focalización que excluye incorrectamente a población elegible o incluye a no-elegibles, (d) ausencia de mecanismos de identificación cuando población objetivo es “invisible” para sistemas administrativos. El caso chileno muestra heterogeneidad. Programas que utilizan el Registro Social de Hogares para focalización cuentan con identificación robusta de población vulnerable. Programas dirigidos a poblaciones informales, migrantes irregulares o grupos con barreras de acceso enfrentan desafíos mayores de alcanzabilidad que frecuentemente no se resuelven en el diseño. 5.2.4 4. Teoría de cambio y cadena causal El componente central de evaluación de diseño examina la teoría de cambio subyacente: el modelo causal que postula cómo las actividades del programa generarán productos, los productos inducirán resultados en beneficiarios, y los resultados contribuirán a impactos de largo plazo. Como establece la metodología CEPAL, esta cadena debe ser explícita, plausible y estar respaldada por evidencia o literatura especializada. Una teoría de cambio robusta especifica: (a) la secuencia lógica actividades → productos → resultados → impactos, (b) los mecanismos causales que vinculan cada eslabón, (c) los supuestos críticos que deben cumplirse para que la cadena opere, (d) los horizontes temporales en que se esperan efectos, (e) las condiciones contextuales que moderan efectividad. Por ejemplo, un programa de capacitación laboral podría postular: capacitaciones (actividad) → competencias técnicas adquiridas (producto) → mayor empleabilidad (resultado) → salida de pobreza (impacto). Esta cadena asume: (a) que las competencias enseñadas corresponden a demandas del mercado laboral local, (b) que beneficiarios pueden aplicar competencias sin barreras adicionales (cuidado infantil, transporte, discriminación), (c) que existe demanda laboral suficiente, (d) que empleos accesibles generan ingresos por sobre línea de pobreza. La evaluación de diseño interroga cada eslabón y supuesto. ¿Las actividades efectivamente generan los productos previstos? ¿Existe evidencia de que esos productos inducen los resultados esperados? ¿Los supuestos son plausibles dado el contexto? Teorías de cambio con eslabones débiles o supuestos implausibles predicen fracaso incluso con implementación impecable. 5.2.5 5. Componentes e intervenciones Este análisis examina si las prestaciones, servicios o transferencias que el programa proveerá son suficientes, coherentes y complementarias para lograr objetivos. Suficiencia implica que componentes cubren las causas relevantes del problema. Coherencia significa que componentes no se contradicen entre sí. Complementariedad indica que componentes se refuerzan mutuamente. Un programa para reducir deserción escolar podría incluir: (a) becas monetarias, (b) apoyo psicosocial, (c) nivelación académica, (d) orientación vocacional. Evaluación de diseño pregunta: ¿Estos componentes abordan las causas identificadas de deserción? ¿Alguna causa importante queda sin intervención? ¿Los componentes se refuerzan o generan tensiones? ¿La intensidad de cada componente es apropiada? Riesgos incluyen sobrediseño (componentes excesivos que exceden capacidad de gestión o presupuesto), subdiseño (intervención insuficiente que no modificará factores causales), incoherencia (componentes que operan en direcciones contradictorias) y desbalance (sobre-inversión en algunos componentes, sub-inversión en otros críticos). 5.2.6 6. Recursos y factibilidad operacional El componente final evalúa si recursos financieros, humanos, tecnológicos e institucionales son apropiados a los objetivos y componentes. Esto incluye: (a) suficiencia presupuestaria considerando costos unitarios realistas, (b) disponibilidad de personal calificado, (c) infraestructura y sistemas requeridos, (d) capacidades institucionales del ejecutor. Evaluación de diseño debe contrastar presupuesto propuesto con benchmarks de programas similares, verificar factibilidad de costos unitarios proyectados, identificar brechas de capacidades institucionales y evaluar si horizonte de implementación es realista. Programas ambiciosos con presupuestos insuficientes, plazos irrealistas o ejecutores sin capacidades demostradas enfrentan alta probabilidad de fracaso operacional. 5.3 Metodología de evaluación de diseño La evaluación de diseño combina múltiples métodos analíticos. Análisis documental examina documentos de diseño del programa (fundamentación, marcos lógicos, presupuestos), políticas sectoriales, estadísticas oficiales sobre el problema, evaluaciones de programas similares y literatura especializada sobre efectividad de intervenciones análogas. Entrevistas con stakeholders clave —gestores del programa, autoridades sectoriales, expertos temáticos, representantes de población objetivo— proporcionan perspectivas complementarias sobre viabilidad, pertinencia y potenciales obstáculos. Estas entrevistas no reemplazan análisis técnico sino que lo enriquecen con conocimiento experiencial. Benchmarking internacional compara el diseño propuesto con programas similares implementados en otros contextos, identificando mejores prácticas, errores frecuentes y factores críticos de éxito. Chile cuenta con ventaja comparativa en acceso a experiencias regionales documentadas por organismos como BID, CEPAL y Banco Mundial. Análisis de consistencia lógica verifica coherencia interna del diseño mediante herramientas como la Matriz de Marco Lógico y el árbol de problemas. Este análisis identifica inconsistencias, eslabones causales débiles, supuestos implausibles y brechas de diseño. Evaluación de viabilidad examina factibilidad política (¿existe respaldo político sostenido?), técnica (¿capacidades institucionales son suficientes?), administrativa (¿procedimientos son operacionalizables?) y financiera (¿recursos son realistas y sostenibles?). 5.4 Criterios de juicio en evaluación de diseño La evaluación de diseño emplea criterios específicos para emitir juicios fundamentados: Pertinencia: ¿Los objetivos corresponden al problema y necesidades de beneficiarios? ¿Se alinean con políticas superiores? Coherencia interna: ¿Existe lógica causal entre problema, objetivos, componentes y recursos? ¿Los componentes son complementarios? Coherencia externa: ¿El programa se articula apropiadamente con otras intervenciones? ¿Evita duplicaciones o contradicciones? Viabilidad política: ¿Existe respaldo político sostenido? ¿Los stakeholders relevantes apoyan el diseño? Viabilidad técnica: ¿Las intervenciones propuestas tienen respaldo en evidencia? ¿Son técnicamente factibles? Viabilidad administrativa: ¿El ejecutor cuenta con capacidades? ¿Los procedimientos son operacionalizables? Suficiencia de recursos: ¿El presupuesto es apropiado? ¿Los plazos son realistas? 5.5 Caso aplicado: Evaluación de diseño del Programa de Acompañamiento y Acceso Efectivo (PACE) El PACE, implementado desde 2014, busca restituir derecho a educación superior de estudiantes talentosos de establecimientos vulnerables mediante preparación académica y cupos garantizados en universidades. Su evaluación de diseño ex-ante habría examinado: Diagnóstico: Datos DEMRE mostraban que estudiantes de liceos vulnerables accedían marginalmente a educación superior incluso con rendimiento académico similar a pares de colegios privilegiados. Causas: menor preparación PSU, auto-exclusión por percepciones de no pertenencia, restricciones financieras. Pertinencia de objetivos: Objetivo de incrementar acceso y retención de estudiantes vulnerables en educación superior responde al problema diagnosticado y se alinea con compromisos de equidad educacional del gobierno. Población objetivo: Estudiantes del 15% de liceos de mayor vulnerabilidad que cumplan requisitos de participación. Cuantificación basada en matrículas administrativas (~ 25,000 estudiantes/cohorte). Alcanzabilidad mediante selección de establecimientos desde bases MINEDUC. Teoría de cambio: Preparación académica + cupo garantizado → competencias académicas + eliminación de barrera de acceso → ingreso y permanencia en educación superior → movilidad social. Supuestos: (a) preparación académica efectivamente desarrolla competencias, (b) cupo garantizado sin PSU no compromete desempeño universitario, (c) estudiantes cuentan con recursos para mantenerse una vez matriculados. Componentes: (a) Preparación académica desde 3° medio (talleres, tutorías), (b) apoyo psicoeducativo, (c) cupo garantizado en universidad adscrita, (d) acompañamiento en educación superior. Evaluación preguntaría: ¿Estos componentes abordan causas de inequidad de acceso? ¿Alguna causa relevante queda sin intervención? (Por ejemplo: restricciones financieras de mantención) Recursos y viabilidad: Presupuesto por estudiante, capacidades de liceos para implementar preparación, compromisos de universidades de sostener cupos, sistemas de seguimiento de trayectorias. Una evaluación de diseño robusta habría identificado fortalezas (articulación preparación-acceso-acompañamiento, compromiso institucional de universidades) y debilidades (dependencia crítica de calidad de implementación en liceos, riesgo de deserción por restricciones financieras no cubiertas, complejidad de coordinación con múltiples universidades). 5.6 Limitaciones y complementariedades La evaluación de diseño tiene limitaciones inherentes. Opera con supuestos y proyecciones, no con evidencia empírica de implementación efectiva. Un diseño técnicamente impecable puede fracasar por factores contextuales impredecibles, resistencias organizacionales o cambios en condiciones externas. Inversamente, programas con diseños deficientes ocasionalmente logran resultados por factores no anticipados. Estas limitaciones demandan complementariedad con evaluaciones de procesos (que examinan implementación efectiva), resultados (que miden logros observados) e impacto (que estiman efectos causales). El ciclo evaluativo completo requiere múltiples tipos de evaluación en diferentes momentos, no sustitución de evaluación ex-post por ex-ante. No obstante, en contextos de recursos limitados, evaluaciones de diseño bien conducidas previenen errores costosos y orientan inversiones hacia intervenciones con fundamentos sólidos. Como argumenta CNEP (2023), el sistema chileno ha privilegiado históricamente evaluación ex-post sobre ex-ante, generando aprendizajes tardíos cuando recursos ya se han comprometido. Fortalecer evaluación de diseño constituye prioridad para mejora del sistema. 5.7 Preguntas de reflexión Seleccione un programa social chileno reciente. Evalúe la coherencia entre su diagnóstico del problema y los objetivos propuestos. ¿Los objetivos responden a las causas fundamentales o solo a síntomas? Considere el programa Ingreso Ético Familiar. Construya su teoría de cambio identificando actividades, productos, resultados e impactos esperados. ¿Qué supuestos críticos debe cumplirse para que la cadena causal opere? ¿Por qué una intervención puede tener un diseño técnicamente sólido pero fracasar en implementación? Identifique 3 factores que podrían explicar esta brecha entre diseño e implementación. Compare las ventajas y limitaciones de invertir recursos en evaluación ex-ante (de diseño) versus evaluación ex-post (de impacto). ¿En qué contextos priorizaría cada una? 5.8 Referencias del capítulo DIPRES (2015). Evaluación Ex-Post: Conceptos y Metodologías. Ministerio de Hacienda. Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico para la planificación, el seguimiento y la evaluación de proyectos y programas. CEPAL. CNEP (2023). Actualización y reenfoque al sistema de evaluación de programas públicos. Comisión Nacional de Evaluación y Productividad. Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. "],["árbol-de-problemas-metodología-cepal.html", "Capítulo 6 Árbol de problemas: metodología CEPAL 6.1 Concepto y propósito 6.2 Pasos metodológicos 6.3 Errores comunes 6.4 Del árbol de problemas al árbol de objetivos 6.5 Caso aplicado: Programa Acompañamiento Psicosocial 6.6 Ejercicio práctico 6.7 Preguntas de reflexión 6.8 Referencias", " Capítulo 6 Árbol de problemas: metodología CEPAL 6.1 Concepto y propósito El árbol de problemas es una herramienta visual que estructura jerárquicamente un problema central, sus causas y sus efectos. Desarrollado por CEPAL (Ortegón, Pacheco &amp; Prieto, 2005), permite identificar relaciones causales y priorizar puntos de intervención. No se trata de simple listado de problemas, sino de análisis causal que distingue síntomas, problema central, causas directas, causas raíz y efectos de diverso nivel. 6.2 Pasos metodológicos 6.2.1 1. Identificar problema central Definir con precisión el problema que la política busca resolver. Debe ser específico, verificable, relevante socialmente. Formular como estado negativo actual, no como ausencia de solución. Ejemplo correcto: “Alta tasa de deserción escolar en educación media” Ejemplo incorrecto: “Falta de programas de retención escolar” 6.2.2 2. Identificar causas Preguntar sistemáticamente: ¿Por qué ocurre este problema? Distinguir: - Causas directas: factores inmediatos - Causas indirectas: factores estructurales subyacentes Organizar jerárquicamente. Verificar lógica causal: ¿la causa propuesta efectivamente genera el problema? 6.2.3 3. Identificar efectos Preguntar: ¿Qué consecuencias genera este problema? Distinguir: - Efectos directos: consecuencias inmediatas - Efectos indirectos: consecuencias de segundo y tercer orden 6.2.4 4. Diagramar el árbol Representar visualmente con problema central al centro, causas abajo (raíces), efectos arriba (ramas). Usar flechas para indicar dirección causal. 6.2.5 5. Validar la lógica causal Verificar que cada relación causal sea plausible. Eliminar pseudo-causas. Revisar completitud: ¿faltan causas o efectos relevantes? 6.3 Errores comunes Confundir causa con síntoma: “Bajo rendimiento escolar” puede ser síntoma de problema más profundo Confundir causa con ausencia de solución: “Falta de programas X” no es causa sino ausencia de intervención Incluir múltiples problemas centrales: El árbol debe enfocarse en un problema principal Lógica causal débil: Relaciones causales que no resisten escrutinio Exceso de complejidad: Árboles con 20+ causas pierden utilidad analítica 6.4 Del árbol de problemas al árbol de objetivos Convertir cada problema en su estado positivo correspondiente: - Problema: “Alta deserción escolar” → Objetivo: “Reducir deserción escolar” - Causas se transforman en medios - Efectos se transforman en fines Este árbol de objetivos fundamenta el diseño de componentes de intervención. 6.5 Caso aplicado: Programa Acompañamiento Psicosocial [DESARROLLAR: Árbol de problemas completo para familias en situación de vulnerabilidad. Mostrar problema central, 3-4 causas directas con causas raíz, 3-4 efectos directos con efectos indirectos. Incluir diagrama Mermaid] 6.6 Ejercicio práctico Construir árbol de problemas para: “Baja cobertura de educación parvularia en zonas rurales” Identificar 3 causas directas Para cada causa directa, identificar 2 causas raíz Identificar 3 efectos directos Para cada efecto directo, identificar 1 efecto indirecto Diagramar 6.7 Preguntas de reflexión ¿Por qué es importante distinguir entre causas y síntomas en el diagnóstico de problemas públicos? Piense en un programa que conozca. ¿Su diseño responde a causas del problema o solo a síntomas? ¿Qué limitaciones tiene el árbol de problemas para capturar problemas complejos con múltiples causas interrelacionadas? 6.8 Referencias Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico. CEPAL. "],["marco-lógico-instrumento-de-diseño-y-evaluación.html", "Capítulo 7 Marco lógico: instrumento de diseño y evaluación 7.1 Origen y propósito 7.2 Estructura de la matriz 7.3 Lógica vertical: Teoría del cambio 7.4 Lógica horizontal: Medición de logros 7.5 Indicadores en el marco lógico 7.6 Supuestos críticos 7.7 Construcción de un marco lógico 7.8 Caso aplicado: Programa Alimentación Escolar 7.9 Limitaciones del marco lógico 7.10 Preguntas de reflexión 7.11 Referencias", " Capítulo 7 Marco lógico: instrumento de diseño y evaluación 7.1 Origen y propósito La Matriz de Marco Lógico (MML) surge en años 1960s en USAID como herramienta de planificación y evaluación. CEPAL la adapta para América Latina (Ortegón, Pacheco &amp; Prieto, 2005), consolidándola como estándar regional. La MML sintetiza en formato tabular: objetivos jerárquicos, indicadores de logro, medios de verificación y supuestos críticos de una intervención. Su valor está en obligar explicitación de teoría de cambio y establecer base mensurable para evaluación. 7.2 Estructura de la matriz La MML organiza información en 4 filas (niveles de objetivos) y 4 columnas: 7.2.1 Filas: Jerarquía de objetivos Fin: Objetivo de desarrollo de largo plazo al que contribuye el programa Propósito: Efecto directo esperable si el programa se ejecuta exitosamente Componentes: Productos/servicios que el programa debe entregar Actividades: Acciones requeridas para generar cada componente 7.2.2 Columnas Resumen narrativo: Descripción de objetivos Indicadores: Medidas verificables de logro Medios de verificación: Fuentes de información para cada indicador Supuestos: Factores externos críticos para éxito 7.3 Lógica vertical: Teoría del cambio La MML expresa teoría causal ascendente: - SI se ejecutan actividades → ENTONCES se generan componentes - SI se entregan componentes → ENTONCES se logra propósito - SI se logra propósito → ENTONCES se contribuye al fin Cada relación está condicionada por supuestos: factores externos que deben cumplirse. 7.4 Lógica horizontal: Medición de logros Para cada nivel de objetivo: - Indicador especifica QUÉ se medirá como evidencia de logro - Medio de verificación establece DÓNDE se obtendrá esa información - Supuesto identifica QUÉ condiciones externas deben darse 7.5 Indicadores en el marco lógico Indicadores deben cumplir criterios SMART: - Specific (específicos) - Measurable (medibles) - Achievable (alcanzables) - Relevant (relevantes) - Time-bound (temporalmente definidos) Tipos de indicadores según nivel: - Fin: Indicadores de impacto (cambios de largo plazo) - Propósito: Indicadores de resultado/efecto - Componentes: Indicadores de producto - Actividades: Indicadores de proceso 7.6 Supuestos críticos Supuestos son factores externos al control del programa pero necesarios para éxito. Ejemplos: - Contexto macroeconómico estable - Continuidad de políticas complementarias - Disponibilidad de contrapartes institucionales - Participación de beneficiarios Identificar supuestos críticos permite: 1. Evaluar riesgos ex-ante 2. Diseñar estrategias de mitigación 3. Interpretar correctamente fracasos (¿falló programa o supuestos?) 7.7 Construcción de un marco lógico 7.7.1 Paso 1: Del árbol de problemas al árbol de objetivos Convertir cada elemento del árbol de problemas en estado positivo. 7.7.2 Paso 2: Identificar jerarquía de objetivos Fin: ¿A qué objetivo superior contribuye? Propósito: Reformulación positiva del problema central Componentes: Medios transformados en productos/servicios Actividades: Acciones para generar cada componente 7.7.3 Paso 3: Formular indicadores Para cada nivel, definir 2-3 indicadores SMART con línea base y meta. 7.7.4 Paso 4: Identificar medios de verificación ¿Dónde están disponibles los datos? Registros administrativos, encuestas, bases de datos oficiales. 7.7.5 Paso 5: Identificar supuestos Para cada nivel, preguntar: ¿Qué factores externos podrían impedir que el siguiente nivel se alcance? 7.8 Caso aplicado: Programa Alimentación Escolar [DESARROLLAR CASO COMPLETO: MML de programa PAE con todos sus componentes] Ejemplo estructura: | Resumen Narrativo | Indicadores | Medios Verificación | Supuestos | |——————-|————-|———————|———–| | FIN: Mejorar estado nutricional de niños vulnerables | % de niños con malnutrición | Encuesta MINSAL | Políticas de salud escolar continúan | | PROPÓSITO: Asegurar alimentación adecuada en horario escolar | [completar] | [completar] | [completar] | | etc. | | | | 7.9 Limitaciones del marco lógico Linealidad excesiva: Asume relaciones causales simples; dificultad con intervenciones complejas Rigidez: Una vez definido, incentiva resistencia a adaptaciones necesarias Foco en programas individuales: Dificultad para capturar sinergias entre programas Énfasis en lo medible: Riesgo de ignorar dimensiones cualitativas importantes 7.10 Preguntas de reflexión Seleccione un programa. Construya su marco lógico identificando fin, propósito, 3 componentes y actividades clave. ¿Por qué es importante distinguir entre propósito (resultado directo) y fin (impacto de largo plazo)? ¿Qué sucede si un programa cumple todos sus componentes pero no logra su propósito? ¿Qué podría explicarlo? 7.11 Referencias Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico. CEPAL. Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. "],["indicadores-de-desempeño-y-líneas-base.html", "Capítulo 8 Indicadores de desempeño y líneas base 8.1 Concepto de indicador 8.2 Tipología de indicadores 8.3 Criterios SMART 8.4 Línea base 8.5 Construcción de indicadores 8.6 Errores comunes 8.7 Caso: Sistema de indicadores para programa de capacitación laboral 8.8 Ejercicio práctico 8.9 Preguntas de reflexión 8.10 Referencias", " Capítulo 8 Indicadores de desempeño y líneas base 8.1 Concepto de indicador Un indicador es una medida que permite evaluar el desempeño de un programa en relación con objetivos. Según Bonnefoy y Armijo (2005), un buen indicador cumple funciones de: medición, comparación (con metas o benchmarks), y señalización (alertar sobre desviaciones). 8.2 Tipología de indicadores 8.2.1 Por dimensión evaluada Economía: Costo de recursos (ej: costo unitario por beneficiario) Eficiencia: Relación producto/insumo (ej: beneficiarios atendidos por $ millón) Eficacia: Grado de cumplimiento de metas (ej: % de meta de cobertura lograda) Calidad: Atributos de productos/servicios (ej: % de usuarios satisfechos) 8.2.2 Por nivel de objetivo (marco lógico) Insumos: Recursos asignados Procesos: Actividades ejecutadas Productos: Bienes/servicios entregados Resultados: Efectos en beneficiarios Impactos: Cambios de largo plazo 8.3 Criterios SMART Specific: Específico sobre qué mide Measurable: Cuantificable objetivamente Achievable: Alcanzable con recursos disponibles Relevant: Pertinente al objetivo que representa Time-bound: Con período de medición definido 8.4 Línea base La línea base es la medición inicial del indicador, previa a la intervención. Establece el punto de partida contra el cual se medirá cambio. Sin línea base confiable, es imposible atribuir cambios al programa versus tendencias preexistentes. 8.4.1 Fuentes para líneas base Registros administrativos del programa Encuestas nacionales (CASEN, ENE, etc.) Bases de datos sectoriales (MINSAL, MINEDUC, etc.) Estudios ad-hoc si datos secundarios no existen 8.5 Construcción de indicadores 8.5.1 Paso 1: Identificar el objetivo a medir ¿Qué aspecto específico del desempeño se busca evaluar? 8.5.2 Paso 2: Formular el indicador Definir fórmula clara. Ejemplos: - (Nº de beneficiarios atendidos / Nº de beneficiarios meta) * 100 - Costo total del programa / Nº de beneficiarios 8.5.3 Paso 3: Establecer línea base y meta Línea base: Valor inicial Meta: Valor esperado al término del período 8.5.4 Paso 4: Definir medio de verificación ¿De dónde provendrán los datos? ¿Con qué frecuencia se actualizarán? 8.5.5 Paso 5: Asignar responsable ¿Quién generará y reportará el indicador? 8.6 Errores comunes Indicadores no medibles: “Mejorar bienestar” sin especificar cómo se mide Confundir actividades con resultados: “N° de talleres realizados” no mide cambio en beneficiarios Metas irrealistas: Proponer mejoras del 100% en un año Falta de línea base: No se sabe desde dónde se parte Indicadores sin fuente verificable: No existe dato disponible 8.7 Caso: Sistema de indicadores para programa de capacitación laboral [DESARROLLAR TABLA: - 2 indicadores de producto - 2 indicadores de resultado - 1 indicador de eficiencia Con fórmulas, línea base, meta, medio de verificación] 8.8 Ejercicio práctico Para el Programa de Alimentación Escolar (PAE): 1. Formule 1 indicador de cobertura (producto) 2. Formule 1 indicador de resultado (nutricional) 3. Formule 1 indicador de eficiencia 4. Para cada uno, establezca: fórmula, línea base hipotética, meta, medio de verificación 8.9 Preguntas de reflexión ¿Por qué medir solo productos (ej: becas entregadas) es insuficiente para evaluar éxito de un programa? Un programa logra 100% de su meta de cobertura pero beneficiarios no mejoran. ¿Qué podría estar fallando? ¿Cuándo es aceptable no tener línea base para un indicador? 8.10 Referencias Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. DIPRES (2015). Evaluación Ex-Post: Conceptos y Metodologías. Ministerio de Hacienda. "],["sistemas-de-seguimiento-y-monitoreo.html", "Capítulo 9 Sistemas de seguimiento y monitoreo 9.1 Diferencia entre monitoreo y evaluación 9.2 Componentes de un sistema de monitoreo 9.3 Monitoreo en el ciclo de gestión 9.4 Diseño de un sistema de monitoreo: Pasos metodológicos 9.5 Herramientas digitales para monitoreo 9.6 Caso: Sistema de monitoreo del Programa Familias, Seguridades y Oportunidades 9.7 Errores comunes en sistemas de monitoreo 9.8 Ejercicio práctico 9.9 Referencias", " Capítulo 9 Sistemas de seguimiento y monitoreo 9.1 Diferencia entre monitoreo y evaluación Monitoreo: Seguimiento continuo y sistemático de actividades, productos y uso de recursos durante la implementación de un programa. Pregunta central: ¿Se está ejecutando según lo planificado? Evaluación: Análisis periódico y en profundidad de resultados e impactos logrados por la intervención. Pregunta central: ¿Se están logrando los objetivos? ¿La intervención es efectiva y eficiente? 9.1.1 Características distintivas Aspecto Monitoreo Evaluación Frecuencia Continuo (mensual, trimestral) Episódica (anual, bienal, final) Enfoque Descriptivo (qué se hace) Analítico (qué se logra y por qué) Nivel Actividades y productos Resultados e impactos Responsable Unidad ejecutora del programa Evaluadores externos o internos Propósito Gestión operativa Aprendizaje y rendición de cuentas El monitoreo es continuo y descriptivo, enfocado en la gestión diaria. La evaluación es episódica y analítica, enfocada en determinar efectividad y causalidad. 9.2 Componentes de un sistema de monitoreo Un sistema de monitoreo robusto debe incluir los siguientes componentes: 9.2.1 1. Indicadores de seguimiento Indicadores de insumos: Recursos humanos, financieros y materiales asignados - Ejemplo: Presupuesto ejecutado mensual, personal contratado, equipamiento adquirido Indicadores de actividades: Acciones realizadas por el programa - Ejemplo: Talleres realizados, visitas domiciliarias efectuadas, controles de salud realizados Indicadores de productos: Bienes y servicios entregados a beneficiarios - Ejemplo: Personas capacitadas, subsidios entregados, kits distribuidos Indicadores de cobertura: Alcance de la población objetivo - Ejemplo: % de población objetivo inscrita, % de territorios priorizados atendidos 9.2.2 2. Fuentes de información Registros administrativos: Bases de datos de beneficiarios, sistemas de información gerencial Reportes de gestión: Informes de avance de unidades ejecutoras Sistemas de información financiera: Ejecución presupuestaria, rendiciones Encuestas de satisfacción: Percepción de beneficiarios sobre servicios recibidos 9.2.3 3. Periodicidad y responsables Mensual: Indicadores operacionales críticos (cobertura, ejecución presupuestaria) - Responsable: Jefes de unidades operativas Trimestral: Indicadores de productos y avance en metas - Responsable: Coordinadores regionales o territoriales Anual: Cumplimiento de metas anuales comprometidas - Responsable: Dirección del programa 9.2.4 4. Productos del monitoreo Tableros de control (dashboards): Visualización en tiempo real de indicadores clave Informes de avance: Reporte sistemático a autoridades y stakeholders Alertas tempranas: Señales automáticas cuando indicadores caen bajo umbrales Reportes de excepción: Análisis de desviaciones significativas 9.2.5 5. Uso de la información de monitoreo Correcciones operacionales: Ajustes en la implementación ante desviaciones Reasignación de recursos: Redistribución presupuestaria o de personal Mejora de procesos: Identificación de cuellos de botella y buenas prácticas Insumo para evaluación: Provisión de datos de proceso para evaluaciones posteriores 9.3 Monitoreo en el ciclo de gestión El monitoreo cumple un rol articulador en el ciclo de gestión por resultados: Planificación: Define líneas base y metas a monitorear Ejecución: Monitoreo continuo detecta desviaciones y permite correcciones Evaluación: Información de monitoreo alimenta evaluaciones de resultados e impacto Mejora: Hallazgos del monitoreo y evaluación informan rediseños Complementariedad monitoreo-evaluación: Programas sin sistemas de monitoreo robustos dificultan evaluaciones posteriores por carencia de datos de proceso. El monitoreo responde al “qué” (se ejecutó), la evaluación al “si” (funcionó) y “por qué” (mecanismos causales). 9.4 Diseño de un sistema de monitoreo: Pasos metodológicos 9.4.1 Paso 1: Identificar usuarios de la información ¿Quiénes necesitan información de monitoreo y para qué decisiones? 9.4.2 Paso 2: Seleccionar indicadores clave Priorizar indicadores críticos (máximo 10-15) que capturen dimensiones esenciales 9.4.3 Paso 3: Definir medios de verificación Identificar fuentes de datos confiables y accesibles 9.4.4 Paso 4: Establecer periodicidad y responsables Asignar roles claros de recolección, análisis y reporte 9.4.5 Paso 5: Diseñar productos de reporte Formatos de tableros, informes, alertas adaptados a usuarios 9.4.6 Paso 6: Institucionalizar el sistema Integrar el monitoreo en flujos de trabajo y procesos de decisión 9.5 Herramientas digitales para monitoreo Sistemas de información gerencial (MIS): Plataformas integradas para registro y seguimiento de beneficiarios, actividades y productos Tableros de control (Dashboards): Visualizaciones interactivas de indicadores clave (ej: Power BI, Tableau) Sistemas de alerta temprana: Notificaciones automáticas cuando indicadores cruzan umbrales críticos Aplicaciones móviles: Recolección de datos en terreno en tiempo real 9.6 Caso: Sistema de monitoreo del Programa Familias, Seguridades y Oportunidades El Subsistema de Seguridades y Oportunidades del Ministerio de Desarrollo Social de Chile implementa un sistema de monitoreo que incluye: Indicadores de cobertura: - Número de familias ingresadas mensualmente al programa - % de cobertura de población en situación de pobreza extrema por región - Tasa de egreso mensual de familias Indicadores de proceso: - Número de visitas domiciliarias realizadas por profesional de apoyo - % de familias con plan de intervención actualizado - Tiempo promedio entre ingreso y primera visita domiciliaria Indicadores de productos: - Número de familias con acceso efectivo a prestaciones (salud, educación, vivienda) - % de familias con al menos un miembro incorporado a programas de empleabilidad - Número de transferencias monetarias entregadas Tablero de control: El programa cuenta con un dashboard web que muestra en tiempo real: - Mapa de cobertura por comuna - Ejecución presupuestaria mensual vs programado - Alertas de comunas con baja cobertura respecto a población objetivo Periodicidad: - Reporte mensual a nivel regional - Informe trimestral a nivel nacional con análisis de tendencias - Evaluación anual de cumplimiento de metas Uso: La información de monitoreo ha permitido: - Reasignar profesionales de apoyo a regiones con mayor demanda - Identificar comunas con barreras de acceso y diseñar estrategias focalizadas - Proveer insumos para evaluación de impacto del programa 9.7 Errores comunes en sistemas de monitoreo Exceso de indicadores: Monitorear demasiados indicadores diluye atención en lo crítico Falta de uso de la información: Recolectar datos sin incorporarlos en decisiones de gestión Periodicidad inadecuada: Informes muy frecuentes agobian, muy esporádicos llegan tarde Indicadores sin línea base ni meta: Imposibilita evaluar avance Ausencia de responsables claros: Nadie se hace cargo de recolección o análisis Desconexión con planificación: Monitorear actividades no vinculadas a objetivos del programa 9.8 Ejercicio práctico Diseñe un sistema de monitoreo básico para un programa de apoyo escolar a estudiantes de educación media en situación de vulnerabilidad. El programa incluye: - Tutorías personalizadas 2 veces por semana - Talleres grupales de técnicas de estudio - Entrega de material escolar Defina: 1. Cinco indicadores clave (insumos, actividades, productos, cobertura) 2. Fuente de datos para cada indicador 3. Periodicidad de medición 4. Un formato simple de tablero de control 9.9 Referencias Bonnefoy, J. C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL. Ortegón, E., Pacheco, J. F., &amp; Prieto, A. (2005). Metodología del marco lógico para la planificación, el seguimiento y la evaluación de proyectos y programas. CEPAL. Kusek, J. Z., &amp; Rist, R. C. (2004). Ten steps to a results-based monitoring and evaluation system. World Bank. "],["evaluación-de-procesos-y-gestión.html", "Capítulo 10 Evaluación de procesos y gestión 10.1 Concepto de evaluación de procesos 10.2 Preguntas típicas de una evaluación de procesos 10.3 Métodos de evaluación de procesos 10.4 Dimensiones clave de una evaluación de procesos 10.5 Utilidad de las evaluaciones de procesos 10.6 Estudio de caso: Evaluación de procesos del Subsistema Chile Crece Contigo 10.7 Diseño de una evaluación de procesos: Pasos metodológicos 10.8 Ejercicio práctico 10.9 Referencias", " Capítulo 10 Evaluación de procesos y gestión 10.1 Concepto de evaluación de procesos La evaluación de procesos examina CÓMO se implementa un programa, más allá de QUÉ resultados logra. Se enfoca en: Fidelidad al diseño: ¿El programa se implementa según lo planificado? Calidad de ejecución: ¿Los productos y servicios cumplen estándares de calidad? Eficiencia operacional: ¿Se utilizan recursos de manera óptima? Factores contextuales: ¿Qué obstáculos y facilitadores afectan la implementación? Objetivo central: Identificar brechas entre la implementación planificada y la implementación real, y comprender las razones de estas desviaciones. 10.1.1 Diferencia con evaluación de resultados Aspecto Evaluación de procesos Evaluación de resultados Pregunta ¿Cómo funciona el programa? ¿El programa logra sus objetivos? Enfoque Implementación y operación Cambios en beneficiarios Métodos Cualitativos y observacionales Cuantitativos y comparativos Timing Durante la implementación Al final o en hitos clave Uso Mejora operativa inmediata Decisión sobre continuidad/escalamiento 10.2 Preguntas típicas de una evaluación de procesos 10.2.1 Fidelidad de implementación ¿Las actividades se ejecutan según el cronograma y con la calidad esperada? ¿Los componentes del programa se implementan como se diseñaron? ¿Existen adaptaciones locales? ¿Son apropiadas o distorsionan la intervención? 10.2.2 Cobertura y focalización ¿Se alcanza a la población objetivo definida? ¿Existen filtración (beneficiarios no elegibles) o exclusión (elegibles no atendidos)? ¿Qué barreras enfrentan potenciales beneficiarios para acceder al programa? 10.2.3 Calidad de productos y servicios ¿Los beneficiarios reciben productos/servicios según los estándares del diseño? ¿Cuál es la experiencia de los beneficiarios en el acceso y uso de servicios? ¿La calidad de atención varía entre puntos de entrega o territorios? 10.2.4 Eficiencia operacional ¿Existen cuellos de botella que retrasan o encarecen la implementación? ¿Los costos unitarios son razonables comparados con programas similares? ¿Se aprovechan economías de escala o existen redundancias? 10.2.5 Factores contextuales ¿Qué factores facilitan u obstaculizan la implementación? ¿Cómo afectan el contexto político, institucional y social? ¿Cómo varía la implementación entre territorios o unidades ejecutoras? 10.3 Métodos de evaluación de procesos Las evaluaciones de procesos combinan métodos cualitativos y cuantitativos: 10.3.1 1. Análisis de registros administrativos Datos a analizar: - Cobertura real vs meta: Beneficiarios atendidos comparado con población objetivo - Tiempos de entrega: Desde postulación hasta recepción de beneficio - Costos unitarios: Costo por beneficiario atendido - Ejecución presupuestaria: Gasto real vs programado Ventajas: Datos completos de todos los beneficiarios, permite análisis territorial y temporal Limitaciones: Calidad de datos depende de sistema de registro, no captura experiencia de beneficiarios 10.3.2 2. Entrevistas a implementadores Actores clave a entrevistar: - Gestores del programa a nivel central - Coordinadores regionales o territoriales - Operadores de ventanilla o atención directa - Proveedores de servicios (cuando son tercerizados) Temas a explorar: - Percepción sobre funcionamiento del programa - Obstáculos operacionales enfrentados - Sugerencias de mejora desde la experiencia práctica - Variabilidad en implementación entre territorios 10.3.3 3. Observación directa Qué observar: - Visitas a puntos de atención (municipios, consultorios, escuelas) - Procesos de inscripción y entrega de servicios - Condiciones de infraestructura y equipamiento - Interacción entre funcionarios y beneficiarios Protocolo de observación: Pauta estructurada con dimensiones a observar y escalas de evaluación 10.3.4 4. Grupos focales con beneficiarios Participantes: 8-12 beneficiarios por grupo, idealmente homogéneos por tipo de beneficio o territorio Temas a explorar: - Experiencias de acceso: facilidad/dificultad para postular y recibir beneficios - Calidad de atención: trato, información recibida, oportunidad - Pertinencia de servicios: adecuación a necesidades - Sugerencias de mejora 10.3.5 5. Encuestas a beneficiarios Aplicación: Muestra representativa de beneficiarios para cuantificar satisfacción y experiencias Dimensiones típicas: - Satisfacción general con el programa - Oportunidad en la entrega de servicios - Calidad de atención recibida - Conocimiento sobre requisitos y procedimientos 10.3.6 6. Análisis de reclamos y sugerencias Fuentes: - Sistema de reclamos del programa - Contraloría General de la República - Redes sociales y prensa Análisis: Categorizar reclamos por tipo (acceso, calidad, trato) e identificar patrones 10.4 Dimensiones clave de una evaluación de procesos 10.4.1 A. Alcance y cobertura Cobertura efectiva vs nominal: ¿Cuántos de los elegibles efectivamente reciben el beneficio? Tasa de rechazo: % de postulaciones rechazadas y razones Tasa de deserción: % de beneficiarios que abandonan el programa Focalización: % de beneficiarios que cumplen criterios de elegibilidad 10.4.2 B. Calidad de implementación Fidelidad al diseño: % de componentes implementados según diseño Dotación de personal: Ratio beneficiarios/profesional comparado con estándar Infraestructura: Adecuación de espacios físicos y equipamiento Materiales: Disponibilidad y calidad de insumos necesarios 10.4.3 C. Eficiencia operativa Costo unitario: Costo total / beneficiarios atendidos Costo-efectividad: Costo por unidad de producto entregado Tiempos de proceso: Desde postulación hasta beneficio Ejecución presupuestaria: % del presupuesto ejecutado al final del año 10.4.4 D. Satisfacción de beneficiarios Satisfacción general: % de beneficiarios satisfechos o muy satisfechos Oportunidad: % que considera oportuna la entrega de servicios Trato: % que evalúa positivamente el trato recibido Información: % que recibió información clara sobre el programa 10.5 Utilidad de las evaluaciones de procesos Las evaluaciones de procesos son especialmente valiosas en los siguientes contextos: 10.5.1 1. Programas nuevos o pilotos Permiten detectar tempranamente problemas de implementación y realizar ajustes antes de escalar 10.5.2 2. Programas con resultados débiles Ayudan a identificar si el problema está en el diseño (teoría de cambio incorrecta) o en la ejecución (falla de implementación) Ejemplo: Un programa de capacitación laboral no logra inserción laboral de beneficiarios. La evaluación de procesos revela que: - Solo 30% de los capacitados completan el programa (alta deserción) - Las capacitaciones no se ajustan a demanda laboral local - Implicancia: El problema es de diseño e implementación, no solo de efectividad intrínseca 10.5.3 3. Programas descentralizados Capturan variabilidad en implementación entre territorios y identifican buenas prácticas replicables 10.5.4 4. Preparación para escalamiento Documentan condiciones necesarias para replicar el programa exitosamente en nuevos contextos 10.5.5 5. Rendición de cuentas sobre gestión Proveen evidencia sobre cumplimiento de compromisos operacionales 10.6 Estudio de caso: Evaluación de procesos del Subsistema Chile Crece Contigo Chile Crece Contigo (ChCC) es un sistema de protección integral a la infancia que acompaña a niños y niñas desde la gestación hasta los 9 años. Una evaluación de procesos realizada en 2012 examinó su implementación. 10.6.1 Metodología empleada Análisis de registros administrativos: Cobertura en los 345 municipios del país Encuesta a encargados municipales: 200 municipios Entrevistas a actores clave: Directores de programas a nivel central y regional Grupos focales: 20 grupos con familias beneficiarias en 10 comunas Observación en terreno: Visitas a consultorios y oficinas municipales 10.6.2 Hallazgos principales Cobertura: - 95% de los nacimientos del país ingresaron al sistema - Variabilidad territorial: Cobertura va de 78% en comunas rurales a 99% en urbanas - Brecha identificada: Dificultad para llegar a población migrante y rural dispersa Calidad de atención: - 78% de las familias evaluaron positivamente la atención recibida - Tiempos de espera para atención médica cumplían estándares en 65% de consultorios - Brecha identificada: Calidad heterogénea entre consultorios con y sin recursos adicionales Fidelidad de implementación: - Componente de salud (controles, vacunas) implementado en 100% de comunas - Componente de educación parental implementado solo en 45% de comunas - Brecha identificada: Débil articulación intersectorial en nivel local Eficiencia: - Costo promedio por niño atendido: $120.000 anuales - Gran variabilidad: $80.000 en comunas grandes vs $200.000 en comunas pequeñas - Hallazgo: Deseconomías de escala en comunas con pocos beneficiarios Factores facilitadores: - Compromiso político de autoridades locales - Dotación de profesionales especializados (enfermeras, educadoras) - Sistema de información integrado para seguimiento de niños Factores obstaculizadores: - Alta rotación de personal municipal por cambios de autoridades - Fragmentación de oferta programática entre ministerios - Escasa participación de padres en talleres (15% asistencia) 10.6.3 Recomendaciones derivadas Fortalecer articulación intersectorial mediante convenios formales entre sectores Estandarizar calidad a través de certificación de establecimientos de atención Focalizar recursos adicionales en comunas con mayores brechas de cobertura Rediseñar talleres parentales con horarios compatibles y contenidos pertinentes Estabilizar equipos mediante contratos de más largo plazo 10.6.4 Impacto de la evaluación La evaluación de procesos condujo a: - Creación de un sistema de acreditación de calidad para centros de atención - Reasignación presupuestaria hacia comunas con brechas de implementación - Rediseño del componente de educación parental con modalidades flexibles - Institucionalización de monitoreo trimestral de indicadores de proceso 10.7 Diseño de una evaluación de procesos: Pasos metodológicos 10.7.1 Paso 1: Definir objetivos y preguntas de evaluación ¿Qué aspectos de la implementación se quieren conocer? ¿Para qué decisiones se usará la información? 10.7.2 Paso 2: Desarrollar modelo de implementación Diagrama de cómo se supone que funciona el programa en la práctica (flujos, actores, procesos) 10.7.3 Paso 3: Seleccionar dimensiones e indicadores Definir qué se medirá en cada dimensión (cobertura, calidad, eficiencia, satisfacción) 10.7.4 Paso 4: Determinar métodos de recolección Combinar análisis de datos administrativos con métodos cualitativos (entrevistas, grupos focales, observación) 10.7.5 Paso 5: Muestreo Seleccionar territorios, unidades ejecutoras y beneficiarios a estudiar (representatividad y variabilidad) 10.7.6 Paso 6: Trabajo de campo Aplicar instrumentos, recolectar información, documentar observaciones 10.7.7 Paso 7: Análisis e interpretación Triangular hallazgos de distintas fuentes, identificar patrones, formular hipótesis sobre causas de brechas 10.7.8 Paso 8: Recomendaciones Proponer mejoras operacionales específicas y factibles 10.8 Ejercicio práctico Un programa municipal de apoyo a emprendedores entrega: - Capacitación en gestión empresarial (20 horas) - Asesoría técnica personalizada (5 sesiones) - Fondo concursable para capital semilla (hasta $2 millones) Diseñe una evaluación de procesos que incluya: Cinco preguntas de evaluación sobre diferentes dimensiones del proceso Métodos para responder cada pregunta (cuantitativos y cualitativos) Indicadores clave para medir cobertura, calidad y eficiencia Fuentes de información para cada indicador 10.9 Referencias Pignatta, M. A. (2015). Evaluación de procesos de programas sociales. CEPAL. DIPRES (2015). Guía metodológica: Evaluación de programas nuevos. Ministerio de Hacienda, Chile. Rossi, P. H., Lipsey, M. W., &amp; Henry, G. T. (2019). Evaluation: A systematic approach (8th ed.). Sage. Saunders, R. P., Evans, M. H., &amp; Joshi, P. (2005). Developing a process-evaluation plan for assessing health promotion program implementation: A how-to guide. Health Promotion Practice, 6(2), 134-147. "],["evaluación-de-resultados-criterios-oecd-dac.html", "Capítulo 11 Evaluación de resultados: Criterios OECD-DAC 11.1 Los seis criterios OECD-DAC 11.2 Aplicación práctica de criterios OECD-DAC 11.3 Caso: Aplicación de criterios OECD-DAC al Programa de Acompañamiento y Acceso Efectivo (PACE) 11.4 Ejercicio práctico 11.5 Referencias", " Capítulo 11 Evaluación de resultados: Criterios OECD-DAC La evaluación de resultados examina si un programa logró sus objetivos y generó los cambios esperados. A diferencia de la evaluación de procesos (que examina la implementación), la evaluación de resultados se enfoca en los cambios efectivos en beneficiarios y contexto. 11.1 Los seis criterios OECD-DAC El Comité de Ayuda al Desarrollo (DAC) de la OCDE desarrolló seis criterios estándar para evaluar programas de cooperación internacional. Estos criterios han sido adoptados ampliamente para evaluar políticas y programas públicos en general. Actualización 2019: La OCDE actualizó los criterios separando Pertinencia de Coherencia como dimensiones independientes. 11.1.1 1. Pertinencia (Relevance) Pregunta central: ¿Los objetivos del programa corresponden a las necesidades y prioridades reales de los beneficiarios y del contexto? Dimensiones a evaluar: Pertinencia ex-ante: ¿El diseño del programa respondía a un problema real identificado mediante diagnóstico riguroso? Pertinencia dinámica: ¿El programa se adaptó a cambios en necesidades y contexto durante su implementación? Pertinencia para beneficiarios: ¿Los servicios/productos ofrecidos son valorados y útiles para los beneficiarios? Alineamiento con prioridades: ¿El programa está alineado con prioridades sectoriales, regionales y nacionales? Métodos de evaluación: Análisis documental: Revisión de diagnósticos sectoriales, estudios de línea base, documentos de diseño Comparación con brechas actuales: Contraste entre problema que aborda el programa y situación actual del sector Consultas a beneficiarios: Encuestas o grupos focales sobre pertinencia de servicios recibidos Consultas a expertos: Entrevistas a especialistas del sector sobre prioridades actuales Ejemplo: Un programa de alfabetización digital para adultos mayores diseñado en 2010 enfocado en uso de computadores de escritorio puede haber perdido pertinencia en 2020, cuando la necesidad crítica es uso de smartphones para acceso a servicios públicos y trámites en línea. Indicadores típicos: - % de beneficiarios que declaran que el programa responde a sus necesidades prioritarias - Alineamiento entre objetivos del programa y brechas identificadas en diagnósticos sectoriales recientes - Grado de ajuste del programa a cambios en el contexto (flexibilidad) 11.1.2 2. Coherencia (Coherence) Pregunta central: ¿El programa es compatible con otras intervenciones similares y está alineado con políticas sectoriales? Dimensiones a evaluar: Coherencia interna: Consistencia entre objetivos, componentes y actividades del programa Coherencia externa: - Complementariedad: ¿El programa se complementa con otros programas para generar sinergias? - No duplicación: ¿El programa evita duplicar esfuerzos de otras intervenciones? - Alineamiento sectorial: ¿El programa es consistente con políticas sectoriales vigentes? - Articulación institucional: ¿El programa coordina con otras instituciones relevantes? Métodos de evaluación: Mapeo de intervenciones: Identificar programas que operan en misma población/territorio/sector Análisis de complementariedades: Evaluar si existen sinergias o duplicaciones con otros programas Revisión de políticas: Verificar alineamiento con marcos normativos y estratégicos sectoriales Entrevistas institucionales: Consultar a otras instituciones sobre coordinación y articulación Ejemplo: Un programa municipal de apoyo a microemprendedores puede tener baja coherencia si duplica servicios de SERCOTEC (servicio nacional), en lugar de complementarse focalizando en segmentos no atendidos o especializándose en rubros específicos. Indicadores típicos: - Número de instancias formales de coordinación con programas similares - % de beneficiarios que acceden simultáneamente a programas complementarios - Nivel de alineamiento con políticas sectoriales (análisis documental) 11.1.3 3. Eficacia (Effectiveness) Pregunta central: ¿El programa logró sus objetivos y metas comprometidas? Principio fundamental: La eficacia se mide comparando resultados esperados (establecidos en diseño) con resultados observados (medidos en evaluación). Dimensiones a evaluar: Cumplimiento de metas cuantitativas: ¿Se alcanzaron las metas numéricas de cobertura, productos y resultados? Logro de objetivos cualitativos: ¿Se produjeron los cambios esperados en beneficiarios? Eficacia diferencial: ¿La eficacia varía entre subgrupos, territorios o modalidades? Contribución relativa: Distinguir entre objetivos principales y secundarios Advertencia metodológica: Un programa puede cumplir metas secundarias pero fracasar en su objetivo principal. La eficacia debe evaluarse priorizando objetivos según su importancia relativa. Métodos de evaluación: Análisis de cumplimiento de metas: Comparar indicadores de producto y resultado con metas comprometidas Encuestas a beneficiarios: Medir cambios en variables de resultado (conocimientos, comportamientos, condiciones de vida) Análisis de heterogeneidad: Identificar para quiénes el programa fue más o menos eficaz Comparación con línea base: Medir cambio en indicadores desde situación inicial Ejemplo: Programa de mejoramiento de viviendas con objetivo de mejorar condiciones habitacionales y reducir enfermedades respiratorias en niños. Eficacia en objetivo primario (salud): Reducción de 15% en enfermedades respiratorias (meta: 20%) → Eficacia parcial Eficacia en objetivo secundario (satisfacción): 85% de beneficiarios satisfechos con mejoras (meta: 70%) → Meta superada Evaluación global: Eficacia moderada, pues no alcanzó plenamente objetivo principal Indicadores típicos: - % de cumplimiento de metas de resultado comprometidas - Cambio promedio en variables de resultado entre beneficiarios - Diferencia entre línea base y línea de cierre en indicadores clave 11.1.4 4. Eficiencia (Efficiency) Pregunta central: ¿Los resultados se lograron con un uso óptimo de recursos? Concepto: La eficiencia relaciona productos obtenidos con recursos utilizados. Un programa es eficiente si maximiza resultados con recursos dados, o minimiza costos para lograr resultados deseados. Tipos de análisis de eficiencia: 1. Eficiencia productiva: ¿Se minimizaron costos unitarios? Costo por beneficiario atendido Costo por unidad de producto entregado Comparación con estándares técnicos o programas similares 2. Eficiencia asignativa: ¿Los recursos se asignaron a usos que maximizan impacto? Distribución del presupuesto entre componentes Focalización de recursos en beneficiarios con mayor necesidad/potencial de cambio 3. Eficiencia temporal: ¿Los procesos operan sin demoras innecesarias? Tiempo desde postulación hasta recepción de beneficio Identificación de cuellos de botella Métodos de evaluación: Análisis de costos unitarios: Calcular costo por beneficiario, por producto, por resultado Benchmarking: Comparar costos y productividad con programas similares (nacional o internacional) Análisis de procesos: Identificar ineficiencias operacionales (duplicación, demoras, subutilización) Frontera de producción: Estimar máximo producto alcanzable con recursos dados Ejemplo: Programa de capacitación laboral Costo por capacitado: $500.000 por persona Benchmark nacional: Promedio de programas similares es $350.000 Hallazgo: Programa tiene baja eficiencia productiva (40% más caro que promedio) Explicación: Alta proporción de costos administrativos (30% vs 15% estándar) y grupos pequeños (10 personas vs 20 óptimo) Indicadores típicos: - Costo unitario por beneficiario o producto - Ratio costo administrativo / costo directo - Ejecución presupuestaria (% de presupuesto ejecutado) - Tiempo promedio de procesos críticos Herramientas avanzadas: Análisis Envolvente de Datos (DEA): Identifica unidades (territorios, establecimientos) más eficientes como referencia Análisis costo-efectividad: Costo por unidad de resultado logrado (ej: costo por estudiante que mejora rendimiento) 11.1.5 5. Impacto (Impact) Pregunta central: ¿Qué efectos de largo plazo y de amplio alcance ha generado el programa, más allá de sus resultados inmediatos? Diferencia entre resultado e impacto: Resultado: Cambio directo e inmediato en beneficiarios (ej: personas capacitadas) Impacto: Efecto de largo plazo, amplio y potencialmente indirecto (ej: reducción sostenida de pobreza en territorio) Dimensiones de impacto: Impacto en beneficiarios directos: Cambios sostenidos en condiciones de vida Impacto en no-beneficiarios: Efectos indirectos (spillovers) Impacto territorial: Cambios en comunidades o territorios completos Impacto sectorial: Transformaciones en sectores de política pública Desafío central: Atribución causal Para afirmar que el programa causó el impacto, es necesario distinguir: - Efectos del programa (atribuibles a la intervención) - Tendencias contextuales (que habrían ocurrido de todos modos) - Otros factores (cambios económicos, otras políticas) Métodos de evaluación de impacto: Ver Capítulo 12 para desarrollo detallado de diseños experimentales y quasi-experimentales. Indicadores típicos: - Cambios en indicadores de bienestar de mediano/largo plazo (ingresos, salud, educación) - Efectos en indicadores territoriales o sectoriales - Persistencia de cambios 2-5 años post-intervención 11.1.6 6. Sostenibilidad (Sustainability) Pregunta central: ¿Los beneficios del programa perdurarán después de que finalice la intervención? Dimensiones de sostenibilidad: 1. Sostenibilidad financiera - ¿Existen recursos comprometidos para continuar el programa o sus componentes? - ¿Se fortalecieron capacidades de generación de recursos locales? 2. Sostenibilidad institucional - ¿Se crearon estructuras organizacionales que perduran? - ¿Se instalaron procedimientos y sistemas de gestión? - ¿Hay apropiación institucional del programa? 3. Sostenibilidad técnica - ¿Se desarrollaron capacidades técnicas locales? - ¿Existen equipos formados que pueden continuar operando? - ¿Se transfirió conocimiento y metodología? 4. Sostenibilidad social - ¿Hay apropiación del programa por parte de beneficiarios y comunidad? - ¿Se fortalecieron organizaciones comunitarias? - ¿Hubo cambios en normas y prácticas sociales que persisten? 5. Sostenibilidad ambiental (cuando aplica) - ¿Las intervenciones son ambientalmente sostenibles? - ¿Se consideraron efectos ambientales de largo plazo? Métodos de evaluación: Seguimiento post-programa: Medir si beneficios se mantienen 1-3 años después del cierre Análisis institucional: Evaluar apropiación y capacidades instaladas Análisis financiero: Revisar compromisos presupuestarios futuros Consultas a actores: Entrevistas sobre expectativas de continuidad Ejemplo: Programa de fortalecimiento de centros comunitarios Alta sostenibilidad social: Organizaciones comunitarias apropiadas de gestión de centros Baja sostenibilidad financiera: Municipio no comprometió presupuesto para mantención Evaluación: Riesgo de deterioro de infraestructura a pesar de apropiación social Indicadores típicos: - % de componentes del programa que continúan operando post-cierre - Presupuesto comprometido para continuidad - Número de personal capacitado que permanece en funciones - Nivel de apropiación comunitaria/institucional (escala) 11.2 Aplicación práctica de criterios OECD-DAC La evaluación de resultados usando criterios OECD-DAC requiere operacionalizar cada criterio mediante: 11.2.1 Matriz de evaluación Criterio Pregunta evaluativa Indicadores Fuentes de evidencia Métodos Pertinencia ¿El programa responde a necesidades actuales? - % beneficiarios que valoran pertinencia- Alineamiento con diagnósticos - Encuesta beneficiarios- Análisis documental - Survey- Análisis comparativo Coherencia ¿El programa se complementa con otros? - Número de programas complementarios- Nivel coordinación - Entrevistas institucionales- Mapeo programas - Análisis de redes- Entrevistas Eficacia ¿Se cumplieron objetivos? - % cumplimiento metas- Cambio en variables resultado - Registros programa- Encuesta beneficiarios - Análisis estadístico- Pre-post Eficiencia ¿Se optimizaron recursos? - Costo unitario- Benchmark - Ejecución presupuestaria- Estudios comparativos - Análisis costos- Benchmarking Impacto ¿Efectos de largo plazo? - Cambios sostenidos bienestar- Efectos atribuibles - Datos panel- Grupo control - Quasi-experimento- DiD Sostenibilidad ¿Beneficios perduran? - Componentes que continúan- Apropiación local - Seguimiento post- Entrevistas - Estudios longitudinales- Análisis institucional 11.3 Caso: Aplicación de criterios OECD-DAC al Programa de Acompañamiento y Acceso Efectivo (PACE) El Programa PACE (desde 2014) busca restablecer igualdad de oportunidades en acceso a educación superior de estudiantes de establecimientos educacionales vulnerables mediante preparación académica y acceso garantizado a universidades. 11.3.1 Evaluación por criterios 1. PERTINENCIA: ALTA Evidencia: Chile presenta alta segregación educacional. Solo 15% de estudiantes de liceos vulnerables accede a educación superior (vs 60% de liceos privados) Hallazgo: PACE responde a brecha real y prioritaria en política educacional Indicador: 89% de estudiantes PACE declaran que preparación recibida fue pertinente para sus necesidades (Encuesta 2018) 2. COHERENCIA: MEDIA-ALTA Interna: Componentes (preparación académica + cupos garantizados) son coherentes y complementarios Externa: Se articula con Beca Nuevo Milenio y Gratuidad universitaria (complementariedad), pero genera tensión con sistema de admisión PSU/PDT (coherencia parcial) Evidencia: 45% de estudiantes PACE acceden a otros beneficios complementarios (becas, gratuidad) 3. EFICACIA: MEDIA Meta de acceso: 70% de beneficiarios que completan preparación acceden a educación superior Resultado observado: 55% accede (79% de cumplimiento de meta) Meta de titulación: 50% de ingresados se titula en tiempo esperado Resultado observado: 32% se titula (64% de cumplimiento) Hallazgo: Eficacia parcial. Logra aumentar acceso pero enfrenta desafíos en retención y titulación 4. EFICIENCIA: MEDIA-BAJA Costo por beneficiario: $1.200.000 anual por estudiante en preparación Benchmark: Programas similares de nivelación pre-universitaria cuestan $600.000-$800.000 Hallazgo: Costos elevados explicados por acompañamiento integral y acceso garantizado, pero existen ineficiencias en gestión territorial (duplicación de talleres, subutilización de cupos) Costo-efectividad: $3.750.000 por estudiante que accede a educación superior 5. IMPACTO: MEDIO (en evaluación) Impacto en beneficiarios: Estudiantes PACE tienen 25 puntos porcentuales más de probabilidad de acceder a ES que similares no-PACE (estudio cuasi-experimental 2019) Impacto territorial: Liceos PACE aumentaron matrícula en enseñanza media (efecto reputacional) Limitación: Faltan estudios de impacto en largo plazo (ingresos, movilidad social) 6. SOSTENIBILIDAD: MEDIA Financiera: Programa institucionalizado con presupuesto permanente (alta) Institucional: 29 universidades participantes han creado unidades especializadas (alta) Técnica: Liceos han incorporado metodologías de acompañamiento a prácticas regulares (media) Riesgo: Dependencia de financiamiento estatal; universidades no comprometen recursos propios (sostenibilidad financiera vulnerable a cambios de prioridades) 11.3.2 Síntesis evaluativa PACE Criterio Valoración Fortalezas Debilidades Pertinencia Alta Responde a brecha prioritaria - Coherencia Media-Alta Buena articulación con becas Tensión con sistema admisión Eficacia Media Aumenta acceso significativamente Baja titulación oportuna Eficiencia Media-Baja Acompañamiento integral Costos elevados, ineficiencias territoriales Impacto Medio Efecto causal demostrado en acceso Falta evidencia largo plazo Sostenibilidad Media Institucionalización en universidades Dependencia fiscal Recomendaciones derivadas: Eficiencia: Estandarizar gestión territorial para reducir duplicaciones y optimizar costos Eficacia: Fortalecer acompañamiento en educación superior (no solo en acceso) para mejorar retención Sostenibilidad: Incentivar cofinanciamiento de universidades participantes 11.4 Ejercicio práctico Considere un programa municipal de rehabilitación de espacios públicos (plazas, parques) en barrios vulnerables que operó durante 3 años con los siguientes componentes: Mejoramiento de infraestructura (juegos infantiles, bancas, áreas verdes) Talleres de apropiación comunitaria (formación de comités vecinales) Capacitación en mantención para dirigentes Datos disponibles: - Presupuesto ejecutado: $800 millones para 15 plazas - 2,500 familias beneficiadas directamente - 78% de satisfacción de usuarios - 12 comités vecinales creados (de 15 plazas) - Programa concluyó hace 1 año; 8 plazas mantienen buenas condiciones Tarea: Evalúe el programa aplicando los 6 criterios OECD-DAC: Para cada criterio: 1. Formule una pregunta evaluativa específica 2. Identifique qué información adicional necesitaría recolectar 3. Proponga al menos un indicador cuantitativo 4. Emita un juicio evaluativo preliminar (alto/medio/bajo) basado en datos disponibles Ejemplo para Pertinencia: - Pregunta: ¿La rehabilitación de plazas respondía a necesidades prioritarias de las comunidades? - Información adicional necesaria: Diagnóstico participativo pre-intervención, ranking de prioridades vecinales - Indicador: % de beneficiarios que declaran que mejoramiento de espacios públicos era necesidad prioritaria - Juicio preliminar: Medio (falta evidencia de diagnóstico participativo) Complete el análisis para los otros 5 criterios. 11.5 Referencias OECD-DAC (2019). Better Criteria for Better Evaluation: Revised Evaluation Criteria Definitions and Principles for Use. OECD Publishing. DIPRES (2017). Guías metodológicas para la evaluación ex-post de programas sociales. Ministerio de Hacienda, Chile. Gertler, P. J., Martinez, S., Premand, P., Rawlings, L. B., &amp; Vermeersch, C. M. (2016). Impact evaluation in practice (2nd ed.). World Bank. Rossi, P. H., Lipsey, M. W., &amp; Henry, G. T. (2019). Evaluation: A systematic approach (8th ed.). Sage Publications. CEPAL (2015). Manual de formulación y evaluación de proyectos sociales. Santiago de Chile. "],["evaluación-de-impacto-fundamentos.html", "Capítulo 12 Evaluación de impacto: Fundamentos 12.1 Concepto de evaluación de impacto 12.2 El problema fundamental del contrafactual 12.3 Sesgo de selección 12.4 Métodos experimentales: Experimentos Controlados Aleatorizados (RCT) 12.5 Métodos cuasi-experimentales 12.6 Comparación de métodos 12.7 Requerimientos para evaluación de impacto 12.8 Caso: Evaluación de impacto de la Subvención Escolar Preferencial (SEP) 12.9 Ejercicio práctico 12.10 Referencias", " Capítulo 12 Evaluación de impacto: Fundamentos 12.1 Concepto de evaluación de impacto La evaluación de impacto busca determinar los efectos causalmente atribuibles a una intervención pública. A diferencia de la evaluación de resultados (que mide cambios en beneficiarios), la evaluación de impacto establece que esos cambios fueron causados por el programa. Pregunta fundamental: ¿Qué habría ocurrido a los beneficiarios si no hubieran participado en el programa? Esta pregunta se refiere al contrafactual: la situación hipotética de los beneficiarios sin el programa. Definición formal de impacto: Impacto = Resultado observado (con programa) - Contrafactual (sin programa) 12.2 El problema fundamental del contrafactual Problema: No podemos observar simultáneamente a la misma persona con y sin tratamiento. Cada individuo solo puede estar en una de dos condiciones: Tratado (participa en el programa): observamos Y₁ No tratado (no participa): observamos Y₀ Pero nunca observamos ambos para la misma persona al mismo tiempo. 12.2.1 Ejemplo del problema María participó en un programa de capacitación laboral y consiguió empleo. Resultado observado: María tiene empleo Pregunta causal: ¿El empleo se debe a la capacitación o lo habría conseguido de todos modos? Contrafactual no observable: ¿Qué le habría pasado a María sin la capacitación? 12.2.2 Soluciones al problema del contrafactual Dado que no podemos observar el contrafactual individual, construimos grupos de comparación que se asemejen a los beneficiarios: Método experimental (RCT): Asignación aleatoria garantiza que tratados y control son estadísticamente idénticos antes del programa Métodos cuasi-experimentales: Construyen contrafactuales plausibles usando variación no aleatoria pero sistemática en la exposición al programa 12.3 Sesgo de selección Sesgo de selección: Error en la estimación del impacto cuando tratados y control difieren en características que afectan el resultado. Ejemplo: Comparar beneficiarios de capacitación laboral con población general. Beneficiarios probablemente tienen mayor motivación y mejores redes que llevaron a postular Si beneficiarios tienen mayor empleabilidad, comparación sobrestimará impacto del programa Tipos de sesgo de selección: Por características observables: Diferencias en edad, educación, ingresos (controlables mediante matching o regresión) Por características no observables: Diferencias en motivación, habilidades no medidas (requieren diseños más robustos) 12.4 Métodos experimentales: Experimentos Controlados Aleatorizados (RCT) 12.4.1 Principio fundamental La asignación aleatoria a tratamiento y control garantiza que los grupos son estadísticamente idénticos en todas las características (observables y no observables) antes del programa. Por lo tanto, cualquier diferencia post-intervención es atribuible al programa. 12.4.2 Diseño básico de RCT Definir población elegible: Identificar beneficiarios potenciales Aleatorización: Sorteo para asignar quién recibe el programa (tratamiento) y quién no (control) Línea base: Medir indicadores antes del programa en ambos grupos Implementación: Solo el grupo tratado recibe el programa Seguimiento: Medir indicadores post-programa en ambos grupos Estimación de impacto: Comparar resultados entre tratados y control Estimador de impacto: Impacto = E[Y₁ | Tratado] - E[Y₀ | Control] Donde E[·] denota valor esperado (promedio). 12.4.3 Ventajas de RCT Estándar de oro para inferencia causal Elimina sesgo de selección (observado y no observado) Estimación simple y directa del impacto Permite análisis de heterogeneidad de efectos 12.4.4 Limitaciones de RCT 1. Consideraciones éticas ¿Es ético negar beneficios a grupo control cuando el programa es claramente beneficioso? Solución: Aleatorización en fases (control accede después) o lotería cuando demanda excede oferta 2. Restricciones políticas y administrativas Resistencia a “sortear” beneficios en contexto de derechos sociales Programas universales (todos son elegibles) no permiten aleatorización 3. Costos y tiempos RCT requiere seguimiento longitudinal costoso Resultados tardan años en materializarse 4. Validez externa limitada Resultados de RCT en contexto piloto pueden no replicarse a escala masiva Efectos de equilibrio general no capturados 5. Contaminación y desgaste Spillovers: Grupo control recibe efectos indirectos del programa Atrición: Pérdida de participantes en seguimiento No cumplimiento: Tratados no participan, control accede al programa 12.4.5 Ejemplo de RCT: Progresa/Oportunidades (México) Programa: Transferencias monetarias condicionadas a asistencia escolar y controles de salud. Diseño RCT: - 506 localidades rurales elegibles - Aleatorización: 320 recibieron programa en 1998, 186 en 2000 (control temporal) - Mediciones: 1997 (línea base), 1998, 1999, 2000 Resultados: - Aumento de 3.4% en asistencia escolar primaria - Reducción de 12% en enfermedades infantiles - Impacto mayor en niñas y zonas más pobres Ventaja del diseño: Control eventualmente recibe programa (éticamente aceptable) 12.5 Métodos cuasi-experimentales Cuando la aleatorización no es factible o ética, los métodos cuasi-experimentales construyen contrafactuales plausibles explotando variación no aleatoria pero sistemática en la exposición al programa. 12.5.1 1. Diferencias en Diferencias (DiD) Idea: Comparar el cambio en el grupo tratado con el cambio en el grupo control. Supuesto clave: Tendencias paralelas - Sin el programa, tratados y control habrían evolucionado de manera similar. Estimador DiD: Impacto = [Y_tratado(post) - Y_tratado(pre)] - [Y_control(post) - Y_control(pre)] Ventaja: Controla por diferencias fijas entre grupos que no varían en el tiempo. Ejemplo: Evaluación de programa de mejoramiento de barrios Tratados: 50 barrios que recibieron mejoramiento en 2018 Control: 50 barrios similares sin mejoramiento Mediciones: 2017 (pre) y 2020 (post) Variable: Percepción de seguridad (escala 1-10) Grupo 2017 (pre) 2020 (post) Cambio Tratados 4.2 6.8 +2.6 Control 5.0 6.2 +1.2 Impacto DiD: 2.6 - 1.2 = +1.4 puntos Interpretación: El programa causó un aumento de 1.4 puntos en percepción de seguridad, más allá de la tendencia general. Test del supuesto de tendencias paralelas: Verificar que tratados y control tenían tendencias similares en períodos pre-tratamiento. 12.5.2 2. Regresión Discontinua (RD) Idea: Explotar umbrales de elegibilidad. Comparar individuos justo por encima y debajo del umbral (similares excepto en elegibilidad). Supuesto: Individuos cerca del umbral son similares; la asignación es “como si fuera aleatoria” en la vecindad del umbral. Ejemplo: Beca Indígena (Chile) Elegibilidad: Estudiantes indígenas con promedio ≥ 5.0 Comparación: Estudiantes con promedio 4.95-4.99 (no elegibles) vs 5.01-5.05 (elegibles) Variable de resultado: Años de escolaridad alcanzados Estimación: Discontinuidad (salto) en resultado en el umbral indica impacto causal. Ventajas: - No requiere aleatorización - Estimador creíble si diseño es riguroso Limitaciones: - Solo estima impacto para individuos cerca del umbral (validez externa limitada) - Requiere muestra grande en vecindad del umbral - Vulnerable a manipulación (individuos manipulan puntaje para ser elegibles) 12.5.3 3. Variables Instrumentales (VI) Idea: Utilizar una variable (instrumento) que afecta la participación en el programa pero NO afecta directamente el resultado. Requisitos del instrumento: 1. Relevancia: Instrumento afecta participación 2. Exogeneidad: Instrumento no afecta resultado excepto a través de participación Ejemplo: Evaluación de impacto de educación superior en ingresos Problema: Estudiantes universitarios difieren en habilidades no observadas (sesgo) Instrumento: Distancia a universidad más cercana Afecta probabilidad de estudiar (relevancia) No afecta ingresos directamente (exogeneidad) Estimación: Impacto de educación superior usando solo variación en educación causada por distancia. Limitación: Difícil encontrar instrumentos válidos. 12.5.4 4. Matching (Emparejamiento) Idea: Construir grupo control emparejando beneficiarios con no-beneficiarios de características observables similares. Métodos: Exact matching: Emparejar individuos idénticos en características observadas Propensity Score Matching (PSM): Emparejar según probabilidad estimada de participar (propensity score) Coarsened Exact Matching (CEM): Emparejar en categorías de características Ejemplo PSM: Evaluación de capacitación laboral Estimar probabilidad de participar en función de edad, educación, experiencia Emparejar cada capacitado con no-capacitado de propensity score similar Comparar tasas de empleo entre capacitados y sus pares emparejados Limitación crítica: Solo controla por características observables. Sesgo persiste si hay diferencias en factores no observados (motivación, habilidades). 12.6 Comparación de métodos Método Supuesto clave Ventajas Limitaciones Cuándo usar RCT Ninguno (aleatorización) Estándar de oro Costos, ética, política Programas piloto, lotería natural DiD Tendencias paralelas Controla diferencias fijas Requiere datos panel Rollout gradual, shocks exógenos RD Continuidad en umbral No requiere aleatorización Validez local Programas con umbrales VI Instrumento válido Controla no observables Difícil encontrar IV válido Variación exógena disponible Matching Selección en observables Datos transversales suficientes No controla no observables Datos ricos, selección observable 12.7 Requerimientos para evaluación de impacto 12.7.1 1. Teoría de cambio clara ¿Qué impactos se esperan? ¿En quiénes? ¿En qué plazo se materializarán? ¿Cuáles son los mecanismos causales? 12.7.2 2. Datos longitudinales de calidad Línea base: Medición antes del programa Seguimiento(s): Mediciones post-programa Datos de tratados y control 12.7.3 3. Variación en exposición al programa Algunos reciben el programa, otros no (al menos temporalmente) Variación debe ser exógena (no determinada por características de beneficiarios) 12.7.4 4. Tamaño muestral suficiente Poder estadístico: Capacidad de detectar efectos reales Muestras pequeñas generan estimaciones imprecisas Cálculo de poder: Depende de: - Tamaño del efecto esperado - Varianza del indicador - Nivel de significancia deseado 12.7.5 5. Tiempo suficiente para que impactos se materialicen Programas educativos: Impactos en ingresos tardan años Programas de salud: Efectos en morbilidad requieren seguimiento prolongado 12.8 Caso: Evaluación de impacto de la Subvención Escolar Preferencial (SEP) La Ley SEP (2008) entrega recursos adicionales a escuelas por cada estudiante vulnerable matriculado, condicionado a implementación de planes de mejoramiento educativo. 12.8.1 Desafío de evaluación Programa universal (todas las escuelas elegibles) No hubo aleatorización ni grupo control natural 12.8.2 Diseño cuasi-experimental: DiD con variación en timing Explotación de variación: Escuelas se incorporaron gradualmente entre 2008-2011 Estrategia: - Tratados tempranos: Escuelas que ingresaron en 2008 - Tratados tardíos (control temporal): Escuelas que ingresaron en 2010-2011 - Comparación: Evolución de rendimiento académico (SIMCE) antes y después de recibir SEP Datos: - SIMCE 2006-2013 - Registros administrativos de incorporación a SEP - Características de escuelas y estudiantes 12.8.3 Resultados Impacto en puntajes SIMCE (Matemáticas 4° básico): Tratados tempranos (SEP desde 2008): 2006: 240 puntos 2013: 252 puntos (+12 puntos) Tratados tardíos (SEP desde 2011): 2006: 238 puntos 2013: 244 puntos (+6 puntos) Impacto DiD: 12 - 6 = +6 puntos en SIMCE Interpretación: SEP causó un aumento de 6 puntos en puntajes SIMCE de matemáticas, equivalente a ~0.15 desviaciones estándar. 12.8.4 Heterogeneidad de impactos Mayor impacto en escuelas rurales (+9 puntos) Menor impacto en escuelas urbanas grandes (+4 puntos) Hipótesis: Escuelas pequeñas usan recursos de manera más focalizada 12.8.5 Limitaciones del estudio Supuesto de tendencias paralelas: No completamente verificable Efectos de largo plazo: Evaluación solo mide impactos hasta 5 años Efectos en otras dimensiones: Estudio se enfoca en aprendizajes, no en otros resultados (retención, clima escolar) 12.9 Ejercicio práctico Un municipio implementó un programa de inserción laboral para jóvenes (18-24 años) desempleados que incluye: - Orientación vocacional - Capacitación técnica (3 meses) - Intermediación laboral Datos disponibles: - 500 jóvenes participaron en 2022 - 2,000 jóvenes elegibles no participaron (lista de espera) - Encuesta de seguimiento en 2023: Tasa de empleo de participantes 65%, no-participantes 45% Preguntas: ¿Podemos concluir que el programa causó un aumento de 20 puntos porcentuales en empleo? ¿Por qué sí o no? ¿Qué sesgos podrían afectar esta comparación simple? Proponga dos diseños cuasi-experimentales que podrían mejorar la estimación del impacto Para cada diseño propuesto, indique: Grupo de tratamiento y control Supuesto clave Datos necesarios 12.10 Referencias Gertler, P. J., Martinez, S., Premand, P., Rawlings, L. B., &amp; Vermeersch, C. M. (2016). Impact evaluation in practice (2nd ed.). World Bank. Angrist, J. D., &amp; Pischke, J. S. (2009). Mostly harmless econometrics: An empiricist’s companion. Princeton University Press. Imbens, G. W., &amp; Wooldridge, J. M. (2009). Recent developments in the econometrics of program evaluation. Journal of Economic Literature, 47(1), 5-86. DIPRES (2015). Evaluación de impacto de programas públicos. Ministerio de Hacienda, Chile. Bravo, D., Mukhopadhyay, S., &amp; Todd, P. E. (2010). Effects of school reform on education and labor market performance: Evidence from Chile’s universal voucher system. Quantitative Economics, 1(1), 47-95. "],["métodos-cuantitativos-de-evaluación.html", "Capítulo 13 Métodos cuantitativos de evaluación 13.1 Fuentes de datos cuantitativos 13.2 Técnicas econométricas para evaluación de impacto 13.3 Inferencia estadística y pruebas de hipótesis 13.4 Análisis de heterogeneidad de efectos 13.5 Software estadístico para evaluación 13.6 Limitaciones de métodos cuantitativos 13.7 Caso: Evaluación cuantitativa del Ingreso Ético Familiar 13.8 Ejercicio práctico 13.9 Referencias", " Capítulo 13 Métodos cuantitativos de evaluación Los métodos cuantitativos utilizan datos numéricos y técnicas estadísticas para medir efectos de programas públicos. Este capítulo presenta las principales herramientas cuantitativas para evaluación, sus supuestos, aplicaciones y limitaciones. 13.1 Fuentes de datos cuantitativos 13.1.1 1. Encuestas de evaluación Definición: Cuestionarios diseñados específicamente para medir resultados del programa en beneficiarios y grupos de comparación. Componentes típicos: Variables de resultado (outcomes): Indicadores de los efectos que el programa busca generar Ejemplo educación: Puntajes en pruebas estandarizadas, años de escolaridad Ejemplo salud: Prevalencia de enfermedades, acceso a servicios Ejemplo empleo: Tasa de ocupación, ingresos laborales Variables de tratamiento: Participación en el programa, dosis recibida, duración Características sociodemográficas: Edad, sexo, educación, ingreso (para control estadístico) Variables de proceso: Satisfacción, conocimiento del programa Otras intervenciones: Participación en otros programas Diseño de muestreo: Muestreo aleatorio simple: Cada unidad tiene igual probabilidad de selección Muestreo estratificado: Dividir población en estratos y muestrear dentro de cada uno Muestreo por conglomerados: Muestrear grupos (escuelas, barrios) y luego individuos Tamaño muestral: Depende de: - Tamaño del efecto esperado - Varianza de la variable de resultado - Poder estadístico deseado (típicamente 80%) - Nivel de significancia (típicamente 5%) Ejemplo: Encuesta de evaluación de programa de transferencias monetarias Muestra: 2,000 beneficiarios + 2,000 no beneficiarios Variables de resultado: Ingreso familiar, gasto en alimentos, escolaridad de niños Tratamiento: Monto de transferencia recibido, meses en el programa Covariables: Edad jefe de hogar, educación, composición familiar, región 13.1.2 2. Registros administrativos Definición: Bases de datos generadas por la operación del programa o por sistemas públicos (registros civiles, impuestos, seguridad social). Ventajas: - Bajo costo (datos ya existen) - Cobertura completa (censo de beneficiarios) - Datos longitudinales (seguimiento en el tiempo) - Sin sesgo de respuesta (no hay cuestionarios) Limitaciones: - Variables limitadas a información administrativa - Calidad variable (errores, datos faltantes) - Difícil vincular múltiples bases de datos (problemas de identificación) Ejemplos en Chile: - Registro Social de Hogares: Caracterización socioeconómica de familias - Bases FONASA y MINSAL: Atenciones de salud - Bases MINEDUC: Matrícula, asistencia, rendimiento escolar - Bases SII: Ingresos declarados Vinculación de bases: Usar RUT (identificador único) para conectar información de distintas fuentes y construir variables de resultado. 13.1.3 3. Datos experimentales Definición: Datos generados por experimentos controlados aleatorizados (RCT) donde la asignación a tratamiento fue aleatorizada por los evaluadores. Características: - Línea base: Medición pre-tratamiento - Seguimientos: Mediciones post-tratamiento (1, 2, 5 años) - Grupo tratado y control comparables por aleatorización 13.2 Técnicas econométricas para evaluación de impacto 13.2.1 1. Regresión lineal múltiple Modelo básico: Y_i = β₀ + β₁·T_i + β₂·X_i + ε_i Donde: - Y_i: Variable de resultado para individuo i - T_i: Variable de tratamiento (1 si participa, 0 si no) - X_i: Vector de características observables (covariables) - β₁: Efecto del tratamiento (impacto) - ε_i: Error aleatorio Interpretación: β₁ mide el cambio promedio en Y asociado a participar en el programa, controlando por las características X. Supuesto clave: Selección en observables - Toda la diferencia entre tratados y control está capturada por las covariables X incluidas en el modelo. Limitación: Si hay características no observables (motivación, habilidad) que afectan tanto la participación como el resultado, β₁ estará sesgado. Ejemplo: Efecto de capacitación laboral en ingresos Ingreso_i = β₀ + β₁·Capacitación_i + β₂·Edad_i + β₃·Educación_i + β₄·Experiencia_i + ε_i Si β₁ = $150,000, interpretación: La capacitación aumenta ingresos en $150,000 anuales, controlando por edad, educación y experiencia. Problema potencial: Si capacitados tienen mayor motivación (no observable), β₁ sobrestima el efecto causal. 13.2.2 2. Propensity Score Matching (PSM) Idea: Emparejar cada beneficiario con un no-beneficiario de características observables similares para construir contrafactual. Pasos: Paso 1: Estimar Propensity Score El propensity score es la probabilidad de participar en el programa dado las características observables: P(T=1 | X) = P_i Se estima mediante regresión logística: Logit(P_i) = α₀ + α₁·Edad + α₂·Educación + α₃·Ingreso_basal + ... Paso 2: Emparejar tratados con controles Métodos de emparejamiento: Nearest neighbor: Emparejar cada tratado con el control de propensity score más cercano Kernel matching: Usar promedio ponderado de múltiples controles Caliper matching: Solo emparejar si diferencia en propensity score &lt; umbral Paso 3: Estimar impacto Comparar resultados entre tratados y sus controles emparejados: Impacto = (1/N) Σ [Y_tratado,i - Y_control,i] Supuesto clave: Independencia condicional - Condicional en X, la participación es “como si fuera” aleatoria. Ventajas: - Intuitivo: Compara “similares” - Reduce sesgo por características observables Limitaciones: - Solo controla por observables - Requiere soporte común: Tratados y controles deben tener rangos similares de propensity scores - Sensible a especificación del modelo de propensity score Ejemplo: Evaluación de subsidio habitacional Estimar P(recibir subsidio | edad, ingreso, tamaño familia, región) Emparejar familias subsidiadas con no-subsidiadas de propensity score similar Comparar hacinamiento, calidad de vivienda entre emparejados Diagnóstico: Balance de covariables - Verificar que tratados y controles emparejados sean similares en X. 13.2.3 3. Diferencias en Diferencias (DiD) Modelo: Y_it = β₀ + β₁·Tratado_i + β₂·Post_t + β₃·(Tratado_i × Post_t) + ε_it Donde: - i: Individuo - t: Tiempo (pre o post) - Tratado_i: 1 si pertenece a grupo tratado, 0 si control - Post_t: 1 si período post-tratamiento, 0 si pre-tratamiento - β₃: Efecto del programa (estimador DiD) Interpretación de coeficientes: β₁: Diferencia entre tratados y control en período pre (diferencia de nivel) β₂: Cambio temporal para grupo control (tendencia general) β₃: Diferencia en los cambios entre tratados y control = Impacto del programa Estimador DiD: β₃ = [E(Y|Tratado,Post) - E(Y|Tratado,Pre)] - [E(Y|Control,Post) - E(Y|Control,Pre)] Supuesto clave: Tendencias paralelas - En ausencia del programa, tratados y control habrían evolucionado de manera similar. Test del supuesto: Graficar evolución de Y para tratados y control en períodos pre-tratamiento. Si tendencias son paralelas antes del programa, supuesto es plausible. Variante: DiD con covariables Y_it = β₀ + β₁·Tratado_i + β₂·Post_t + β₃·(Tratado_i × Post_t) + β₄·X_it + ε_it Controla por cambios en características observables. Ejemplo: Impacto de programa de mejoramiento escolar Periodo Grupo Tratado (escuelas mejoradas) Grupo Control Diferencia 2018 (pre) 250 puntos SIMCE 248 puntos +2 2021 (post) 265 puntos 252 puntos +13 Cambio +15 +4 +11 Impacto DiD = 15 - 4 = +11 puntos en SIMCE Interpretación: El programa causó un aumento de 11 puntos en SIMCE, más allá de la tendencia general. 13.2.4 4. Regresión Discontinua (RD) Contexto: Programa tiene umbral de elegibilidad basado en variable continua (puntaje, ingreso, edad). Ejemplo: Beca universitaria para estudiantes con promedio ≥ 5.5 Idea: Comparar estudiantes justo por encima (5.51-5.60) con estudiantes justo por debajo (5.40-5.49) del umbral. Supuesto: En la vecindad del umbral, estudiantes son similares; única diferencia es elegibilidad para beca. Modelo RD - Sharp design (todos los elegibles reciben tratamiento): Y_i = α + τ·T_i + f(S_i) + ε_i Donde: - S_i: Running variable (promedio) - T_i: 1 si S_i ≥ umbral (elegible), 0 si no - f(S_i): Función que controla por efecto continuo de S sobre Y (típicamente polinomio) - τ: Efecto causal para individuos en el umbral Estimación gráfica: Graficar Y (eje vertical) contra S (eje horizontal) Ajustar funciones f(S) separadas para S &lt; umbral y S ≥ umbral Discontinuidad (salto) en el umbral = τ = Impacto causal Ventajas: - Identificación creíble (quasi-aleatorización en el umbral) - Fácil visualizar (gráfico de discontinuidad) - No requiere supuestos sobre características no observables Limitaciones: - Validez externa limitada: Solo estima impacto para individuos cerca del umbral - Requiere muestra grande en vecindad del umbral - Vulnerable a manipulación: Si individuos manipulan S para quedar sobre umbral, sesgo Test de manipulación: Verificar que densidad de S sea continua en el umbral (test de McCrary) Ejemplo chileno: Beca Indígena Umbral: Promedio de notas ≥ 5.0 Running variable: Promedio de notas Resultado: Años de escolaridad completados Estimación: Estudiantes con promedio 4.9-4.99 vs 5.0-5.1 Hallazgo: Beca aumenta escolaridad en 0.8 años para estudiantes en el umbral 13.2.5 5. Variables Instrumentales (VI) Problema: Sesgo por endogeneidad - variable de tratamiento correlacionada con error (características no observables). Solución: Usar instrumento Z que afecta participación pero NO afecta directamente el resultado. Requisitos del instrumento: Relevancia: Z afecta T (participación) Corr(Z, T) ≠ 0 Verificable empíricamente (primera etapa fuerte) Exogeneidad: Z NO afecta Y excepto a través de T Corr(Z, ε) = 0 No verificable directamente; requiere argumento conceptual Estimación en dos etapas (2SLS): Primera etapa: Predecir participación usando instrumento T_i = γ₀ + γ₁·Z_i + γ₂·X_i + u_i Obtener T_i predicho (T̂_i) Segunda etapa: Estimar efecto usando participación predicha Y_i = β₀ + β₁·T̂_i + β₂·X_i + ε_i β₁ es el efecto causal de T sobre Y. Ejemplo: Efecto de educación universitaria en ingresos Problema: Estudiantes universitarios tienen mayor habilidad (no observable) → Sesgo Instrumento: Distancia a universidad más cercana Relevancia: Mayor distancia → Menor probabilidad de estudiar (costo de transporte/mudanza) Exogeneidad: Distancia no afecta ingresos directamente (solo a través de educación) Estimación VI: Usar variación en educación causada solo por distancia Interpretación: β₁ estima efecto de educación para individuos cuya decisión de estudiar fue afectada por la distancia (LATE: Local Average Treatment Effect) Limitación: Difícil encontrar instrumentos que cumplan ambos requisitos. 13.3 Inferencia estadística y pruebas de hipótesis 13.3.1 Significancia estadística p-valor: Probabilidad de observar un efecto tan grande como el estimado si el efecto real fuera cero. Convenciones: - p &lt; 0.01: Altamente significativo () - p &lt; 0.05: Significativo () - p &lt; 0.10: Marginalmente significativo () - p ≥ 0.10: No significativo Advertencia: Significancia estadística ≠ Relevancia práctica Ejemplo: Programa aumenta ingresos en $1,000 anuales, p &lt; 0.01 (significativo). Pero si costo del programa es $10,000 por beneficiario, el efecto es irrelevante económicamente. 13.3.2 Intervalos de confianza Definición: Rango dentro del cual el efecto verdadero se encuentra con 95% de probabilidad. Ejemplo: Impacto = +$50,000, IC 95% = [$30,000, $70,000] Interpretación: Con 95% de confianza, el efecto verdadero está entre $30,000 y $70,000. Utilidad: Captura incertidumbre de la estimación. IC amplio → Estimación imprecisa. 13.3.3 Errores estándar robustos Problema: Errores estándar estándar asumen que observaciones son independientes. Realidad: En evaluación, observaciones suelen estar correlacionadas: - Estudiantes dentro de una misma escuela - Familias dentro de un mismo barrio - Individuos medidos repetidamente en el tiempo Solución: Errores estándar clustered (agrupados) Permiten correlación dentro de clusters (escuelas, barrios) Errores estándar más grandes → Tests más conservadores Regla: Si tratamiento se asigna a nivel de cluster (escuela recibe programa), errores estándar deben ser clustered a ese nivel. 13.4 Análisis de heterogeneidad de efectos Pregunta: ¿El programa tiene efectos diferentes para distintos subgrupos? Método: Estimar modelo con interacciones Y_i = β₀ + β₁·T_i + β₂·Mujer_i + β₃·(T_i × Mujer_i) + ε_i Interpretación: - β₁: Efecto para hombres - β₁ + β₃: Efecto para mujeres - β₃: Diferencia en efecto entre mujeres y hombres Ejemplo: Programa de capacitación laboral Efecto en hombres: +$80,000 anuales (β₁) Efecto en mujeres: +$120,000 anuales (β₁ + β₃) Diferencia: +$40,000 (β₃), p = 0.03 (significativa) Conclusión: Programa es más efectivo para mujeres. Subgrupos relevantes: - Género, edad, nivel educativo - Zona (urbana/rural) - Quintil de ingreso - Nivel de vulnerabilidad 13.5 Software estadístico para evaluación 13.5.1 R Ventajas: Gratuito, código abierto, comunidad activa, paquetes especializados Paquetes clave: - lm(): Regresión lineal - MatchIt: Propensity Score Matching - plm: Diferencias en diferencias con datos panel - rdrobust: Regresión discontinua - AER: Variables instrumentales (2SLS) 13.5.2 Stata Ventajas: Estándar en economía y evaluación, documentación extensa Comandos clave: - reg: Regresión lineal - psmatch2: Propensity Score Matching - xtreg: Datos panel / DiD - rdrobust: Regresión discontinua - ivregress: Variables instrumentales 13.5.3 Python Ventajas: Versátil, integración con machine learning Librerías: - statsmodels: Regresión, modelos econométricos - causalinfer: Propensity Score Matching - linearmodels: Panel data, IV 13.6 Limitaciones de métodos cuantitativos Reduccionismo: Reducen fenómenos complejos a variables medibles, pierden contexto y matices Caja negra: Miden si programa funciona, no explican cómo ni por qué Requerimientos de datos: Necesitan muestras grandes y datos de calidad Supuestos técnicos: Validez depende de supuestos (tendencias paralelas, exogeneidad) que no siempre son verificables Foco en efectos promedio: No capturan experiencias individuales ni casos atípicos Validez externa limitada: Resultados de un contexto pueden no generalizarse a otros Complementariedad: Métodos cuantitativos deben combinarse con evaluación cualitativa (entrevistas, etnografía) para comprender mecanismos causales y contexto. 13.7 Caso: Evaluación cuantitativa del Ingreso Ético Familiar El Ingreso Ético Familiar (IEF, 2012-2017) fue un programa de transferencias monetarias y apoyo psicosocial para familias en extrema pobreza en Chile. 13.7.1 Componentes del programa Transferencia monetaria: Bono base + bonos por logros (escolaridad, salud, trabajo) Apoyo psicosocial: Acompañamiento familiar Acceso preferente: Prioridad en otros programas sociales 13.7.2 Diseño de evaluación Método: Propensity Score Matching (PSM) Datos: - Registro Social de Hogares (características familias) - Base IEF (participantes) - Base empleos formales (Superintendencia de Pensiones) - Encuesta CASEN (ingresos, pobreza) Muestra: - 50,000 familias IEF (tratados) - 100,000 familias elegibles no-IEF (pool de controles) 13.7.3 Paso 1: Estimar Propensity Score Variables incluidas: - Escolaridad jefe de hogar - Edad jefe de hogar - Número de integrantes - Número de niños - Puntaje Ficha de Protección Social - Región - Zona (urbana/rural) Modelo: Regresión logística Resultado: Propensity scores entre 0.05 y 0.95 para tratados (buen soporte común) 13.7.4 Paso 2: Emparejamiento Método: Kernel matching con bandwidth = 0.06 Resultado: Cada familia IEF emparejada con promedio ponderado de 50-100 controles similares Test de balance: Después del matching, diferencias en covariables entre tratados y controles &lt; 5% (balance satisfactorio) 13.7.5 Paso 3: Estimación de impactos Resultados (2 años post-ingreso): Indicador IEF Control Impacto p-valor Ingreso familiar mensual $285,000 $245,000 +$40,000 &lt; 0.01 % con empleo formal (jefe hogar) 35% 28% +7 pp &lt; 0.01 % bajo línea pobreza 28% 38% -10 pp &lt; 0.01 Escolaridad niños (% asiste) 94% 91% +3 pp 0.04 Interpretación: IEF aumentó ingreso familiar en $40,000 mensuales (16% respecto a control) Aumentó empleo formal del jefe de hogar en 7 puntos porcentuales Redujo pobreza en 10 puntos porcentuales Mejoró levemente asistencia escolar 13.7.6 Heterogeneidad de efectos Efectos mayores en: - Zonas urbanas: +$50,000 (vs +$25,000 rural) - Familias con niños pequeños: -15 pp pobreza (vs -5 pp sin niños) - Jefas de hogar mujeres: +10 pp empleo formal (vs +4 pp hombres) 13.7.7 Limitaciones del estudio Selección en observables: PSM solo controla por características observadas; si motivación o redes afectan participación, sesgo persiste Efectos de mediano plazo: Evaluación mide impactos 2 años post-ingreso; efectos de largo plazo (5-10 años) desconocidos Efectos indirectos: No captura efectos en niños (desarrollo, aspiraciones) que emergerían en largo plazo 13.8 Ejercicio práctico Un programa de tutorías escolares se implementó en 100 escuelas municipales en 2020. Otras 150 escuelas municipales no recibieron el programa. Datos disponibles: - SIMCE 2019 (pre-programa) y 2022 (post) para todas las escuelas - Características de escuelas: Matrícula, % vulnerabilidad, docentes por estudiante - Identificador de escuelas tratadas vs control Tareas: Proponga un método cuantitativo para estimar el impacto del programa en puntajes SIMCE. Justifique su elección. Especifique el modelo econométrico que usaría (ecuación con variables y coeficientes) Identifique el supuesto clave del método y proponga un test para verificarlo Diseñe un análisis de heterogeneidad: ¿Para qué subgrupos de escuelas el programa podría ser más o menos efectivo? 13.9 Referencias Angrist, J. D., &amp; Pischke, J. S. (2009). Mostly harmless econometrics: An empiricist’s companion. Princeton University Press. Gertler, P. J., Martinez, S., Premand, P., Rawlings, L. B., &amp; Vermeersch, C. M. (2016). Impact evaluation in practice (2nd ed.). World Bank. Khandker, S. R., Koolwal, G. B., &amp; Samad, H. A. (2010). Handbook on impact evaluation: Quantitative methods and practices. World Bank. Wooldridge, J. M. (2010). Econometric analysis of cross section and panel data (2nd ed.). MIT Press. DIPRES (2015). Evaluación de impacto de programas públicos. Ministerio de Hacienda, Chile. "],["análisis-de-efectos-estructurales-y-sinergias.html", "Capítulo 14 Análisis de efectos estructurales y sinergias 14.1 Más allá del efecto directo: Complejidad de los impactos 14.2 Efectos directos vs indirectos (externalidades) 14.3 Efectos estructurales 14.4 Sinergias entre programas 14.5 Metodologías para capturar efectos complejos 14.6 Caso: Efectos territoriales del Programa Quiero Mi Barrio 14.7 Desafíos metodológicos en evaluación de efectos complejos 14.8 Ejercicio práctico 14.9 Referencias", " Capítulo 14 Análisis de efectos estructurales y sinergias 14.1 Más allá del efecto directo: Complejidad de los impactos Las evaluaciones tradicionales se enfocan en medir el efecto de un programa sobre sus beneficiarios directos. Sin embargo, las políticas públicas pueden generar efectos más complejos que trascienden esta relación lineal: Efectos indirectos sobre no-beneficiarios Efectos estructurales sobre instituciones y mercados Sinergias entre múltiples intervenciones Efectos de largo plazo que emergen años después Efectos heterogéneos que varían según contextos Este capítulo aborda métodos para identificar y medir estos efectos complejos. 14.2 Efectos directos vs indirectos (externalidades) 14.2.1 Efectos directos Definición: Cambios en los beneficiarios directos del programa atribuibles a su participación Ejemplo: Niños que reciben alimentación escolar mejoran su asistencia a clases (efecto en beneficiarios del programa) 14.2.2 Efectos indirectos (spillovers o externalidades) Definición: Cambios en no-beneficiarios generados por externalidades del programa Tipos de efectos indirectos: 1. Efectos de pares (peer effects) - Compañeros de aula de niños que reciben tutorías también mejoran rendimiento académico por efecto de aprendizaje colaborativo - Trabajadores no capacitados aumentan productividad al trabajar con colegas capacitados 2. Efectos de equilibrio general - Programa de empleabilidad aumenta oferta laboral y reduce salarios en sector específico - Subsidio habitacional aumenta precios de viviendas en zona beneficiada 3. Efectos de difusión social - Programa de salud reproductiva cambia normas sociales sobre planificación familiar en comunidad completa - Transferencias monetarias condicionadas modifican expectativas educacionales de familias no beneficiarias 4. Efectos en proveedores - Programa de alimentación escolar genera ingresos para productores locales de alimentos - Subsidios a la demanda (vouchers educacionales) incentivan mejora de oferta de servicios 14.2.3 Implicancias para evaluación Los efectos indirectos tienen importantes consecuencias metodológicas: Contaminación del grupo de control: Si el grupo de control recibe efectos indirectos del programa, la comparación tratado vs control subestima el efecto total Ejemplo: Un programa de vacunación genera inmunidad de rebaño. Niños no vacunados (grupo de control) también reducen contagios. El efecto medido (diferencia tratados-control) subestima el beneficio total del programa. Solución: Diseños que aíslan grupos de control (ej: aleatorización por conglomerados geográficamente distantes) 14.3 Efectos estructurales Definición: Cambios en instituciones, mercados, normas o prácticas que persisten más allá de la existencia del programa individual y afectan el funcionamiento de sistemas completos. 14.3.1 Tipos de efectos estructurales 1. Cambios institucionales Ejemplo: Programa de evaluación de desempeño docente crea institucionalidad permanente de carrera profesional docente Características: - Nuevas reglas, leyes o regulaciones - Creación de organismos o unidades especializadas - Cambios en procesos de toma de decisiones 2. Transformaciones de mercados Ejemplo: Subsidio de vivienda transforma mercado inmobiliario al crear segmento de vivienda económica Características: - Entrada de nuevos proveedores - Cambios en estructura de precios - Innovación en productos o servicios 3. Cambios en prácticas profesionales Ejemplo: Programa de capacitación docente en metodologías activas transforma currículo de formación inicial en universidades Características: - Adopción de nuevas metodologías o tecnologías - Cambios en estándares de calidad - Difusión de buenas prácticas 4. Transformación de normas sociales Ejemplo: Programa de prevención de violencia de género cambia percepciones sociales sobre roles de género Características: - Cambios en valores y creencias colectivas - Modificación de comportamientos socialmente aceptados - Redefinición de roles sociales 14.3.2 Identificación de efectos estructurales Métodos: Estudios longitudinales: Seguimiento 5-10 años post-intervención para observar cambios que perduran Análisis de trayectorias institucionales: Process tracing que documenta cómo el programa gatilló cambios en instituciones Comparación de sistemas: Contraste entre territorios donde el programa operó vs donde no operó, observando diferencias estructurales Métodos mixtos: Combinación de medición cuantitativa de cambios con análisis cualitativo de mecanismos causales 14.4 Sinergias entre programas Definición: Programas que operan complementariamente pueden generar efectos superiores a la suma de sus efectos individuales operando aisladamente. Formalización: Efecto de A + Efecto de B &lt; Efecto de (A+B) 14.4.1 Tipos de sinergias 1. Sinergias de refuerzo Programas que atacan múltiples dimensiones de un mismo problema de forma complementaria. Ejemplo: Chile Crece Contigo (desarrollo infantil) + Programas de empleo femenino - ChCC solo: mejora desarrollo infantil - Empleo femenino solo: aumenta ingreso familiar - ChCC + Empleo: Sinergia → Mayor desarrollo infantil (más recursos + autonomía materna) + Mayor inserción laboral femenina (acceso a cuidado infantil) 2. Sinergias de habilitación Un programa genera condiciones previas necesarias para que otro programa sea efectivo. Ejemplo: Programa de salud + Programa educacional - Niños con parásitos intestinales (desnutrición) no pueden aprender efectivamente - Programa de salud habilita que programa educacional logre impactos - Efecto educacional mayor cuando opera simultáneamente con salud 3. Sinergias de escala Múltiples programas comparten infraestructura, reduciendo costos unitarios. Ejemplo: Red de Protección Social compartiendo sistema de focalización (Registro Social de Hogares) - Múltiples programas usan misma plataforma de postulación y focalización - Reducción de costos administrativos y mejora de experiencia de usuario 14.4.2 Medición de sinergias: Diseño factorial Para medir sinergias, se requiere diseño experimental que compare: Grupo Programa A Programa B Efecto observado 1 (control puro) No No 0 2 Sí No E(A) 3 No Sí E(B) 4 Sí Sí E(A+B) Sinergia = E(A+B) - [E(A) + E(B)] Si sinergia &gt; 0: Complementariedad positiva Si sinergia &lt; 0: Sustitución o interferencia entre programas Desafío práctico: Diseño factorial requiere cuatro grupos, aumentando tamaño muestral y costo 14.4.3 Evidencia de sinergias en Chile Caso 1: Programa Puente + Subsidios Monetarios (2002-2006) Programa Puente (apoyo psicosocial a familias) operó en combinación con acceso preferente a subsidios. Hallazgo: Familias que recibieron solo Puente no mejoraron ingresos. Familias que recibieron solo subsidios tampoco salieron de pobreza de forma sostenible. Familias con Puente + subsidios lograron mayor reducción de pobreza. Mecanismo de sinergia: Puente generó “activación” y organización familiar, subsidios proveyeron recursos. Recursos sin activación no son efectivos; activación sin recursos tampoco. Caso 2: JUNAEB (alimentación escolar) + Subvención Escolar Preferencial Programa de alimentación (JUNAEB) opera hace décadas. Subvención Escolar Preferencial (SEP, desde 2008) entrega recursos adicionales a escuelas por estudiantes vulnerables. Hallazgo: Escuelas que recibieron SEP y ya operaban con JUNAEB lograron mayores mejoras en rendimiento académico que escuelas solo con SEP. Mecanismo: Alimentación resuelve barrera fisiológica del aprendizaje (desnutrición), SEP provee recursos para mejorar pedagogía. Sinergia entre condiciones nutricionales y calidad educativa. 14.5 Metodologías para capturar efectos complejos Las evaluaciones de efectos complejos requieren diseños que trascienden la comparación simple tratado/control: 14.5.1 1. Análisis de redes sociales Objetivo: Capturar efectos de difusión y contagio a través de redes Aplicación: - Identificar líderes de opinión que amplifican efectos de programas - Medir cómo comportamientos se difunden entre pares - Mapear flujos de información e influencia Ejemplo: Estudio de programa de microcrédito en India (Banerjee et al., 2013) usó análisis de redes para identificar que efectos del programa se difunden a través de líderes religiosos y sociales, alcanzando 2-3 veces más personas que beneficiarios directos. 14.5.2 2. Modelos multinivel (hierarchical models) Objetivo: Separar efectos a nivel individual, grupal (escuela, barrio) y territorial (comuna, región) Especificación: - Nivel 1: Características individuales (edad, educación) - Nivel 2: Características grupales (calidad de escuela, capital social del barrio) - Nivel 3: Características territoriales (índice de desarrollo comunal, inversión pública regional) Aplicación: Permite identificar si efectos de un programa varían según contexto (heterogeneidad contextual) Ejemplo: Evaluación de programa de mejoramiento de barrios puede mostrar que efectos en cohesión social son mayores en comunas con tradición de organización comunitaria (efecto multinivel). 14.5.3 3. Estudios longitudinales de largo plazo Objetivo: Seguimiento de beneficiarios 5, 10, 15 años post-intervención para capturar efectos que emergen en el largo plazo Desafíos: - Atrición: Dificultad para rastrear beneficiarios a lo largo del tiempo - Costos elevados de múltiples rondas de seguimiento - Cambio de prioridades políticas que descontinúan financiamiento Ejemplo paradigmático: Perry Preschool Project (EE.UU.) siguió a beneficiarios de educación preescolar hasta los 40 años, encontrando efectos sobre ingresos, criminalidad y salud que no eran visibles en evaluaciones de corto plazo. 14.5.4 4. Métodos mixtos (cuanti-cuali) Objetivo: Combinar medición cuantitativa de magnitud de efectos con comprensión cualitativa de mecanismos causales Diseño típico: 1. Fase cuantitativa: RCT o quasi-experimento para medir efecto promedio 2. Fase cualitativa: Entrevistas en profundidad y etnografía para entender cómo y por qué funciona el programa 3. Integración: Triangulación de hallazgos para explicar heterogeneidad de efectos Ejemplo: Evaluación de transferencias monetarias condicionadas que combina: - Encuestas para medir cambios en ingresos y escolaridad - Entrevistas para entender cómo familias toman decisiones de uso de transferencias - Observación etnográfica para capturar cambios en dinámicas familiares y comunitarias 14.5.5 5. Evaluaciones de efectos en equilibrio general Objetivo: Capturar efectos de programas masivos que modifican mercados completos Métodos: - Modelos de equilibrio general computable (CGE): Simulación de cómo programa afecta oferta, demanda y precios en múltiples sectores económicos - Diferencias en diferencias espaciales: Comparar territorios con distinta intensidad de implementación del programa Ejemplo: Evaluación de programa de empleabilidad masivo en Colombia estimó que el programa redujo salarios en 2% en sectores con alta oferta de capacitados (efecto de equilibrio general negativo sobre beneficiarios). 14.6 Caso: Efectos territoriales del Programa Quiero Mi Barrio El Programa de Recuperación de Barrios “Quiero Mi Barrio” (2006-presente) del Ministerio de Vivienda interviene barrios vulnerables mediante mejoramiento de espacios públicos, fortalecimiento comunitario y acceso a servicios. 14.6.1 Efectos multinivel identificados Nivel 1: Efectos en beneficiarios directos (residentes participantes) - Aumento en satisfacción con barrio: +25 puntos porcentuales - Aumento en participación en organizaciones comunitarias: +15 pp - Mejora en percepción de seguridad: +18 pp Nivel 2: Efectos en residentes no participantes del mismo barrio - Mejora en percepción de seguridad: +12 pp (spillover) - Aumento en valoración de vivienda: +8% (externalidad positiva) - Mayor uso de espacios públicos recuperados por toda la comunidad Nivel 3: Efectos territoriales - Reducción de rotación residencial (mayor estabilidad poblacional) - Aumento en inversión privada en comercio local (mercado inmobiliario) - Cambio en percepción externa del barrio (reducción de estigmatización territorial) 14.6.2 Sinergias con otros programas El programa operó en sinergia con: Programa de Habitabilidad (mejoramiento de viviendas individuales): - Sinergia: Espacios públicos mejorados + viviendas mejoradas = mayor efecto en satisfacción residencial que suma de ambos aislados Programas de seguridad ciudadana (cámaras, iluminación): - Sinergia: Recuperación de espacios + vigilancia = mayor reducción de delitos que intervenciones aisladas 14.6.3 Metodología empleada Diseño quasi-experimental: - Comparación de barrios intervenidos vs barrios en lista de espera - Matching por características pre-intervención (pobreza, delincuencia, infraestructura) Métodos mixtos: - Encuestas a 3,000 residentes (muestra representativa) - 30 entrevistas en profundidad con líderes comunitarios - Observación etnográfica en 5 barrios - Análisis de precios de viviendas (registros conservador de bienes raíces) Seguimiento longitudinal: Mediciones en t0 (línea base), t1 (fin de intervención), t2 (2 años post), t3 (5 años post) 14.6.4 Hallazgos sobre efectos estructurales Institucionalización de participación comunitaria: - Barrios intervenidos mantienen organizaciones comunitarias activas 5 años después (70% vs 30% en barrios control) - Efecto estructural: El programa creó cultura de participación que persiste Transformación del mercado inmobiliario local: - Valorización de viviendas en barrios intervenidos: +15% en 5 años (vs +3% en barrios similares) - Entrada de nuevos comercios: +35% de locales comerciales - Efecto estructural: Cambio en dinámica de mercado inmobiliario de zona 14.7 Desafíos metodológicos en evaluación de efectos complejos 14.7.1 1. Unidad de análisis ¿Individuo, hogar, barrio, territorio? Efectos complejos requieren múltiples unidades de análisis simultáneas 14.7.2 2. Tamaño muestral Detectar efectos indirectos y sinergias requiere muestras mucho mayores que evaluar efecto directo 14.7.3 3. Horizonte temporal Efectos estructurales pueden tardar años en manifestarse, requieren seguimientos de largo plazo 14.7.4 4. Inferencia causal Difícil aislar efecto del programa de otros cambios contextuales en evaluaciones de largo plazo 14.7.5 5. Costos Diseños complejos (factorial, multinivel, longitudinal) son significativamente más costosos 14.8 Ejercicio práctico Considere un programa de mejoramiento de infraestructura escolar (construcción de bibliotecas, laboratorios, canchas deportivas) en escuelas municipales de comunas rurales. Diseñe una evaluación que capture: Efectos directos en estudiantes de escuelas intervenidas Efectos indirectos en establecimientos educacionales cercanos (competencia, emulación) Posibles sinergias con programa de capacitación docente Efectos estructurales sobre sistema educativo comunal (ej: retención de matrícula, cambio en aspiraciones educacionales) Para cada tipo de efecto, especifique: - Indicador a medir - Método de medición - Unidad de análisis - Grupo de comparación 14.9 Referencias García Moreno, M., &amp; García López, R. (2014). Efectos territoriales de programas de desarrollo urbano. CEPAL. Zapata, J. G., &amp; Tejeda, E. (2009). Evaluación de efectos e impactos de programas sociales. CIDE. Banerjee, A., Chandrasekhar, A. G., Duflo, E., &amp; Jackson, M. O. (2013). The diffusion of microfinance. Science, 341(6144). Galiani, S., Gertler, P., &amp; Undurraga, R. (2021). The half-life of happiness: Hedonic adaptation in the subjective well-being of poor slum dwellers to a large improvement in housing. Journal of the European Economic Association, 19(4), 2810-2844. Heckman, J. J., Moon, S. H., Pinto, R., Savelyev, P. A., &amp; Yavitz, A. (2010). The rate of return to the HighScope Perry Preschool Program. Journal of Public Economics, 94(1-2), 114-128. "],["casos-de-evaluación-en-chile.html", "Capítulo 15 Casos de evaluación en Chile 15.1 Caso 1: Chile Crece Contigo - Evaluación de sistema integrado de protección a la infancia 15.2 Caso 2: Subvención Escolar Preferencial (SEP) - Evaluación de incentivos en educación 15.3 Caso 3: Ingreso Ético Familiar - Evaluación de programa multidimensional anti-pobreza 15.4 Lecciones transversales de los tres casos 15.5 Ejercicio práctico: Diseño de evaluación 15.6 Referencias", " Capítulo 15 Casos de evaluación en Chile Este capítulo presenta estudios de caso de evaluaciones de programas chilenos emblemáticos, ilustrando la aplicación práctica de las metodologías presentadas en capítulos anteriores. Cada caso muestra diseños evaluativos reales, hallazgos clave y lecciones aprendidas. 15.1 Caso 1: Chile Crece Contigo - Evaluación de sistema integrado de protección a la infancia 15.1.1 Contexto del programa Chile Crece Contigo (ChCC) constituye el subsistema de protección integral a la primera infancia en Chile, operando desde 2007. Es uno de los sistemas más comprehensivos de América Latina, integrando prestaciones de salud, educación y protección social desde la gestación hasta el ingreso al sistema escolar (4 años de edad). Componentes principales: Atención en salud: Control prenatal, parto, controles de salud infantil, vacunación Educación parental: Talleres de crianza, guías anticipatorias, visitas domiciliarias Prestaciones monetarias: Subsidios complementarios para familias vulnerables Apoyo al desarrollo infantil: Estimulación temprana, jardines infantiles, educación preescolar Prestaciones especiales: Ayudas técnicas para niños con rezago en desarrollo Población objetivo: Todos los niños y niñas desde gestación hasta 4 años, con prestaciones universales y focalizadas según vulnerabilidad. Presupuesto: ~$250.000 millones anuales (2022) 15.1.2 Teoría de cambio del programa Problema identificado: Desigualdades tempranas en desarrollo infantil determinan brechas posteriores en trayectorias educativas y laborales. Supuestos del programa: Ventana crítica: Los primeros 1,000 días (gestación a 2 años) son fundamentales para desarrollo cerebral Integralidad: Múltiples dimensiones (salud, nutrición, estimulación, protección) deben abordarse simultáneamente Universalidad con focalización: Acceso universal asegura no estigmatización; prestaciones focalizadas abordan brechas específicas Intersectorialidad: Coordinación entre salud, educación y protección social maximiza efectividad Cadena causal esperada: Gestación saludable → Parto seguro → Controles de salud → Detección oportuna de rezagos → Estimulación temprana → Mejor desarrollo psicomotor y socioemocional → Ingreso exitoso a educación preescolar → Mejores trayectorias educativas 15.1.3 Evaluaciones realizadas 15.1.3.1 A. Evaluación de diseño (2009) Metodología: Análisis documental, entrevistas a diseñadores del programa, revisión de evidencia internacional Hallazgos: Fortaleza de diseño: Modelo de gestión en red integra niveles nacional, regional y local Coherencia con evidencia: Componentes basados en literatura científica sobre desarrollo infantil Desafío institucional: Requiere articulación de 3 ministerios (Salud, Educación, Desarrollo Social) con tradición de trabajo sectorial Riesgo identificado: Variabilidad en implementación comunal por heterogeneidad de capacidades municipales 15.1.3.2 B. Evaluación de procesos (2012) Metodología: - Análisis de registros administrativos de 345 comunas - Encuesta a 200 encargados comunales ChCC - 50 entrevistas a directores de programas - 20 grupos focales con beneficiarias - Visitas a terreno en 15 comunas Hallazgos de cobertura: Componente Cobertura Meta Brecha Control prenatal 99% 95% ✓ Superado Control niño sano (0-4 años) 92% 90% ✓ Cumplido Visitas domiciliarias integrales 58% 80% ✗ Brecha Talleres de habilidades parentales 35% 60% ✗ Brecha significativa Sala cuna y jardines infantiles 48% 70% ✗ Brecha Hallazgos de calidad: - Componente salud: Alta fidelidad de implementación (98% de consultorios aplica protocolos) - Componente educación: Implementación heterogénea (45% de comunas no ofrece talleres parentales) - Satisfacción de usuarias: 78% evalúa positivamente atención recibida Factores explicativos de variabilidad: - Compromiso político local: Comunas con alcaldes comprometidos tienen 40% más cobertura en componentes no obligatorios - Capacidad técnica: Comunas con profesionales especializados (psicólogos, educadores) logran 35% más fidelidad - Recursos municipales: Comunas con mayor presupuesto per cápita tienen mejor infraestructura Eficiencia: - Costo promedio: $120,000 por niño atendido anualmente - Variabilidad: $80,000 (comunas grandes) a $200,000 (comunas rurales pequeñas) - Hallazgo: Deseconomías de escala en territorios con población dispersa 15.1.3.3 C. Evaluación de resultados (2014) Metodología: Encuesta longitudinal a 5,000 familias beneficiarias, mediciones en gestación, 12 meses y 24 meses Indicadores de resultado: 1. Desarrollo psicomotor (Test de Desarrollo Psicomotor TEPSI a 24 meses) Grupo Normal Riesgo Retraso Beneficiarios ChCC (60% más vulnerable) 72% 20% 8% No beneficiarios (40% menos vulnerable) 85% 12% 3% Hallazgo: ChCC reduce brecha de desarrollo entre grupos socioeconómicos en 40% comparado con escenario sin programa 2. Lactancia materna exclusiva hasta 6 meses - Beneficiarias ChCC: 58% - Promedio nacional pre-ChCC (2005): 43% - Efecto: +15 puntos porcentuales 3. Cumplimiento de controles de salud - ChCC: 92% de niños con controles al día - Pre-ChCC (2006): 78% - Efecto: +14 puntos porcentuales 4. Detección oportuna de rezagos - % de niños con rezago detectado antes de 18 meses: 65% (vs 28% pre-ChCC) 15.1.3.4 D. Evaluación de impacto cuasi-experimental (2016) Pregunta evaluativa: ¿ChCC causó mejoras en desarrollo infantil atribuibles al programa? Desafío metodológico: ChCC es universal desde 2009, no permite comparación simple tratados/control Estrategia de identificación: Diferencias en Diferencias explotando rollout gradual Diseño: - Grupo tratamiento temprano: Cohortes nacidas 2008-2009 (ChCC implementado) - Grupo tratamiento tardío (control temporal): Cohortes nacidas 2005-2006 (pre-ChCC) - Medición: Test de desarrollo a los 30 meses en ambas cohortes - Comparación: Cambio en desarrollo entre cohortes en comunas que implementaron ChCC vs comunas con implementación retrasada Datos: - Registros administrativos de controles de salud de 450,000 niños - Test de desarrollo psicomotor ( TEPSI) aplicado en consultorios Estimación: Impacto DiD = [Desarrollo_2008-09 - Desarrollo_2005-06]_ChCC_temprano - [Desarrollo_2008-09 - Desarrollo_2005-06]_ChCC_tardío Resultados: Dimensión de desarrollo Impacto (puntos TEPSI) Significancia Desarrollo psicomotor total +3.2 puntos p &lt; 0.01 Coordinación +2.8 p &lt; 0.01 Lenguaje +4.1 p &lt; 0.001 Motricidad +2.3 p &lt; 0.05 Interpretación: ChCC causó un aumento de 3.2 puntos en test de desarrollo (equivalente a 0.18 desviaciones estándar), estadísticamente significativo. Heterogeneidad de impactos: Mayor impacto en familias del 40% más vulnerable: +4.5 puntos Menor impacto en familias de ingresos medios: +1.8 puntos Hipótesis: Programa compensa desventajas iniciales de familias más vulnerables Robustez del diseño: Test de tendencias paralelas: Cohortes 2004-2005 (ambas pre-ChCC) no muestran diferencias significativas → Validación del supuesto DiD Análisis de sensibilidad: Resultados robustos a diferentes especificaciones de controles 15.1.4 Evaluación de sostenibilidad Institucionalización (Alta): - ChCC establecido por ley desde 2009 (Ley 20.379) - Presupuesto permanente en Ministerio de Desarrollo Social - Estructura de gobernanza intersectorial consolidada Apropiación territorial (Media): - 85% de comunas mantienen equipos ChCC estables - 60% incorporó componentes a planificación comunal regular - Riesgo: Dependencia de financiamiento central (municipios aportan &lt;5%) Sostenibilidad técnica (Alta): - Formación de ~3,000 profesionales especializados en primera infancia - Protocolos clínicos institucionalizados en red de salud pública 15.1.5 Lecciones aprendidas del caso ChCC 15.1.5.1 Fortalezas del diseño evaluativo Múltiples evaluaciones complementarias: Diseño → Procesos → Resultados → Impacto permite visión integral Aprovechamiento de rollout gradual: Variación temporal generó oportunidad para evaluación cuasi-experimental Combinación métodos mixtos: Cuantitativos (impacto) + cualitativos (procesos) explican qué funciona y por qué 15.1.5.2 Desafíos enfrentados Atribución causal compleja: Múltiples componentes simultáneos dificultan identificar qué componente específico genera efectos Efectos de largo plazo: Impactos en trayectorias educativas (objetivo final) requieren seguimiento 10-15 años Variabilidad territorial: Implementación heterogénea complica generalización de resultados 15.1.5.3 Usos de la evaluación Rediseño operacional: Evaluación de procesos llevó a fortalecer componentes débiles (educación parental) Reasignación de recursos: Identificación de brechas territoriales orientó inversión adicional Escalamiento: Evidencia de impacto justificó ampliación de cobertura (de 0-4 a 0-9 años en 2016) 15.2 Caso 2: Subvención Escolar Preferencial (SEP) - Evaluación de incentivos en educación 15.2.1 Contexto del programa La Ley de Subvención Escolar Preferencial (2008) constituye una de las reformas educativas más importantes de Chile post-retorno a democracia. Entrega recursos adicionales a escuelas (públicas y privadas subvencionadas) por cada estudiante prioritario (60% más vulnerable) y preferente (siguiente 20% vulnerable) matriculado. Componente clave: Recursos condicionados a suscripción de Plan de Mejoramiento Educativo (PME) con metas de aprendizaje. Monto de la subvención (2022): - Estudiante prioritario: $147,000 adicionales por alumno/año - Estudiante preferente: $98,000 adicionales por alumno/año Cobertura: ~1.2 millones de estudiantes prioritarios/preferentes en 9,500 escuelas Inversión anual: ~$600,000 millones 15.2.2 Teoría de cambio Problema: Estudiantes vulnerables concentrados en escuelas de bajo rendimiento, perpetuando desigualdad educativa. Hipótesis de intervención: Recursos adicionales permiten a escuelas contratar profesores, reducir tamaño de cursos, adquirir materiales Incentivos por matrícula vulnerable generan competencia por estudiantes prioritarios (antes evitados) Compromiso con metas (PME) orienta recursos a mejoras pedagógicas Cadena causal esperada: Recursos SEP → Inversión en calidad (profesores, materiales, infraestructura) → Mejores prácticas pedagógicas → Mayor aprendizaje de estudiantes prioritarios → Reducción de brecha de rendimiento Supuestos críticos: - Escuelas tienen capacidad técnica para diseñar e implementar PME efectivos - Recursos se usan en mejoras pedagógicas (no capturados por sostenedores) - Rendición de cuentas (Superintendencia) asegura cumplimiento de compromisos 15.2.3 Evaluaciones realizadas 15.2.3.1 A. Evaluación ex-ante de diseño (2008) Metodología: Simulación de efectos de equilibrio general (modelo de elección escolar) Pregunta: ¿Incentivos SEP modificarán comportamiento de escuelas y familias? Hallazgos de simulación: Efecto en composición: Modelo predice aumento de 5-8% en matrícula de estudiantes prioritarios en escuelas privadas subvencionadas (antes evitados por costos de selección) Riesgo de cream-skimming: Escuelas podrían seleccionar prioritarios de “mejor rendimiento” dentro del grupo vulnerable Efectos redistributivos: Transferencia de $300,000 millones anuales a escuelas que atienden vulnerabilidad Recomendaciones derivadas: - Prohibir cobros y selección a estudiantes prioritarios (incorporado en ley final) - Supervisar uso de recursos mediante Superintendencia de Educación 15.2.3.2 B. Evaluación de procesos: Uso de recursos SEP (2012) Metodología: Encuesta a 800 escuelas + análisis de 200 PME + visitas a 30 escuelas Pregunta: ¿En qué invierten las escuelas los recursos SEP? Resultados: Categoría de gasto % del total SEP Evaluación Contratación de profesores y asistentes 45% ✓ Uso prioritario Material didáctico y recursos de aprendizaje 18% ✓ Pertinente Capacitación docente 12% ✓ Pertinente Infraestructura menor 10% Neutro Servicios de apoyo (psicólogos, tutores) 8% ✓ Pertinente Gastos administrativos 7% ⚠ Cuestionable Hallazgo principal: 75% de recursos se invierte en áreas pedagógicamente pertinentes. Variabilidad por dependencia: - Escuelas municipales: Mayor gasto en personal (55%) y menor en capacitación (8%) - Escuelas particulares subvencionadas: Mayor gasto en capacitación (16%) y materiales (22%) Calidad de los PME: - Solo 35% de PME incluye metas específicas y medibles - 48% de escuelas no realiza seguimiento trimestral de PME - Brecha identificada: Débil capacidad técnica de escuelas para planificación estratégica 15.2.3.3 C. Evaluación de resultados: Cumplimiento de metas PME (2013) Metodología: Análisis de 5,000 PME y resultados SIMCE 2011-2013 Pregunta: ¿Las escuelas SEP cumplen las metas de aprendizaje comprometidas en PME? Resultados: Nivel de cumplimiento % de escuelas Cumplimiento total (&gt;90% de metas) 28% Cumplimiento parcial (50-89% de metas) 45% Bajo cumplimiento (&lt;50% de metas) 27% Factores asociados a cumplimiento: Calidad técnica del PME (explicar 35% de varianza): Escuelas con PME específicos y focalizados cumplen 40% más metas Acompañamiento externo: Escuelas con asesoría técnica (ATE) cumplen 25% más que escuelas sin apoyo Tamaño de escuela: Escuelas grandes (&gt;500 alumnos) cumplen 15% más que escuelas pequeñas 15.2.3.4 D. Evaluación de impacto cuasi-experimental en aprendizajes (2014-2015) Pregunta central: ¿SEP causó mejoras en rendimiento académico de estudiantes prioritarios? Desafío metodológico: SEP es universal en escuelas que lo suscriben (no hay aleatorización) Estrategia de identificación: Diferencias en Diferencias explotando timing de incorporación Diseño: - Tratados tempranos: Escuelas que ingresaron a SEP en 2008 (1° año) - Tratados tardíos (control temporal): Escuelas que ingresaron en 2011-2012 - Período: 2006 (pre-SEP) a 2013 - Datos: Resultados SIMCE 4° básico de 5,000 escuelas Especificación del modelo: SIMCE_it = β₀ + β₁·SEP_it + β₂·X_it + μ_i + λ_t + ε_it Donde: - SIMCE_it: Puntaje escuela i en año t - SEP_it: Indicador de participación en SEP - X_it: Controles (matrícula, NSE, ruralidad) - μ_i: Efectos fijos de escuela - λ_t: Efectos fijos de año - β₁: Impacto causal de SEP Resultados de impacto: Asignatura Impacto (puntos SIMCE) Equivalente Significancia Matemáticas +6.2 puntos 0.15 DE p &lt; 0.01 Lenguaje +4.8 puntos 0.12 DE p &lt; 0.01 Ciencias Naturales +3.5 puntos 0.09 DE p &lt; 0.05 Interpretación: SEP causó aumentos moderados pero significativos en aprendizajes, equivalentes a 3-4 meses adicionales de escolaridad. Análisis de heterogeneidad: Por nivel socioeconómico de escuela: - Escuelas 40% más vulnerable: +8.5 puntos en Matemáticas - Escuelas vulnerabilidad media: +4.2 puntos - Hallazgo: Mayor impacto donde mayor concentración de pobreza Por dependencia administrativa: - Escuelas municipales: +5.8 puntos - Escuelas particulares subvencionadas: +6.5 puntos - Hallazgo: Efectos similares entre sectores Por tamaño de escuela: - Escuelas rurales pequeñas (&lt;100 alumnos): +9.2 puntos - Escuelas urbanas grandes: +4.5 puntos - Hipótesis: Escuelas pequeñas usan recursos de manera más focalizada Test de robustez: Tendencias paralelas: Verificación de evolución SIMCE 2004-2007 (pre-SEP) muestra tendencias similares entre tratados tempranos y tardíos → Supuesto DiD validado Placebo: No se observan efectos en escuelas que no recibieron SEP Variables instrumentales: Uso de elegibilidad administrativa como instrumento confirma resultados 15.2.4 Evaluación de eficiencia y costo-efectividad (2016) Pregunta: ¿SEP es costo-efectiva comparada con otras intervenciones educativas? Cálculo de costo-efectividad: Costo: $147,000 por estudiante prioritario/año Efecto: +6.2 puntos SIMCE en Matemáticas Costo-efectividad: $23,700 por punto SIMCE Comparación con alternativas: Intervención Costo por punto SIMCE Evaluación SEP $23,700 Referencia Reducción tamaño de curso (de 35 a 25) $45,000 Menos costo-efectiva Programa de tutorías individualizadas $18,000 Más costo-efectiva Jornada Escolar Completa $35,000 Menos costo-efectiva Hallazgo: SEP tiene costo-efectividad intermedia, mejor que intervenciones masivas (JEC) pero inferior a intervenciones muy focalizadas (tutorías). 15.2.5 Evaluación de efectos no intencionados (2017) Pregunta: ¿SEP generó efectos perversos no anticipados? Efectos analizados: 1. Selección de estudiantes prioritarios - Hipótesis: Escuelas podrían seleccionar prioritarios de mejor rendimiento - Evidencia: No se detecta aumento en selección encubierta (test de distribución de habilidades de prioritarios) 2. Reclasificación estratégica - Hipótesis: Escuelas podrían manipular clasificación de estudiantes como prioritarios - Evidencia: &lt;2% de casos sospechosos (control del Registro Social de Hogares limita manipulación) 3. Efectos en estudiantes no prioritarios - Hipótesis: Recursos focalizados en prioritarios podrían desatender a no prioritarios - Evidencia: No se observa reducción de rendimiento en no prioritarios (efecto neutro) 4. Segregación entre escuelas - Hipótesis: SEP podría aumentar segregación si escuelas privadas atraen prioritarios de mejor nivel - Evidencia: Leve reducción de segregación (-3% en índice de disimilitud Duncan) por prohibición de selección 15.2.6 Lecciones aprendidas del caso SEP 15.2.6.1 Fortalezas del diseño del programa Incentivos bien calibrados: Monto suficiente para generar cambios sin crear dependencia Prohibición de selección: Evitó cream-skimming y redujo segregación Flexibilidad en uso de recursos: Escuelas priorizan según contexto 15.2.6.2 Debilidades identificadas Capacidad técnica heterogénea: Muchas escuelas no saben diseñar PME efectivos Supervisión débil: Superintendencia no sanciona incumplimiento de metas Horizontetemporalista: Presión por resultados anuales desincentiva inversiones de largo plazo (ej: formación docente) 15.2.6.3 Efectividad de la estrategia evaluativa DiD aprovechó variación temporal: Rollout gradual permitió evaluación causal sin experimento Múltiples dimensiones evaluadas: Desde uso de recursos hasta aprendizajes Análisis de heterogeneidad: Identificó para quiénes funciona mejor 15.2.6.4 Usos de la evidencia Reformas al programa: Evaluación llevó a mejorar acompañamiento técnico a escuelas Expansión de cobertura: Evidencia de impacto justificó ampliación a estudiantes preferentes (2011) Debate público informado: Resultados usados en discusión de reforma educacional 2015 15.3 Caso 3: Ingreso Ético Familiar - Evaluación de programa multidimensional anti-pobreza 15.3.1 Contexto del programa El Ingreso Ético Familiar (IEF, 2012-2018) fue un programa emblemático del segundo gobierno de Sebastián Piñera, diseñado como estrategia integral para superar pobreza extrema. Reemplazó y amplió el Programa Puente (2002-2011). Componentes: Bono al Trabajo de la Mujer: Subsidio al ingreso laboral de mujeres (hasta $60,000/mes) Bono por Hijo: $13,500 mensuales por hijo Bono de Protección: Transferencia base de $25,000 para familias extremadamente pobres Acompañamiento psicosocial: Apoyo familiar personalizado (similar a Puente) Acceso preferente: Prioridad en programas de vivienda, salud, educación Población objetivo: 170,000 familias en situación de pobreza extrema Inversión: ~$180,000 millones anuales 15.3.2 Objetivos del programa Objetivo general: Erradicar pobreza extrema en Chile al 2018 Objetivos específicos: 1. Aumentar ingresos monetarios de familias 2. Incrementar inserción laboral femenina 3. Mejorar acceso a prestaciones sociales 4. Fortalecer autonomía y organización familiar 15.3.3 Teoría de cambio Diagnóstico: Pobreza extrema es multidimensional - requiere simultáneamente: - Ingresos monetarios - Activación laboral (especialmente mujeres) - Acceso a servicios sociales - Fortalecimiento de capacidades familiares Hipótesis central: Sinergia entre transferencias monetarias + incentivos laborales + acompañamiento psicosocial genera salida sostenible de pobreza. Mecanismo causal esperado: Bono Trabajo Mujer → Incentivo a trabajar → Ingreso laboral femenino → Mayor ingreso familiar → Salida de pobreza Acompañamiento → Organización familiar → Acceso a servicios → Mejora en dimensiones no monetarias → Bienestar sostenible 15.3.4 Evaluación de impacto: Diseño y metodología 15.3.4.1 Pregunta evaluativa central ¿El IEF causó aumento en ingresos y empleo femenino atribuibles al programa? 15.3.4.2 Desafío metodológico No hubo aleatorización (programa no experimental) Familias beneficiarias son extremadamente pobres (sesgo de selección observable y no observable) Rollout rápido (toda población elegible recibió programa en 2012-2013) 15.3.4.3 Estrategia de identificación: Propensity Score Matching (PSM) Supuesto: Selección en observables - Todas las diferencias relevantes entre beneficiarios y no beneficiarios son capturadas por características observables. Paso 1: Estimación del Propensity Score Probabilidad de participar en IEF en función de características pre-programa: P(IEF=1 | X) = Logit(α₀ + α₁·Ingreso₀ + α₂·Jefa_mujer + α₃·Años_escolaridad + α₄·N_hijos + α₅·Ruralidad + α₆·Región + ...) Variables incluidas en el modelo: - Ingreso per cápita familiar 2011 (pre-programa) - Composición del hogar (N° de niños, adultos mayores, jefatura femenina) - Escolaridad de jefa de hogar - Situación laboral 2011 - Acceso a servicios (salud, vivienda) - Territorio (región, ruralidad) Datos: Encuesta CASEN 2011 (pre-programa) y 2013 (post-programa) Muestra: - Tratados: 15,000 hogares IEF - Pool de controles potenciales: 45,000 hogares pobres no-IEF Paso 2: Matching Cada hogar IEF fue emparejado con hogar no-IEF de propensity score similar: Método: Nearest neighbor matching (1:1) con caliper de 0.01 Balance: Test de diferencias en medias pre-programa entre tratados y matched controls Validación del matching: Variable Tratados (IEF) Controles matched Diferencia p-value Ingreso per cápita 2011 $45,200 $44,800 -$400 0.62 (NS) % Jefatura femenina 68% 66% +2pp 0.31 (NS) Años escolaridad jefa 7.2 7.1 +0.1 0.74 (NS) % Ruralidad 22% 23% -1pp 0.68 (NS) Resultado: Grupos balanceados en observables → Matching exitoso Paso 3: Estimación de impacto Comparación de resultados 2013 entre tratados IEF y controles matched: Impacto = E[Y₂₀₁₃ | IEF=1, X] - E[Y₂₀₁₃ | IEF=0, X] 15.3.5 Resultados de la evaluación de impacto 15.3.5.1 Impacto en ingresos familiares Indicador IEF (2013) Control (2013) Impacto % cambio Ingreso per cápita $72,300 $58,400 +$13,900 +24% Ingresos del trabajo $48,500 $42,200 +$6,300 +15% Transferencias monetarias $23,800 $16,200 +$7,600 +47% Interpretación: IEF aumentó ingreso per cápita en $13,900 mensuales (+24%), combinando aumento en ingreso laboral (+$6,300) y transferencias (+$7,600). 15.3.5.2 Impacto en empleo femenino Indicador IEF Control Impacto Significancia Tasa de ocupación mujeres 42% 35% +7 pp p &lt; 0.01 Horas trabajadas (ocupadas) 38.5 hrs 36.2 hrs +2.3 hrs p &lt; 0.05 Ingreso laboral mujeres $185,000 $165,000 +$20,000 p &lt; 0.01 Interpretación: IEF aumentó participación laboral femenina en 7 puntos porcentuales, con efectos también en horas trabajadas e ingresos laborales. Mecanismo: Bono al Trabajo de la Mujer incentivó entrada al mercado laboral al complementar (no sustituir) ingreso laboral. 15.3.5.3 Impacto en pobreza Indicador IEF Control Impacto % en pobreza extrema (ingreso) 12% 28% -16 pp % en pobreza total 38% 52% -14 pp Interpretación: IEF redujo pobreza extrema en 16 puntos porcentuales (de 28% a 12%). 15.3.5.4 Impacto en dimensiones no monetarias Dimensión IEF Control Impacto Acceso a atención de salud oportuna 78% 68% +10 pp Niños con controles de salud al día 85% 74% +11 pp Asistencia escolar regular (6-17 años) 94% 91% +3 pp Interpretación: Efectos positivos en acceso a servicios, aunque menores que efectos monetarios. 15.3.6 Análisis de heterogeneidad de impactos 15.3.6.1 Por composición del hogar Tipo de hogar Impacto en ingreso Impacto en empleo femenino Jefatura femenina sin pareja +$16,200 +12 pp Jefatura femenina con pareja +$12,800 +5 pp Jefatura masculina +$11,500 +3 pp Hallazgo: Mayor impacto en hogares monoparentales con jefa mujer (población más vulnerable). 15.3.6.2 Por territorio Zona Impacto en ingreso Impacto en empleo femenino Urbana +$12,500 +8 pp Rural +$17,200 +4 pp Hallazgo: Mayor impacto monetario en zonas rurales (donde ingresos bajos), pero mayor impacto laboral en zonas urbanas (mayor oferta de empleo). 15.3.7 Evaluación de sostenibilidad: ¿Efectos persisten post-programa? Pregunta: ¿Familias que egresan de IEF mantienen mejoras en ingresos? Metodología: Seguimiento a 2,000 familias que egresaron del programa en 2015-2016, mediciones en 2017 (1 año post-egreso) Resultados: Indicador Familias egresadas IEF (2017) Control Diferencia Tasa de pobreza extrema 18% 28% -10 pp Empleo femenino 38% 35% +3 pp Interpretación: Sostenibilidad parcial: Efectos del programa persisten 1 año post-egreso, pero se reducen (de -16pp a -10pp en pobreza extrema) Empleo femenino más sostenible: Efecto laboral se mantiene (de +7pp a +3pp), sugiriendo que inserción laboral es más duradera que transferencias Hipótesis: Transferencias monetarias tienen efecto inmediato pero temporal; activación laboral tiene efecto más duradero. 15.3.8 Evaluación de eficiencia: Análisis costo-efectividad Costo anual por familia: $1,200,000 Efecto: Reducción de pobreza extrema de 16 puntos porcentuales Costo por familia que sale de pobreza extrema: $1,200,000 / 0.16 = $7,500,000 por familia Comparación con alternativas: Programa Costo por familia sacada de pobreza Ingreso Ético Familiar $7,500,000 Programa Puente (2002-2011) $5,200,000 Transferencias monetarias puras (sin acompañamiento) $3,800,000 Hallazgo: IEF es menos costo-efectivo que programas anteriores. Explicación: Componente de acompañamiento psicosocial es costoso y tiene efectos limitados en pobreza monetaria. 15.3.9 Debate sobre el programa Defensores del IEF: - Enfoque multidimensional aborda pobreza de manera integral - Incentivos laborales (Bono Trabajo Mujer) promueven autonomía vs dependencia de transferencias - Efectos van más allá de ingresos (acceso a servicios, organización familiar) Críticos del IEF: - Baja costo-efectividad comparado con transferencias simples - Acompañamiento psicosocial tiene efectos limitados y es costoso - Condicionalidades laborales pueden excluir a mujeres sin acceso a empleo (rurales, con hijos pequeños) Lecciones de la evaluación: 1. Sinergias limitadas: Componentes múltiples no generaron efectos superiores a suma de partes (contrario a hipótesis de sinergia) 2. Incentivos laborales funcionan: Bono Trabajo Mujer efectivamente aumentó empleo femenino 3. Sostenibilidad requiere empleo: Efectos más duraderos vienen de inserción laboral, no de transferencias 15.3.10 Legado y evolución del programa Reemplazo por Ingreso Familiar de Emergencia (2020): - Gobierno de Sebastián Piñera (2018-2022) reemplazó IEF por nuevo programa - Mantiene transferencias monetarias pero elimina acompañamiento psicosocial (por baja efectividad) - Focalización más estricta en extrema pobreza Evidencia influyó en diseño: Evaluación mostró que componentes de mayor impacto eran transferencias + incentivos laborales, no acompañamiento. 15.4 Lecciones transversales de los tres casos 15.4.1 Sobre diseño evaluativo Aprovechar variación no experimental: Los tres casos usaron cuasi-experimentos (DiD, PSM) ante imposibilidad de RCT Evaluaciones tempranas y continuas: Evaluaciones de procesos permitieron ajustes antes de evaluación de impacto Métodos mixtos: Combinación cuanti-cuali explica no solo “cuánto” sino “cómo” y “por qué” 15.4.2 Sobre uso de evidencia Rediseños informados: Las tres evaluaciones llevaron a modificaciones de programas (ChCC amplió componentes, SEP mejoró acompañamiento, IEF simplificó estructura) Transparencia de limitaciones: Evaluaciones reconocen limitaciones metodológicas (sesgo de selección potencial, efectos de largo plazo no medidos) Diálogo con política: Evidencia no determina decisiones, pero informa debate público 15.4.3 Sobre desafíos metodológicos recurrentes Atribución causal en programas universales: ChCC y SEP requirieron aprovechar rollout gradual Heterogeneidad territorial: Los tres programas muestran gran variabilidad en efectividad según territorio Efectos de largo plazo: Evaluaciones midieron efectos 1-3 años; impactos finales (trayectorias de vida) requieren décadas 15.5 Ejercicio práctico: Diseño de evaluación Considere el Programa de Apoyo a la Retención Escolar (en liceos vulnerables) que incluye: Tutorías académicas personalizadas Apoyo psicosocial (psicólogos en liceos) Becas de manutención ($50,000/mes) Talleres de habilidades socioemocionales Tarea: Diseñe una evaluación de impacto para este programa respondiendo: Pregunta evaluativa principal y 2 preguntas secundarias Indicadores de resultado (al menos 3, incluyendo retención y aprendizajes) Diseño metodológico: ¿RCT, DiD, PSM u otro? Justifique según factibilidad Datos necesarios: Fuentes de información y periodicidad de mediciones Análisis de heterogeneidad: ¿Para qué subgrupos esperaría efectos diferenciales? Cronograma: Timing de línea base, implementación y seguimiento 15.6 Referencias 15.6.1 Chile Crece Contigo Staab, S., &amp; Gerhard, R. (2010). Childcare service expansion in Chile and Mexico: For women or children or both? UNRISD. Ministerio de Desarrollo Social (2016). Evaluación de impacto del programa Chile Crece Contigo en el desarrollo infantil. Santiago. 15.6.2 Subvención Escolar Preferencial Mizala, A., &amp; Torche, F. (2012). Bringing the schools back in: The stratification of educational achievement in the Chilean voucher system. International Journal of Educational Development, 32(1), 132-144. Correa, J. A., Parro, F., &amp; Reyes, L. (2014). The effects of vouchers on school results: Evidence from Chile’s targeted voucher program. Documento de Trabajo CLAPES UC. 15.6.3 Ingreso Ético Familiar Martínez, C., &amp; Perticará, M. (2017). Childcare effects on maternal employment: Evidence from Chile. Journal of Development Economics, 126, 127-137. DIPRES (2016). Informe final de evaluación Programa Ingreso Ético Familiar. Ministerio de Hacienda, Chile. 15.6.4 Generales sobre evaluación en Chile DIPRES (2020). Sistema de Evaluación y Control de Gestión: 30 años de evaluación de programas públicos en Chile. Ministerio de Hacienda. Beyer, H., &amp; Vergara, R. (2019). ¿Qué hacer con Chile? Ensayos sobre desarrollo y política pública. Ediciones Universidad Diego Portales. "],["elaboración-de-informes-de-evaluación.html", "Capítulo 16 Elaboración de informes de evaluación 16.1 Estructura estándar de informe evaluativo 16.2 Principios de redacción 16.3 Visualización de datos evaluativos 16.4 Comunicación a diferentes audiencias 16.5 Caso: Análisis de informe de evaluación DIPRES 16.6 Referencias", " Capítulo 16 Elaboración de informes de evaluación 16.1 Estructura estándar de informe evaluativo 16.1.1 1. Resumen Ejecutivo (2-3 páginas) Contexto del programa Preguntas evaluativas Metodología sintética Hallazgos principales (priorizados) Recomendaciones clave 16.1.2 2. Introducción Antecedentes del programa Propósito de la evaluación Usuarios previstos del informe Estructura del documento 16.1.3 3. Descripción del programa Contexto y problema que aborda Objetivos y teoría de cambio Componentes y prestaciones Población objetivo y cobertura Presupuesto y estructura institucional 16.1.4 4. Metodología de evaluación Preguntas evaluativas Diseño metodológico Fuentes de información Técnicas de análisis Limitaciones metodológicas 16.1.5 5. Hallazgos Organizar por preguntas evaluativas o criterios (pertinencia, eficacia, etc.). Cada hallazgo debe: - Estar respaldado por evidencia específica - Distinguir entre hechos e interpretaciones - Considerar explicaciones alternativas - Citar fuentes de información 16.1.6 6. Conclusiones Síntesis de hallazgos principales respondiendo preguntas evaluativas. Evitar introducir nueva evidencia. 16.1.7 7. Recomendaciones Accionables, específicas, priorizadas, viables. Cada recomendación debe: - Vincularse a hallazgo específico - Especificar a quién va dirigida - Indicar urgencia relativa - Considerar viabilidad política/administrativa 16.1.8 8. Anexos Metodología detallada, instrumentos de recolección, tablas estadísticas, bibliografía. 16.2 Principios de redacción Claridad: Lenguaje directo, párrafos concisos, estructura lógica Precisión: Evitar ambigüedades, cuantificar cuando posible Objetividad: Distinguir hechos de juicios, fundamentar interpretaciones Accesibilidad: Explicar jerga técnica, usar visualizaciones Utilidad: Foco en información accionable para decisiones 16.3 Visualización de datos evaluativos Gráficos efectivos: - Barras comparativas: Comparar resultados entre grupos o períodos - Series de tiempo: Evolución de indicadores - Mapas: Variación territorial - Tablas resumidas: Hallazgos multidimensionales Principios: Simplicidad, títulos claros, ejes etiquetados, fuentes citadas. 16.4 Comunicación a diferentes audiencias Tomadores de decisión: Resumen ejecutivo con implicancias claras Gestores: Análisis operacional detallado con recomendaciones específicas Ciudadanía: Síntesis accesible, visualizaciones, lenguaje no técnico Comunidad académica/técnica: Anexo metodológico completo 16.5 Caso: Análisis de informe de evaluación DIPRES [EJERCICIO: Revisar un informe DIPRES. Evaluar si cumple con estructura estándar. Identificar fortalezas y debilidades de redacción y visualización] 16.6 Referencias DIPRES (2015); Guías metodológicas de evaluación "],["ética-y-buenas-prácticas-en-evaluación.html", "Capítulo 17 Ética y buenas prácticas en evaluación 17.1 Principios éticos fundamentales 17.2 Dilemas éticos comunes 17.3 Estándares profesionales 17.4 Comités de ética 17.5 Caso: Dilemas éticos en evaluación de programa de salud mental 17.6 Preguntas de reflexión 17.7 Referencias", " Capítulo 17 Ética y buenas prácticas en evaluación 17.1 Principios éticos fundamentales 17.1.1 1. Respeto a personas Consentimiento informado de participantes Protección de poblaciones vulnerables Confidencialidad de información personal Derecho a no participar sin consecuencias 17.1.2 2. Beneficencia y no maleficencia Maximizar beneficios, minimizar daños Evaluar riesgos de la evaluación misma Considerar efectos no anticipados Proteger a beneficiarios en evaluaciones experimentales 17.1.3 3. Justicia Distribución equitativa de beneficios y cargas de evaluación No excluir sistemáticamente a grupos de beneficios probados Considerar implicancias distributivas de hallazgos 17.1.4 4. Integridad Honestidad en reporte de hallazgos Transparencia metodológica Reconocimiento de limitaciones Evitar conflictos de interés 17.2 Dilemas éticos comunes 17.2.1 ¿Negar tratamiento a grupos de control? Evaluaciones experimentales asignan aleatoriamente algunos elegibles a control, negándoles temporalmente beneficios. ¿Es esto ético? Depende de: - ¿Existe evidencia previa de efectividad? - ¿Los recursos alcanzarían para todos de todos modos? - ¿El diseño minimiza tiempo sin tratamiento? - ¿Se ofrece tratamiento al grupo control posteriormente? 17.2.2 ¿Usar datos administrativos sin consentimiento? Registros administrativos contienen información personal. ¿Es legítimo usarlos sin consentimiento individual explícito? Requiere: - Anonimización rigurosa - Propósitos claramente de interés público - Resguardos de seguridad de datos - Marcos legales apropiados (Ley de Protección de Datos) 17.2.3 ¿Reportar hallazgos negativos que amenacen programas? Evaluaciones pueden encontrar que programas políticamente valorados son inefectivos. ¿El evaluador debe reportar honestamente incluso si esto compromete el programa? Principio: La integridad técnica requiere honestidad. Pero comunicación debe considerar: - Solidez de evidencia - Explicaciones alternativas - Contexto de toma de decisiones - Responsabilidad con beneficiarios que dependen del programa 17.2.4 ¿Quién es el cliente del evaluador? Evaluador responde simultáneamente a múltiples audiencias: quién paga la evaluación, gestores del programa, beneficiarios, ciudadanía. Pueden existir tensiones entre sus intereses. Resguardos: Independencia técnica, transparencia de financiamiento, publicación de hallazgos. 17.3 Estándares profesionales Organizaciones como American Evaluation Association han desarrollado códigos éticos y estándares de calidad. Principios clave: - Competencia técnica - Integridad - Respeto por las personas - Responsabilidad por el bien común - Transparencia 17.4 Comités de ética Evaluaciones que involucran recolección primaria de datos con seres humanos deben someterse a revisión ética. Comités evalúan: - Riesgos para participantes - Procedimientos de consentimiento - Protección de confidencialidad - Balance riesgo/beneficio - Calificaciones de evaluadores 17.5 Caso: Dilemas éticos en evaluación de programa de salud mental [DESARROLLAR: Escenario con información sensible, poblaciones vulnerables, uso de grupos control. Discutir cómo balancear rigor evaluativo con protección de participantes] 17.6 Preguntas de reflexión ¿En qué circunstancias sería éticamente aceptable negar temporalmente beneficios a un grupo control? Un evaluador descubre que un programa popular es inefectivo. ¿Cómo debe comunicar esto sin dañar innecesariamente a beneficiarios que dependen del programa? ¿Qué resguardos implementaría para proteger confidencialidad en una evaluación que utiliza datos administrativos sensibles? 17.7 Referencias American Evaluation Association (2018). Guiding Principles for Evaluators Nagel, S.S. (2002). Handbook of public policy evaluation. Sage. Zapata, G., &amp; Tejeda, I. (2009). Consideraciones éticas en evaluación "],["referencias-12.html", "Referencias", " Referencias Banco Mundial (2005). Chile: estudio de evaluación de impacto del Programa de Evaluación de Programas. Unidad de Reducción de la Pobreza y Gestión Económica América Latina y el Caribe. Banco Mundial (2010). La formulación de políticas en la OCDE: Ideas para América Latina. Banco Mundial LAC. Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. CEP (2017). Un Estado para la ciudadanía. Informe de la Comisión de Modernización del Estado. Centro de Estudios Públicos. CNEP (2023). Actualización y reenfoque al sistema de evaluación de programas públicos: una propuesta para un Estado más moderno y efectivo. Comisión Nacional de Evaluación y Productividad. DIPRES (2015). Evaluación Ex-Post: Conceptos y Metodologías. Ministerio de Hacienda. García Moreno, M., &amp; García López, R. (2014). La gestión para resultados en el desarrollo: avances y desafíos en América Latina y el Caribe. Banco Interamericano de Desarrollo. Irarrázaval, I., Larrañaga, O., Rodríguez, J., &amp; Valdés, R. (2020). Propuestas para una mejor calidad del gasto y las políticas públicas en Chile. Centro de Políticas Públicas, Pontificia Universidad Católica de Chile. Nagel, S.S. (2002). Handbook of public policy evaluation. Sage Publications. Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico para la planificación, el seguimiento y la evaluación de proyectos y programas. CEPAL. Pérez, G., &amp; Maldonado, C. (Eds.) (2015). Panorama de los sistemas nacionales de monitoreo y evaluación en América Latina. CIDE/Clear LAC. Pignatta, M.A. (2015). Monitoreo y evaluación de políticas públicas en América Latina: Brechas por cerrar. Perspectivas de Políticas Públicas. Zapata, G., &amp; Tejeda, I. (2009). Impactos del aseguramiento de la calidad y acreditación de la educación superior. Consideraciones y proposiciones. Calidad en la Educación. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
