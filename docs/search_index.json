[["index.html", "Evaluación de Políticas Públicas Fundamentos, Metodologías y Aplicaciones Prefacio Propósito del libro Enfoque pedagógico Estructura del libro Agradecimientos Sobre el autor", " Evaluación de Políticas Públicas Fundamentos, Metodologías y Aplicaciones Fabián Belmar 2025-11-17 Prefacio Este libro constituye el material base para el curso de Evaluación de Políticas Públicas impartido en la carrera de Administración Pública de la Universidad de Talca. Su objetivo es proporcionar a estudiantes de pregrado las herramientas conceptuales, metodológicas y prácticas necesarias para comprender, diseñar e implementar evaluaciones de políticas y programas públicos. Propósito del libro La evaluación de políticas públicas se ha consolidado como una función esencial del Estado moderno. En un contexto de recursos limitados y crecientes demandas ciudadanas, la capacidad de determinar si las intervenciones públicas logran sus objetivos, a qué costo y con qué efectos, resulta fundamental para la mejora continua de la gestión pública y la rendición de cuentas democrática. Este texto busca formar profesionales capaces de: Comprender los fundamentos teóricos y metodológicos de la evaluación de políticas públicas Aplicar herramientas de diagnóstico, diseño y evaluación de intervenciones públicas Utilizar el marco lógico y otras metodologías estándar de evaluación Analizar críticamente políticas y programas existentes Proponer mejoras fundamentadas en evidencia Enfoque pedagógico El libro adopta un enfoque aplicado, privilegiando el aprendizaje basado en casos reales del contexto chileno. Cada capítulo integra: Fundamentos conceptuales respaldados por literatura especializada Metodologías paso a paso con ejemplos concretos Casos de estudio de políticas y programas chilenos Ejercicios prácticos y preguntas de reflexión Referencias a fuentes oficiales y bases de datos públicas Estructura del libro El texto se organiza en cuatro partes: Parte I: Fundamentos introduce los conceptos esenciales de la evaluación de políticas públicas, sus enfoques metodológicos, el marco institucional chileno y el rol del gobierno abierto. Parte II: Herramientas de Diagnóstico y Diseño desarrolla las metodologías fundamentales para el diagnóstico de problemas públicos y el diseño de intervenciones, incluyendo el árbol de problemas, el marco lógico y el diseño de indicadores. Parte III: Evaluación de Resultados e Impacto profundiza en las metodologías de evaluación ex-post, abarcando evaluación de procesos, resultados e impacto con sus respectivas técnicas cuantitativas y cualitativas. Parte IV: Aplicaciones integra los contenidos previos mediante casos de evaluación en Chile, la elaboración de informes y las consideraciones éticas de la práctica evaluativa. Agradecimientos Este libro se nutre de la experiencia de múltiples actores del sistema de evaluación chileno, así como de las contribuciones académicas de instituciones como CEPAL, DIPRES y la Comisión Nacional de Evaluación y Productividad. Agradezco especialmente a los estudiantes de Administración Pública de la Universidad de Talca, cuyas preguntas y trabajos han enriquecido continuamente este material. Sobre el autor Fabián Belmar es profesor de Administración Pública en la Universidad de Talca, Chile. "],["introducción-a-la-evaluación-de-políticas-públicas.html", "Capítulo 1 Introducción a la evaluación de políticas públicas 1.1 ¿Qué es la evaluación de políticas públicas? 1.2 Elementos esenciales de la evaluación 1.3 Principios rectores 1.4 Funciones de la evaluación en el sector público 1.5 Desafíos de la evaluación en Chile 1.6 Preguntas de reflexión 1.7 Referencias del capítulo", " Capítulo 1 Introducción a la evaluación de políticas públicas 1.1 ¿Qué es la evaluación de políticas públicas? La evaluación de políticas públicas constituye una función sistemática de análisis destinada a determinar el mérito, valor y utilidad de las intervenciones gubernamentales. Siguiendo a Ortegón, Pacheco y Prieto (2005), la evaluación implica un proceso de recopilación y análisis de información que permite emitir juicios fundamentados sobre el diseño, implementación, efectos e impactos de políticas y programas. No se trata simplemente de un ejercicio técnico de medición, sino de una práctica que articula consideraciones metodológicas, políticas, éticas y presupuestarias para informar la toma de decisiones públicas. En el contexto contemporáneo, la evaluación ha evolucionado desde una función marginal y episódica hacia un componente integral de la gestión pública moderna. Como señala la Comisión Nacional de Evaluación y Productividad (CNEP, 2023), los sistemas de evaluación contribuyen simultáneamente a tres objetivos fundamentales: mejorar la efectividad de las intervenciones públicas mediante el aprendizaje organizacional, optimizar la asignación de recursos escasos y fortalecer la rendición de cuentas democrática ante la ciudadanía. La evaluación se diferencia de otras funciones analíticas del sector público —como la auditoría, el monitoreo o la investigación académica— por su foco específico en determinar el valor de las intervenciones. Mientras el monitoreo rastrea el cumplimiento de metas y el uso de recursos, la evaluación interroga si las metas mismas son apropiadas, si la teoría de cambio subyacente es válida y si los resultados justifican los costos incurridos. Esta distinción resulta crucial para comprender el rol distintivo que la evaluación cumple en el ciclo de las políticas públicas. 1.2 Elementos esenciales de la evaluación Toda evaluación rigurosa se estructura en torno a cuatro elementos constitutivos. Primero, requiere preguntas evaluativas claras que definan qué aspectos específicos del programa se someterán a escrutinio. Estas preguntas pueden interrogar sobre la pertinencia del diseño, la calidad de la implementación, la eficiencia en el uso de recursos, la efectividad en el logro de objetivos o el impacto atribuible a la intervención. Segundo, demanda criterios de evaluación explícitos que establezcan los estándares contra los cuales se juzgará el desempeño. Los criterios del Comité de Ayuda al Desarrollo de la OCDE (OECD-DAC) —pertinencia, coherencia, eficacia, eficiencia, impacto y sostenibilidad— se han consolidado como marco de referencia internacional, aunque cada evaluación debe adaptar y priorizar criterios según su propósito específico. Tercero, requiere evidencia sistemática recopilada mediante métodos apropiados al objeto y preguntas de la evaluación. Esta evidencia puede provenir de fuentes cuantitativas (indicadores administrativos, encuestas, datos experimentales) o cualitativas (entrevistas, grupos focales, análisis documental), frecuentemente combinadas en diseños mixtos que aprovechan las fortalezas complementarias de ambos enfoques. Cuarto, culmina en juicios valorativos fundamentados que sintetizan los hallazgos en conclusiones y recomendaciones accionables. A diferencia de la investigación académica que busca producir conocimiento generalizable, la evaluación apunta a generar aprendizajes utilizables para mejorar intervenciones específicas o informar decisiones de política. 1.3 Principios rectores La práctica evaluativa se guía por principios que resguardan su calidad y utilidad. El principio de utilidad establece que las evaluaciones deben diseñarse y ejecutarse respondiendo a las necesidades de información de usuarios específicos, evitando ejercicios puramente ceremoniales o burocráticos. Como señala Bonnefoy y Armijo (2005), evaluaciones técnicamente sofisticadas pero desconectadas de procesos decisionales reales desperdician recursos y erosionan la credibilidad del sistema evaluativo. El principio de viabilidad reconoce que las evaluaciones operan bajo restricciones de tiempo, presupuesto, acceso a información y capacidades técnicas. Diseños metodológicamente ideales pero impracticables resultan contraproducentes; la evaluación efectiva balancea rigor y factibilidad, maximizando la validez de los hallazgos dentro de las restricciones existentes. El principio de propiedad ética exige que las evaluaciones respeten los derechos de los actores involucrados, protejan la confidencialidad de la información sensible, eviten consecuencias negativas no anticipadas y comuniquen hallazgos con transparencia y honestidad. Esto incluye considerar cuidadosamente cómo los hallazgos evaluativos pueden afectar a beneficiarios, implementadores y otros stakeholders. El principio de precisión técnica demanda que las evaluaciones empleen métodos apropiados, documenten procedimientos transparentemente y fundamenten conclusiones en evidencia sólida. Sin embargo, precisión no equivale a complejidad técnica innecesaria; frecuentemente, diseños relativamente simples pero bien ejecutados producen hallazgos más útiles que ejercicios metodológicamente elaborados pero opacos para usuarios no especializados. 1.4 Funciones de la evaluación en el sector público La evaluación cumple múltiples funciones en el ecosistema de políticas públicas. Su función instrumental consiste en proporcionar información que mejore directamente el diseño o la implementación de intervenciones específicas. Evaluaciones de procesos, por ejemplo, identifican cuellos de botella operacionales cuya corrección incrementa la efectividad del programa sin requerir recursos adicionales. La función conceptual opera a mayor plazo, contribuyendo a transformar gradualmente la comprensión de problemas públicos y las teorías causales que sustentan las intervenciones. Incluso cuando hallazgos específicos no generan cambios inmediatos, la acumulación de evidencia evaluativa modifica paradigmas de política. La consolidación del enfoque de determinantes sociales de la salud, por ejemplo, se nutrió sustancialmente de evaluaciones que documentaron los límites de intervenciones puramente clínicas. La función simbólica o legitimadora reconoce que las evaluaciones operan en contextos políticos donde su mera existencia —independientemente de hallazgos específicos— puede cumplir propósitos de legitimación, señalización de compromiso con transparencia o clausura de controversias. Si bien esta función puede prestarse a usos puramente ceremoniales, también puede servir propósitos democráticos genuinos al dotar de credibilidad técnica a decisiones políticamente complejas. Finalmente, la función de accountability vincula evaluación con rendición de cuentas democrática. Evaluaciones independientes y públicamente accesibles permiten a ciudadanos, legisladores y organizaciones de la sociedad civil escrutar el desempeño gubernamental con información técnicamente sólida, trascendiendo narrativas oficiales no contrastadas. 1.5 Desafíos de la evaluación en Chile El sistema chileno de evaluación enfrenta desafíos estructurales que condicionan la práctica evaluativa. Primero, la fragmentación institucional implica que diferentes servicios públicos operan con capacidades evaluativas heterogéneas, dificultando evaluaciones comprehensivas de políticas que atraviesan múltiples sectores. El Plan Nacional de Cuidados, por ejemplo, requeriría coordinar evaluaciones de MINSAL, MINEDUC, Ministerio de la Mujer y otros servicios, coordinación que el marco institucional actual dificulta. Segundo, las limitaciones de datos restringen el universo de preguntas evaluativas abordables mediante fuentes secundarias. Si bien Chile cuenta con registros administrativos e instrumentos de recopilación de datos relativamente robustos en perspectiva regional, persisten brechas significativas en información de procesos, calidad de servicios y trayectorias individuales de beneficiarios que limitan especialmente evaluaciones cualitativas rigurosas. Tercero, existen tensiones entre independencia y utilidad. Evaluaciones conducidas por unidades independientes maximizan credibilidad técnica pero pueden desconectarse de las necesidades operacionales de los gestores. Evaluaciones conducidas por las propias unidades ejecutoras aseguran relevancia pero enfrentan cuestionamientos sobre objetividad. El sistema chileno ha privilegiado el primer modelo, relegando evaluaciones internas y favoreciendo estudios externos, con consecuencias mixtas para el aprendizaje organizacional. Cuarto, la discontinuidad política genera incentivos débiles para el uso sistemático de evidencia evaluativa. Horizontes de gestión cortos y alta rotación de equipos directivos desincentivan inversiones en evaluaciones cuyos resultados madurarán bajo administraciones sucesoras. Como documenta el Banco Mundial (2005), evaluaciones del sistema chileno frecuentemente se producen cuando las decisiones más relevantes ya se han tomado, limitando su influencia efectiva. 1.6 Preguntas de reflexión ¿En qué se diferencia evaluar una política pública de simplemente medir si cumplió sus metas? Piense en un programa específico. Considere un programa social que conozca. ¿Qué función de la evaluación —instrumental, conceptual, simbólica o de accountability— sería más relevante en ese caso y por qué? El principio de viabilidad sugiere que diseños metodológicamente ideales pero impracticables deben ajustarse. ¿Qué ejemplos concretos de tensiones entre rigor y viabilidad puede identificar? ¿Cómo podrían los desafíos del sistema chileno de evaluación afectar el tipo de preguntas que se pueden responder confiablemente sobre políticas públicas? 1.7 Referencias del capítulo Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. CNEP (2023). Actualización y reenfoque al sistema de evaluación de programas públicos. Comisión Nacional de Evaluación y Productividad. Banco Mundial (2005). Chile: estudio de evaluación de impacto del Programa de Evaluación de Programas. Unidad de Reducción de la Pobreza y Gestión Económica América Latina y el Caribe. Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico para la planificación, el seguimiento y la evaluación de proyectos y programas. CEPAL. "],["enfoques-y-metodologías-de-evaluación.html", "Capítulo 2 Enfoques y metodologías de evaluación 2.1 Tipología de evaluaciones según momento 2.2 Evaluación por objeto de análisis 2.3 Enfoques metodológicos principales 2.4 Criterios de evaluación OECD-DAC 2.5 Métodos cuantitativos y cualitativos 2.6 Selección de enfoque metodológico 2.7 Preguntas de reflexión 2.8 Referencias del capítulo", " Capítulo 2 Enfoques y metodologías de evaluación 2.1 Tipología de evaluaciones según momento Las evaluaciones se clasifican primariamente según el momento del ciclo de política en que se conducen. Las evaluaciones ex-ante se realizan antes de la implementación, analizando el diseño propuesto de la intervención. Su propósito es valorar si el problema está correctamente diagnosticado, si la teoría de cambio es plausible, si los objetivos son realistas y si los recursos previstos resultan apropiados. Como señala DIPRES (2015), evaluaciones ex-ante rigurosas previenen inversiones en intervenciones mal concebidas cuyo fracaso sería predecible desde el diseño. Las evaluaciones ex-post ocurren durante o después de la implementación, examinando resultados efectivos y aprendizajes operacionales. Se subdividen según foco temporal: evaluaciones de procesos durante la implementación, evaluaciones de resultados al cierre o en momentos intermedios, y evaluaciones de impacto que requieren suficiente tiempo transcurrido para que efectos sustantivos se materialicen. La distinción ex-ante/ex-post no es puramente temporal sino sustantiva. Evaluaciones ex-ante trabajan con modelos y proyecciones, juzgando plausibilidad teórica y coherencia lógica. Evaluaciones ex-post trabajan con evidencia empírica de implementación efectiva, juzgando desempeño observado contra objetivos, benchmarks o contrafactuales. Metodológicamente, las primeras privilegian análisis lógico y prospectivo; las segundas, contrastación empírica retrospectiva. 2.2 Evaluación por objeto de análisis Una segunda tipología clasifica evaluaciones según su objeto de análisis específico. La evaluación de diseño examina la arquitectura conceptual de la intervención: coherencia entre diagnóstico del problema, objetivos, teoría de cambio, población objetivo, componentes y recursos. Utiliza herramientas como el marco lógico y el árbol de problemas para identificar inconsistencias o debilidades que comprometerían la efectividad incluso con implementación impecable. La evaluación de procesos analiza la calidad, eficiencia y fidelidad de la implementación. Interroga si las actividades se ejecutan según lo planificado, si los recursos se utilizan eficientemente, si existen cuellos de botella operacionales y si los beneficiarios efectivamente reciben los bienes o servicios contemplados. Como documenta Pignatta (2015), muchos programas con diseños sólidos fracasan por deficiencias de implementación que evaluaciones de procesos habrían detectado tempranamente. La evaluación de resultados examina el logro de productos y efectos inmediatos en beneficiarios. ¿Se entregaron los bienes o servicios comprometidos? ¿Los beneficiarios modificaron comportamientos, adquirieron competencias o mejoraron condiciones según lo esperado? Estas evaluaciones típicamente comparan indicadores con líneas base o metas, sin necesariamente establecer atribución causal rigurosa. La evaluación de impacto constituye el estándar más demandante, buscando determinar efectos causalmente atribuibles a la intervención mediante diseños que controlan por factores confundentes. Requiere contrafactuales creíbles —qué habría ocurrido a los beneficiarios sin el programa— típicamente mediante grupos de control o comparación, diseños experimentales, cuasi-experimentales o métodos econométricos sofisticados. 2.3 Enfoques metodológicos principales El campo evaluativo ha generado múltiples enfoques metodológicos con supuestos, fortalezas y limitaciones distintivas. El enfoque experimental constituye el estándar de oro para estimación de impacto causal. Asigna aleatoriamente individuos o unidades a tratamiento y control, garantizando que diferencias observadas ex-post sean atribuibles al programa. Sin embargo, requiere condiciones raramente satisfechas en contextos reales: factibilidad ética y política de aleatorización, escalas suficientes para potencia estadística, capacidad de mantener la asignación aleatoria durante implementación. Los enfoques cuasi-experimentales aproximan inferencia causal sin aleatorización. Técnicas como diferencias en diferencias, regresión discontinua, variables instrumentales o matching estadístico explotan variación natural en exposición al programa para construir contrafactuales plausibles. Si bien menos robustos que experimentos, resultan frecuentemente más viables y, bajo supuestos apropiados, producen estimaciones creíbles de efectos causales. El enfoque de teoría del cambio privilegia la explicitación y contrastación de los mecanismos causales postulados entre actividades e impactos. En lugar de reducir la evaluación a estimación de efectos netos, mapea la cadena causal completa, interrogando cada eslabón. ¿Las actividades generaron los productos esperados? ¿Los productos indujeron cambios intermedios anticipados? ¿Esos cambios contribuyeron a impactos finales? Este enfoque resulta especialmente valioso para intervenciones complejas con múltiples vías causales o donde la atribución causal rigurosa es impracticable. Los enfoques de estudios de caso examinan intensivamente un número reducido de instancias de implementación. Sacrifican generalización estadística por comprensión profunda de procesos, contextos y mecanismos. Mediante combinación de fuentes documentales, entrevistas y observación, reconstruyen trayectorias de implementación identificando facilitadores, obstáculos y factores contextuales críticos. Como argumenta Nagel (2002), casos bien seleccionados generan aprendizajes teóricos y prácticos que estudios de muestra grande frecuentemente pierden. Los enfoques participativos involucran activamente a beneficiarios y otros stakeholders en el diseño, conducción e interpretación de la evaluación. Parten de reconocer que distintos actores poseen conocimientos parciales pero complementarios sobre la intervención. Metodologías participativas no solo enriquecen la comprensión evaluativa sino que, al generar apropiación de hallazgos, incrementan la probabilidad de que recomendaciones se implementen efectivamente. 2.4 Criterios de evaluación OECD-DAC El Comité de Ayuda al Desarrollo de la OCDE ha consolidado seis criterios como marco de referencia para evaluación de políticas y programas. Pertinencia interroga si los objetivos de la intervención corresponden a las necesidades, prioridades y políticas de beneficiarios y contexto. Una intervención puede implementarse exitosamente y lograr objetivos, pero ser irrelevante si esos objetivos no responden a necesidades reales o prioridades legítimas de la población objetivo. Coherencia examina la compatibilidad de la intervención con otras intervenciones en el mismo contexto. ¿Complementa, duplica o contradice otros programas? ¿Se alinea con políticas y estrategias sectoriales? Falta de coherencia genera desperdicio de recursos, confusión para beneficiarios e incluso efectos contraproducentes cuando intervenciones trabajan en direcciones opuestas. Eficacia mide el grado de logro de los objetivos, considerando su importancia relativa. No se reduce a cumplimiento de metas, sino que pondera diferencialmente el logro de distintos objetivos según su relevancia para resolver el problema central. Un programa que cumple 100% metas secundarias pero fracasa en objetivos primarios no es eficaz, incluso si formalmente “cumple” porcentajes agregados de metas. Eficiencia relaciona resultados con recursos empleados. ¿Se lograron resultados óptimos con recursos dados, o resultados dados con recursos mínimos? Evaluación de eficiencia requiere benchmarking: comparación con intervenciones similares, estándares técnicos o análisis de alternativas. Un programa puede ser eficaz pero ineficiente si resultados equivalentes podrían lograrse con menores costos. Impacto refiere a efectos de largo plazo, amplios y potencialmente indirectos atribuibles a la intervención. Trasciende resultados inmediatos para examinar transformaciones sostenidas en condiciones de vida, capacidades, instituciones o prácticas. Evaluación rigurosa de impacto demanda horizontes temporales extendidos y metodologías que establezcan atribución causal, distinguiendo efectos del programa de tendencias contextuales. Sostenibilidad pregunta si beneficios perdurarán tras finalizar la intervención. ¿Se fortalecieron capacidades locales? ¿Se modificaron instituciones de modo perdurable? ¿Existen recursos y compromisos para continuar actividades clave? Muchos programas generan beneficios durante implementación que se evaporan rápidamente al cesar el apoyo externo, revelando fragilidad de los cambios inducidos. 2.5 Métodos cuantitativos y cualitativos La evaluación puede emplear métodos cuantitativos, cualitativos o mixtos según preguntas evaluativas y disponibilidad de datos. Métodos cuantitativos operan con datos numéricos, privilegian muestras grandes, análisis estadístico y generalización. Incluyen encuestas, análisis de registros administrativos, experimentos, y técnicas econométricas diversas. Su fortaleza radica en medición precisa, identificación de patrones sistemáticos y capacidad de establecer magnitud de efectos. Sus limitaciones incluyen reduccionismo de fenómenos complejos a variables medibles, dificultad para capturar procesos y mecanismos, y dependencia de disponibilidad de datos estructurados. Métodos cualitativos trabajan con datos textuales o visuales, privilegian profundidad sobre amplitud, comprensión contextualizada sobre generalización estadística. Incluyen entrevistas semi-estructuradas, grupos focales, observación participante, análisis documental y etnografía. Su fortaleza está en capturar perspectivas de actores, comprender procesos y contextos, identificar mecanismos causales y generar hallazgos inesperados que métodos estructurados habrían omitido. Sus limitaciones incluyen potencial falta de representatividad, dificultad de generalización, riesgos de sesgo interpretativo del evaluador y intensividad de recursos. Diseños mixtos combinan ambos enfoques aprovechando sus complementariedades. Encuestas cuantitativas establecen patrones agregados mientras entrevistas cualitativas explican mecanismos. Experimentos miden efectos netos mientras estudios de caso reconstruyen procesos de implementación. Como señala García Moreno y García López (2014), evaluaciones comprehensivas de intervenciones complejas típicamente requieren triangulación de múltiples fuentes y métodos. 2.6 Selección de enfoque metodológico La selección de enfoque metodológico debe considerar múltiples factores. Primero, el propósito de la evaluación: si el objetivo primario es atribución causal rigurosa de impactos, enfoques experimentales o cuasi-experimentales resultan apropiados; si el foco está en comprensión de procesos o factores contextuales, enfoques cualitativos o de teoría del cambio tienen prioridad. Segundo, las características de la intervención: programas estandarizados con poblaciones grandes y procesos uniformes facilitan evaluaciones cuantitativas de impacto; intervenciones complejas, multifacéticas y adaptativas requieren enfoques que capturen heterogeneidad y contingencia. Tercero, la disponibilidad de datos: existencia de registros administrativos completos, líneas base o datos de panel habilita diseños cuantitativos sofisticados; ausencia de datos secundarios apropiados requiere recolección primaria cualitativa o cuantitativa. Cuarto, restricciones de tiempo y recursos: evaluaciones experimentales o recolección de datos primarios a gran escala demandan tiempos y presupuestos que frecuentemente exceden disponibilidades; métodos basados en fuentes secundarias, entrevistas focalizadas o análisis documental pueden producir hallazgos útiles con recursos acotados. Quinto, consideraciones éticas y políticas: aleatorización puede ser inaceptable cuando implica negar deliberadamente beneficios a grupos de control; evaluaciones que requieren acceso a información sensible enfrentan restricciones de confidencialidad; contextos políticamente polarizados pueden demandar metodologías de validez incuestionable. 2.7 Preguntas de reflexión Considere el Programa Acompañamiento Psicosocial (del Subsistema Seguridades y Oportunidades). ¿Qué tipo de evaluación sería prioritaria y por qué? ¿En qué circunstancias una evaluación de procesos podría ser más valiosa que una evaluación de impacto? Piense en ejemplos concretos. Analice un programa social chileno usando los seis criterios OECD-DAC. ¿Qué criterio presentaría mayores desafíos de evaluación y por qué? ¿Qué ventajas y limitaciones tendría un enfoque puramente cuantitativo versus uno puramente cualitativo para evaluar el programa PACE? 2.8 Referencias del capítulo DIPRES (2015). Evaluación Ex-Post: Conceptos y Metodologías. Ministerio de Hacienda. García Moreno, M., &amp; García López, R. (2014). La gestión para resultados en el desarrollo: avances y desafíos en América Latina y el Caribe. Banco Interamericano de Desarrollo. Nagel, S.S. (2002). Handbook of public policy evaluation. Sage Publications. Pignatta, M.A. (2015). Monitoreo y evaluación de políticas públicas en América Latina: Brechas por cerrar. Perspectivas de Políticas Públicas. "],["marco-institucional-de-la-evaluación-en-chile.html", "Capítulo 3 Marco institucional de la evaluación en Chile 3.1 Evolución histórica del sistema de evaluación 3.2 Dirección de Presupuestos (DIPRES) 3.3 Comisión Nacional de Evaluación y Productividad (CNEP) 3.4 Contraloría General de la República 3.5 Unidades evaluativas sectoriales 3.6 Acceso público a evaluaciones 3.7 Uso de evaluaciones en decisiones presupuestarias 3.8 Preguntas de reflexión 3.9 Referencias del capítulo", " Capítulo 3 Marco institucional de la evaluación en Chile 3.1 Evolución histórica del sistema de evaluación El sistema chileno de evaluación de programas públicos se consolida gradualmente a partir de la década de 1990, en el contexto de modernización del Estado post-dictadura. Su génesis responde a múltiples factores convergentes: democratización que demanda transparencia y rendición de cuentas, restricciones fiscales que exigen optimización del gasto, y paradigmas internacionales de Nueva Gestión Pública que priorizan resultados sobre procedimientos (Banco Mundial, 2005). En 1994 se institucionaliza el Sistema de Control de Gestión, requiriendo que servicios públicos formulen objetivos, metas e indicadores de desempeño asociados a programas presupuestarios. Esto marca un giro desde contabilidad de insumos hacia gestión por resultados, aunque inicialmente con desarrollo metodológico incipiente y uso limitado en asignación presupuestaria. En 1997 se crea el Programa de Evaluación de Programas Gubernamentales, institucionalizado en la Dirección de Presupuestos (DIPRES) del Ministerio de Hacienda. Este programa establece un ciclo anual de evaluaciones externas, independientes y públicas de programas seleccionados mediante negociación entre DIPRES y ministerios sectoriales. Las evaluaciones, conducidas por consultores externos mediante licitación pública, siguen metodologías estandarizadas y sus resultados informan las negociaciones presupuestarias subsecuentes. En 2001 se introducen los Programas de Mejoramiento de la Gestión (PMG), que vinculan incentivos monetarios colectivos al cumplimiento de metas institucionales. Posteriormente, la Ley de Presupuestos incorpora compromisos de gestión vinculados a evaluación. En 2012 se crea la Comisión Nacional de Evaluación y Productividad (CNEP, inicialmente llamada Comisión Nacional de Productividad), otorgando a la función evaluativa una instancia consultiva autónoma con mandato de asesorar al Presidente en materia de evaluación de políticas públicas. 3.2 Dirección de Presupuestos (DIPRES) DIPRES constituye el actor central del sistema de evaluación chileno. Como entidad dependiente del Ministerio de Hacienda, concentra responsabilidades normativas, coordinación del ciclo evaluativo anual y utilización de hallazgos en la negociación presupuestaria. Su División de Control de Gestión administra el Programa de Evaluación de Programas, definiendo metodologías, términos de referencia, supervisión de evaluaciones externas y seguimiento de recomendaciones. El modelo institucional privilegia evaluación externa. Consultoras independientes, seleccionadas mediante licitación pública, conducen los estudios siguiendo pautas metodológicas de DIPRES. Esto resguarda independencia técnica y credibilidad, evitando autoevaluaciones que enfrentarían conflictos de interés. Sin embargo, genera tensiones: evaluadores externos carecen de conocimiento institucional profundo que gestores poseen, y gestores pueden percibir las evaluaciones como ejercicios fiscalizadores antes que aprendizaje organizacional. DIPRES administra también el Sistema de Información para la Gestión Financiera del Estado (SIGFE), el Banco Integrado de Proyectos (BIP) y el Sistema de Evaluación y Control de Gestión, produciendo información integrada sobre presupuestos, inversiones y desempeño. Esta infraestructura informacional facilita evaluaciones al proporcionar datos longitudinales sobre recursos, metas y resultados, aunque con limitaciones en información de procesos y calidad de servicios. El sistema prioriza dos tipos de evaluación: Evaluaciones de Programas Gubernamentales (EPG), que examinan comprehensivamente diseño, gestión y resultados de programas específicos mediante metodologías mixtas; y Evaluaciones de Impacto, que estiman efectos causales de intervenciones mediante diseños cuantitativos. Adicionalmente, DIPRES conduce Evaluaciones Comprehensivas del Gasto que analizan sectores completos, y Análisis de Focalización que examinan si beneficios alcanzan poblaciones objetivo. 3.3 Comisión Nacional de Evaluación y Productividad (CNEP) Creada en 2012 como organismo asesor técnico autónomo del Presidente, CNEP tiene mandato de proponer políticas para mejorar la evaluación de programas públicos y la productividad de la economía chilena. A diferencia de DIPRES, CNEP no ejecuta evaluaciones sino que genera análisis transversales, propone reformas al sistema evaluativo y elabora metodologías. Su composición —siete profesionales de reconocido prestigio designados por el Presidente previo proceso de selección pública— busca asegurar independencia técnica y pluralismo. Sin capacidad de implementar directamente recomendaciones, CNEP opera mediante incidencia: produce estudios técnicamente sólidos que informan debate público y generan presión para reforma. El informe CNEP (2023) constituye el diagnóstico más comprehensivo del sistema chileno. Identifica debilidades estructurales: énfasis excesivo en evaluaciones individuales de programas versus análisis de políticas integrales; desconexión entre evaluación y procesos de diseño de políticas; debilidades de coordinación intersectorial; limitaciones de capacidades evaluativas en servicios públicos; y uso insuficiente de hallazgos para mejora continua. Propone reorientar el sistema desde fiscalización de programas hacia aprendizaje sistemático para mejora de políticas. 3.4 Contraloría General de la República Si bien su función primaria es auditoría de legalidad y regularidad del gasto, Contraloría realiza auditorías de desempeño que se solapan parcialmente con evaluación. Estas auditorías examinan economía, eficiencia y eficacia de programas desde perspectiva de cumplimiento de normativa y uso apropiado de recursos públicos. A diferencia de evaluaciones de DIPRES que interrogan pertinencia de objetivos y teorías de cambio, auditorías de Contraloría típicamente aceptan objetivos como dados y examinan su logro. La coexistencia de evaluación en DIPRES y auditoría de desempeño en Contraloría genera complementariedades pero también potenciales redundancias y confusión. Evaluaciones enfatizan aprendizaje y mejora; auditorías, accountability y corrección. Los dos enfoques requieren coordinación para evitar sobrelapamiento ineficiente de estudios sobre programas similares. 3.5 Unidades evaluativas sectoriales Ministerios y servicios públicos cuentan con capacidades evaluativas heterogéneas. Algunos sectores —salud, educación, desarrollo social— han desarrollado unidades de estudios o departamentos de evaluación relativamente robustos. Otros sectores mantienen capacidades mínimas, dependiendo críticamente del ciclo DIPRES para generar evidencia evaluativa. Esta heterogeneidad refleja diferencias en tradición de uso de evidencia, disponibilidad de profesionales especializados y culturas organizacionales. Sectores con mayor sofisticación técnica y recursos (MINSAL, MINEDUC) producen evaluaciones internas, estudios de costo-efectividad y proyecciones de impacto que complementan el ciclo DIPRES. Servicios con capacidades limitadas dependen exclusivamente de evaluaciones externas, restringiendo su autonomía analítica. DIPRES ha implementado programas de fortalecimiento de capacidades evaluativas sectoriales, reconociendo que sistemas descentralizados con competencias distribuidas producen mejor evidencia que modelos centralizados. Sin embargo, restricciones presupuestarias y rotación de personal limitan sostenibilidad de estas inversiones. 3.6 Acceso público a evaluaciones Chile destaca internacionalmente por transparencia de su sistema evaluativo. Todas las evaluaciones del programa DIPRES se publican íntegramente en el Portal de Evaluación y Control de Gestión (www.dipres.gob.cl), incluyendo informes finales, anexos metodológicos y bases de datos cuando aplicable. Esta apertura contrasta con sistemas de otros países donde evaluaciones circulan restringidamente entre tomadores de decisión. La transparencia cumple múltiples funciones. Permite escrutinio de calidad técnica por académicos y sociedad civil, incentivando rigor. Habilita uso secundario de hallazgos por investigadores, periodistas y organizaciones. Fortalece accountability al exponer desempeño gubernamental a evaluación pública. Sin embargo, accesibilidad no garantiza acceso efectivo: informes extensos, técnicamente complejos y sin síntesis amigables limitan utilización por audiencias no especializadas. El Banco Integrado de Programas Sociales (BIPS) complementa el portal de evaluaciones, proporcionando información estandarizada sobre diseño, cobertura, presupuesto y población objetivo de programas sociales. Su arquitectura facilita comparaciones entre programas y seguimiento longitudinal, aunque con limitaciones en actualización y completitud. 3.7 Uso de evaluaciones en decisiones presupuestarias El sistema chileno vincula formalmente evaluación con presupuesto: resultados de EPG informan la Ley de Presupuestos del año subsiguiente, y programas evaluados negativamente enfrentan riesgo de reducción o eliminación presupuestaria. Esta vinculación institucionalizada distingue a Chile de sistemas donde evaluaciones se producen pero su uso permanece discrecional. Sin embargo, la relación entre hallazgos evaluativos y asignaciones presupuestarias resulta menos mecánica que sugiere el diseño formal. Como documenta el Banco Mundial (2005), decisiones presupuestarias responden simultáneamente a múltiples factores: prioridades políticas del gobierno, presiones legislativas, compromisos previos y restricciones fiscales. Evaluaciones constituyen un insumo relevante pero no determinante. La implementación de recomendaciones evaluativas enfrenta desafíos. DIPRES realiza seguimiento formal de compromisos, requiriendo que servicios reporten avances en implementación de recomendaciones. No obstante, incentivos para implementación efectiva son débiles cuando recomendaciones demandan cambios organizacionales complejos, coordinación inter-ministerial o recursos adicionales en contextos de restricción fiscal. 3.8 Preguntas de reflexión ¿Qué ventajas y desventajas tiene que DIPRES —la entidad responsable del presupuesto— lidere el sistema de evaluación? Compare el rol de CNEP versus DIPRES. ¿Qué valor agregado aporta tener una comisión consultiva independiente adicional? Revise una evaluación publicada en el portal DIPRES. ¿Qué tan accesibles encuentra los hallazgos para un ciudadano sin formación técnica? ¿Cómo explicaría que evaluaciones puedan ser técnicamente rigurosas pero tener impacto limitado en decisiones de política? 3.9 Referencias del capítulo Banco Mundial (2005). Chile: estudio de evaluación de impacto del Programa de Evaluación de Programas. Unidad de Reducción de la Pobreza y Gestión Económica América Latina y el Caribe. CNEP (2023). Actualización y reenfoque al sistema de evaluación de programas públicos: una propuesta para un Estado más moderno y efectivo. Comisión Nacional de Evaluación y Productividad. CEP (2017). Un Estado para la ciudadanía. Informe de la Comisión de Modernización del Estado. Centro de Estudios Públicos. Pérez, G., &amp; Maldonado, C. (Eds.) (2015). Panorama de los sistemas nacionales de monitoreo y evaluación en América Latina. CIDE/Clear LAC. "],["gobierno-abierto-y-transparencia-en-evaluación.html", "Capítulo 4 Gobierno abierto y transparencia en evaluación 4.1 El paradigma de gobierno abierto 4.2 Transparencia evaluativa en Chile: avances y limitaciones 4.3 Participación ciudadana en procesos evaluativos 4.4 Uso de evaluaciones por organizaciones de la sociedad civil 4.5 Estrategias de comunicación de hallazgos evaluativos 4.6 Desafíos de implementación 4.7 Preguntas de reflexión 4.8 Referencias del capítulo", " Capítulo 4 Gobierno abierto y transparencia en evaluación 4.1 El paradigma de gobierno abierto El gobierno abierto constituye un modelo de gestión pública fundamentado en tres pilares interdependientes: transparencia, participación ciudadana y colaboración entre actores. Como señala el Banco Mundial (2010), este paradigma trasciende la simple apertura de información gubernamental para configurar una arquitectura relacional donde Estado y sociedad civil interactúan horizontalmente en la producción, implementación y evaluación de políticas públicas. La transparencia implica acceso ciudadano a información gubernamental comprensible, oportuna y utilizable. No se reduce a disponibilidad formal de datos sino a inteligibilidad efectiva: información técnicamente accesible pero incomprensible para audiencias no especializadas no satisface estándares de transparencia sustantiva. La participación refiere a involucramiento activo de ciudadanos y organizaciones en procesos de diseño, implementación y evaluación de políticas, reconociendo que distintos actores poseen conocimientos complementarios sobre problemas públicos y efectividad de intervenciones. La colaboración establece que gobierno, sector privado, academia y sociedad civil pueden co-producir soluciones más efectivas que las generadas unilateralmente por el Estado. Este modelo cobra especial relevancia para evaluación de políticas públicas. Como argumenta el informe CEP (2017) sobre modernización del Estado chileno, sistemas evaluativos transparentes, participativos y colaborativos generan simultáneamente mayor calidad técnica —al incorporar perspectivas diversas—, mayor legitimidad —al permitir escrutinio público— y mayor utilización de hallazgos —al generar apropiación entre actores que participaron en la evaluación. 4.2 Transparencia evaluativa en Chile: avances y limitaciones Chile destaca regionalmente por su transparencia evaluativa formal. Desde 1997, todas las evaluaciones del Programa de Evaluación de Programas Gubernamentales se publican íntegramente en el Portal de Evaluación y Control de Gestión de DIPRES (www.dipres.gob.cl). Esta apertura institucionalizada incluye informes finales completos, anexos metodológicos, bases de datos anonimizadas cuando aplicable, compromisos de implementación de recomendaciones y reportes de seguimiento de dichos compromisos. El portal organiza evaluaciones por año, ministerio, tipo de evaluación y estado de implementación de recomendaciones. Usuarios pueden descargar libremente todos los documentos sin registro previo. Adicionalmente, DIPRES publica metodologías estándar, términos de referencia de licitaciones, y reportes anuales que sintetizan el ciclo evaluativo. Esta infraestructura de transparencia contrasta marcadamente con sistemas de otros países donde evaluaciones circulan como documentos internos de gobierno con acceso restringido a tomadores de decisión. Sin embargo, accesibilidad formal no garantiza acceso efectivo. Como documenta Irarrázaval et al. (2020), la mayoría de evaluaciones DIPRES constituyen documentos técnicamente complejos, extensos (frecuentemente 100+ páginas) y redactados en lenguaje especializado que presupone familiaridad con metodologías evaluativas. Ciudadanos sin formación técnica, periodistas generalistas e incluso funcionarios públicos de áreas no especializadas enfrentan barreras significativas para interpretar y utilizar estos documentos. La ausencia sistemática de resúmenes ejecutivos amigables —síntesis de 2-3 páginas en lenguaje accesible destacando hallazgos prioritarios y recomendaciones accionables— limita severamente la democratización efectiva del acceso a evidencia evaluativa. Mientras especialistas —académicos, consultores, analistas de think tanks— utilizan extensivamente el portal DIPRES, audiencias más amplias permanecen efectivamente excluidas del acceso a esta información pública. El Banco Integrado de Programas Sociales (BIPS), administrado por el Ministerio de Desarrollo Social, complementa parcialmente el portal DIPRES al proporcionar información estandarizada sobre diseño, población objetivo, cobertura y presupuesto de programas sociales. Su interfaz más amigable y fichas sintéticas facilitan comparaciones entre programas. Sin embargo, el BIPS enfrenta limitaciones de actualización y completitud, con programas descritos de modo heterogéneo y datos frecuentemente desactualizados. 4.3 Participación ciudadana en procesos evaluativos La participación puede operar en múltiples momentos del ciclo evaluativo con diferentes niveles de intensidad. Arnstein (1969) propone una escalera de participación que distingue entre: (1) información —comunicación unidireccional de decisiones ya tomadas—, (2) consulta —recopilación de opiniones sin compromiso vinculante de incorporarlas—, (3) participación colaborativa —influencia real en decisiones—, y (4) delegación —transferencia de poder decisional a ciudadanos. En evaluación de políticas, estas modalidades se traducen en prácticas diferenciadas. En la definición de preguntas evaluativas, participación implica involucrar a beneficiarios, implementadores y otros stakeholders en identificar qué aspectos del programa resultan más críticos de examinar. Gestores pueden priorizar eficiencia operacional; beneficiarios, calidad de atención y pertinencia de prestaciones; implementadores de primera línea, viabilidad de procedimientos. Cada perspectiva enriquece el diseño evaluativo al revelar dimensiones que evaluadores externos podrían ignorar. En recolección de datos, participación opera mediante entrevistas semi-estructuradas, grupos focales, talleres participativos o encuestas que capturan experiencias, valoraciones y recomendaciones de quienes interactúan cotidianamente con el programa. Métodos participativos como cartografía social, fotovoz o relatos de vida permiten que beneficiarios documenten sus experiencias en formatos no extractivos donde ellos controlan narrativas. En interpretación de hallazgos, participación implica contrastar evidencia cuantitativa con conocimiento experiencial de implementadores y beneficiarios. Talleres de validación donde evaluadores presentan hallazgos preliminares y reciben retroalimentación permiten identificar interpretaciones erróneas, matizar conclusiones y generar recomendaciones más viables. La literatura internacional documenta que evaluaciones participativas —cuando bien diseñadas— generan múltiples beneficios: mayor validez al incorporar conocimiento local que evaluadores externos carecen, mayor legitimidad al incluir voces de afectados, mayor utilización al generar apropiación de hallazgos entre actores que participaron, y empoderamiento de beneficiarios al reconocer su expertise sobre el programa. El caso chileno muestra participación limitada y selectiva. Evaluaciones DIPRES típicamente se diseñan mediante negociación entre la División de Control de Gestión y los ministerios sectoriales responsables del programa evaluado, con involucramiento acotado incluso de gestores operacionales y prácticamente nulo de beneficiarios en la definición de preguntas evaluativas. Durante la recolección de datos, consultoras externas conducen entrevistas con directivos y funcionarios, y ocasionalmente grupos focales con beneficiarios, pero en modalidades extractivas donde ciudadanos proveen información sin participar en análisis o interpretación. Este déficit participativo refleja tensiones estructurales del sistema chileno. La priorización de independencia técnica y rigor metodológico —valores centrales del modelo— genera recelos hacia participación que podría comprometer objetividad. El énfasis en evaluación como instrumento de fiscalización presupuestaria incentiva diseños tecnocráticos sobre enfoques participativos. Restricciones de tiempo y presupuesto limitan la viabilidad de procesos participativos genuinos que demandan inversión sostenida de recursos. 4.4 Uso de evaluaciones por organizaciones de la sociedad civil Más allá de acceso gubernamental a evaluaciones para decisiones presupuestarias, sistemas de gobierno abierto aspiran a que múltiples actores —organizaciones de la sociedad civil, medios de comunicación, academia, ciudadanos individuales— utilicen evidencia evaluativa para advocacy, escrutinio de políticas, investigación o periodismo. Esta diversificación de usuarios democratiza el sistema evaluativo y genera presión para uso efectivo de hallazgos por parte de gobierno. En Chile, organizaciones especializadas —think tanks como Centro de Estudios Públicos, Fundación Sol, CIPER, Espacio Público— utilizan intensivamente evaluaciones DIPRES para análisis de políticas, reportes de investigación y columnas de opinión. Estas organizaciones cuentan con capacidades técnicas para interpretar metodologías complejas y extraer implicancias de política. Sus productos —policy briefs, estudios, artículos de prensa— traducen hallazgos técnicos a formatos más accesibles para audiencias amplias. Académicos emplean evaluaciones gubernamentales como fuentes secundarias para investigación, contrastando hallazgos con estudios propios, replicando análisis con metodologías alternativas o utilizando datos públicos para nuevas preguntas de investigación. Este uso académico genera escrutinio de calidad técnica del sistema evaluativo, identifica limitaciones metodológicas y produce conocimiento que enriquece debates de política. Medios de comunicación utilizan evaluaciones ocasionalmente, típicamente cuando hallazgos son políticamente controversiales (programas emblemáticos evaluados negativamente, casos de ineficiencia severa, denuncias de mala focalización). Sin embargo, cobertura mediática es episódica y frecuentemente superficial, reproduciendo titulares sin análisis profundo de metodologías o matices de hallazgos. Organizaciones sectoriales —sindicatos de profesores, colegios profesionales, federaciones de funcionarios— utilizan selectivamente evaluaciones cuando hallazgos respaldan sus demandas. Esto genera uso estratégico donde actores citan evidencia favorable e ignoran hallazgos inconvenientes, fenómeno que refleja naturaleza política del uso de evidencia. Ciudadanos individuales muestran uso marginal de evaluaciones, limitado a casos donde programas les afectan directamente y disponen de capital educacional para navegar documentos técnicos. Esta brecha de acceso efectivo sugiere que transparencia formal beneficia primariamente a élites técnicas y políticas, reproduciendo desigualdades epistémicas. 4.5 Estrategias de comunicación de hallazgos evaluativos Comunicación efectiva de evaluaciones requiere reconocer que distintas audiencias necesitan diferentes productos con niveles de detalle, complejidad técnica y formato adaptados a sus usos previstos. Una evaluación comprehensiva debe generar múltiples productos comunicacionales, no un único informe técnico. Para tomadores de decisión políticos y directivos superiores: Resúmenes ejecutivos de 2-3 páginas que sinteticen hallazgos priorizados según relevancia para decisiones, expliciten implicancias de política con claridad, y presenten recomendaciones accionables jerarquizadas por urgencia e impacto potencial. Lenguaje debe ser directo, evitar jerga técnica innecesaria y privilegiar visualizaciones sobre tablas densas. Para gestores operacionales del programa evaluado: Informes detallados que profundicen en dimensiones operacionales —cuellos de botella específicos, variaciones territoriales de implementación, análisis de costos unitarios, comparaciones con benchmarks sectoriales— con recomendaciones concretas sobre ajustes de procedimientos, capacitación de personal o reasignación de recursos. Estos usuarios valoran granularidad sobre síntesis. Para ciudadanía general: Síntesis visuales de 1-2 páginas con infografías, gráficos simples y lenguaje completamente desprovisto de tecnicismos. Estos productos deben responder preguntas básicas: ¿El programa cumplió sus objetivos? ¿Los recursos se usaron bien? ¿Qué mejoras se proponen? Videos cortos o podcasts pueden complementar productos escritos para audiencias con preferencias por formatos audiovisuales. Para comunidad académica y técnica: Informes completos con anexos metodológicos exhaustivos, bases de datos, especificaciones de modelos estadísticos, discusión de limitaciones y comparación con literatura especializada. Estos usuarios priorizan rigor y replicabilidad sobre accesibilidad. Para medios de comunicación: Notas de prensa sintéticas que destaquen hallazgos noticiables, contextualicen con datos accesibles, incluyan citas atribuibles y proporcionen contactos para ampliar información. Kits de prensa con gráficos descargables facilitan cobertura. Estrategias exitosas incluyen lanzamientos públicos de evaluaciones con presentaciones a múltiples audiencias, conferencias de prensa para evaluaciones de alto perfil, webinars para actores especializados, y repositorios digitales organizados temáticamente que faciliten búsqueda. Redes sociales institucionales pueden difundir hallazgos en formatos breves que dirijan a documentos completos. El desafío radica en balancear transparencia comprehensiva —publicación de informes técnicos completos— con accesibilidad diferenciada —productos adaptados a audiencias diversas. Sistemas evaluativos maduros invierten recursos significativos en comunicación, reconociendo que evaluaciones técnicamente impecables pero no comunicadas efectivamente desperdician su potencial de mejora de políticas y rendición de cuentas. 4.6 Desafíos de implementación La agenda de gobierno abierto en evaluación enfrenta obstáculos estructurales. Primero, tensiones entre independencia técnica y participación: evaluadores externos independientes maximizan credibilidad pero carecen de conocimiento contextual que implementadores y beneficiarios poseen; participación puede enriquecer comprensión pero genera riesgos de captura donde actores con intereses en el programa influencian sesgadamente hallazgos. Segundo, restricciones de recursos y tiempo: procesos participativos genuinos demandan inversión sostenida en convocatoria, facilitación, devolución de resultados e incorporación efectiva de insumos. Evaluaciones con plazos ajustados y presupuestos limitados difícilmente pueden implementar participación robusta sin comprometer calidad técnica. Tercero, capacidades diferenciadas de actores: participación efectiva requiere que ciudadanos y organizaciones cuenten con conocimientos mínimos sobre evaluación para contribuir constructivamente. Ausencia de capacidades genera participación ceremonial donde actores proveen opiniones desconectadas de evidencia o desconocimiento de trade-offs inevitables en política pública. Cuarto, cultura organizacional resistente: instituciones públicas habituadas a operar opacamente enfrentan costos de transición hacia transparencia y rendición de cuentas. Funcionarios pueden percibir evaluaciones participativas como amenazas antes que oportunidades de mejora, generando resistencias que obstaculizan acceso a información o sesgan interacciones con evaluadores. 4.7 Preguntas de reflexión Revise el portal DIPRES (www.dipres.gob.cl). Descargue una evaluación reciente. ¿Qué tan accesible encuentra el documento para un ciudadano sin formación en evaluación? ¿Qué elementos comunicacionales agregaría? Considere el Programa Acompañamiento Psicosocial. ¿En qué momentos del ciclo evaluativo involucraría a familias beneficiarias? ¿Qué metodologías participativas emplearía sin comprometer rigor técnico? Un think tank de oposición política utiliza selectivamente hallazgos de una evaluación gubernamental para criticar un programa emblemático del gobierno, ignorando matices metodológicos. ¿Cómo debería responder el evaluador? ¿Qué responsabilidades tiene sobre uso de sus hallazgos? ¿Qué ventajas y riesgos tiene que evaluaciones gubernamentales sean completamente públicas versus circulación restringida entre tomadores de decisión? 4.8 Referencias del capítulo Banco Mundial (2010). La formulación de políticas en la OCDE: Ideas para América Latina. Banco Mundial LAC. CEP (2017). Un Estado para la ciudadanía. Informe de la Comisión de Modernización del Estado. Centro de Estudios Públicos. Irarrázaval, I., Larrañaga, O., Rodríguez, J., &amp; Valdés, R. (2020). Propuestas para una mejor calidad del gasto y las políticas públicas en Chile. Centro de Políticas Públicas UC. Pérez, G., &amp; Maldonado, C. (Eds.) (2015). Panorama de los sistemas nacionales de monitoreo y evaluación en América Latina. CIDE/Clear LAC. "],["evaluación-de-diseño-de-políticas-y-programas.html", "Capítulo 5 Evaluación de diseño de políticas y programas 5.1 Concepto y propósito de la evaluación de diseño 5.2 Componentes esenciales de la evaluación de diseño 5.3 Metodología de evaluación de diseño 5.4 Criterios de juicio en evaluación de diseño 5.5 Caso aplicado: Evaluación de diseño del Programa de Acompañamiento y Acceso Efectivo (PACE) 5.6 Limitaciones y complementariedades 5.7 Preguntas de reflexión 5.8 Referencias del capítulo", " Capítulo 5 Evaluación de diseño de políticas y programas 5.1 Concepto y propósito de la evaluación de diseño La evaluación de diseño constituye una valoración sistemática de la arquitectura conceptual y operacional de una intervención, realizada típicamente en fase ex-ante —previa a implementación completa— aunque también aplicable a programas en operación para identificar debilidades estructurales que explican desempeño deficiente. Como establece DIPRES (2015), su propósito es determinar si el programa cuenta con fundamentos técnicos sólidos que hagan razonable esperar que, bajo implementación apropiada, logre sus objetivos. A diferencia de evaluaciones de resultados o impacto que interrogan desempeño observado empíricamente, la evaluación de diseño opera mediante análisis lógico de coherencia, consistencia y viabilidad. No pregunta “¿funcionó el programa?” sino “¿debería funcionar si se implementa adecuadamente?”. Esta distinción resulta crucial: programas con diseños defectuosos fracasarán independientemente de calidad de gestión, mientras programas bien diseñados pueden fracasar por deficiencias de implementación. Ortegón, Pacheco y Prieto (2005) argumentan que evaluaciones ex-ante rigurosas previenen desperdicio de recursos en intervenciones cuyo fracaso era predecible desde el diseño. En contextos de restricción fiscal, identificar tempranamente programas con fundamentos técnicos débiles —antes de invertir recursos sustantivos en implementación— constituye contribución mayor a eficiencia del gasto público que evaluar ex-post programas ya ejecutados. La evaluación de diseño cumple múltiples funciones. Para tomadores de decisión, proporciona evidencia técnica para decidir si proceder con implementación, modificar el diseño o abandonar la iniciativa. Para gestores, identifica ajustes necesarios antes que deficiencias se materialicen en fracasos operacionales. Para beneficiarios potenciales, resguarda que intervenciones efectivamente respondan a sus necesidades. Para el sistema público, fortalece cultura de planificación basada en evidencia versus improvisación. 5.2 Componentes esenciales de la evaluación de diseño 5.2.1 1. Diagnóstico del problema público El primer componente examina si el problema que motiva la intervención está correctamente identificado, caracterizado y cuantificado. Un diagnóstico robusto especifica: (a) manifestaciones observables del problema, (b) población afectada con estimaciones cuantitativas, (c) distribución territorial y temporal, (d) causas fundamentales versus síntomas, (e) tendencias históricas y proyecciones, (f) experiencias previas de abordaje. Errores comunes incluyen confundir síntomas con problemas (tratar “bajo rendimiento escolar” sin identificar sus causas), definir problemas como ausencia de soluciones (“falta de programa X”), diagnósticos sin respaldo empírico que reproducen supuestos no contrastados, y sub-estimación o sobre-estimación de magnitud que genera programas desproporcionados. Un buen diagnóstico debe ser contrastable: afirmaciones sobre magnitud, distribución y causas del problema deben respaldar en datos verificables de fuentes confiables. El estándar chileno privilegia uso de encuestas nacionales (CASEN, ENE, CENSO), registros administrativos sectoriales (MINSAL, MINEDUC, Superintendencias) y estudios académicos publicados, evitando diagnósticos basados exclusivamente en percepciones de expertos o narrativas políticas sin sustento empírico. 5.2.2 2. Pertinencia de objetivos Este componente evalúa si los objetivos del programa responden efectivamente al problema diagnosticado y se alinean con políticas superiores. Pertinencia implica correspondencia lógica: objetivos deben apuntar a resolver el problema o sus causas fundamentales, no a síntomas secundarios. Un programa cuyo diagnóstico identifica “deserción escolar por embarazo adolescente y trabajo infantil” pero cuyos objetivos se limitan a “mejorar infraestructura de establecimientos” sufre de incoherencia problema-objetivos. Adicionalmente, objetivos deben alinearse con políticas sectoriales, compromisos internacionales y prioridades gubernamentales. Un programa de primera infancia que ignore compromisos del Subsistema Chile Crece Contigo o estándares de la Convención de Derechos del Niño enfrenta cuestionamientos de coherencia sistémica. Los objetivos deben ser SMART: específicos (no ambiguos), medibles (verificables), alcanzables (realistas dado recursos y horizonte temporal), relevantes (importantes para resolver el problema) y temporizados (con plazos definidos). Objetivos vagos como “mejorar la calidad de vida” sin especificar dimensiones, magnitud y plazo resultan técnicamente deficientes. 5.2.3 3. Población objetivo y focalización La evaluación de diseño interroga si la población objetivo está correctamente definida, cuantificada y es alcanzable. Definición precisa especifica criterios de elegibilidad verificables objetivamente. Cuantificación estima tamaño de población elegible mediante fuentes estadísticas confiables. Alcanzabilidad considera si el programa cuenta con mecanismos efectivos para identificar y contactar a potenciales beneficiarios. Problemas frecuentes incluyen: (a) definiciones difusas que generan ambigüedad sobre quién califica (“familias vulnerables” sin especificar umbral operacional), (b) estimaciones de población objetivo desactualizadas o metodológicamente débiles, (c) focalización que excluye incorrectamente a población elegible o incluye a no-elegibles, (d) ausencia de mecanismos de identificación cuando población objetivo es “invisible” para sistemas administrativos. El caso chileno muestra heterogeneidad. Programas que utilizan el Registro Social de Hogares para focalización cuentan con identificación robusta de población vulnerable. Programas dirigidos a poblaciones informales, migrantes irregulares o grupos con barreras de acceso enfrentan desafíos mayores de alcanzabilidad que frecuentemente no se resuelven en el diseño. 5.2.4 4. Teoría de cambio y cadena causal El componente central de evaluación de diseño examina la teoría de cambio subyacente: el modelo causal que postula cómo las actividades del programa generarán productos, los productos inducirán resultados en beneficiarios, y los resultados contribuirán a impactos de largo plazo. Como establece la metodología CEPAL, esta cadena debe ser explícita, plausible y estar respaldada por evidencia o literatura especializada. Una teoría de cambio robusta especifica: (a) la secuencia lógica actividades → productos → resultados → impactos, (b) los mecanismos causales que vinculan cada eslabón, (c) los supuestos críticos que deben cumplirse para que la cadena opere, (d) los horizontes temporales en que se esperan efectos, (e) las condiciones contextuales que moderan efectividad. Por ejemplo, un programa de capacitación laboral podría postular: capacitaciones (actividad) → competencias técnicas adquiridas (producto) → mayor empleabilidad (resultado) → salida de pobreza (impacto). Esta cadena asume: (a) que las competencias enseñadas corresponden a demandas del mercado laboral local, (b) que beneficiarios pueden aplicar competencias sin barreras adicionales (cuidado infantil, transporte, discriminación), (c) que existe demanda laboral suficiente, (d) que empleos accesibles generan ingresos por sobre línea de pobreza. La evaluación de diseño interroga cada eslabón y supuesto. ¿Las actividades efectivamente generan los productos previstos? ¿Existe evidencia de que esos productos inducen los resultados esperados? ¿Los supuestos son plausibles dado el contexto? Teorías de cambio con eslabones débiles o supuestos implausibles predicen fracaso incluso con implementación impecable. 5.2.5 5. Componentes e intervenciones Este análisis examina si las prestaciones, servicios o transferencias que el programa proveerá son suficientes, coherentes y complementarias para lograr objetivos. Suficiencia implica que componentes cubren las causas relevantes del problema. Coherencia significa que componentes no se contradicen entre sí. Complementariedad indica que componentes se refuerzan mutuamente. Un programa para reducir deserción escolar podría incluir: (a) becas monetarias, (b) apoyo psicosocial, (c) nivelación académica, (d) orientación vocacional. Evaluación de diseño pregunta: ¿Estos componentes abordan las causas identificadas de deserción? ¿Alguna causa importante queda sin intervención? ¿Los componentes se refuerzan o generan tensiones? ¿La intensidad de cada componente es apropiada? Riesgos incluyen sobrediseño (componentes excesivos que exceden capacidad de gestión o presupuesto), subdiseño (intervención insuficiente que no modificará factores causales), incoherencia (componentes que operan en direcciones contradictorias) y desbalance (sobre-inversión en algunos componentes, sub-inversión en otros críticos). 5.2.6 6. Recursos y factibilidad operacional El componente final evalúa si recursos financieros, humanos, tecnológicos e institucionales son apropiados a los objetivos y componentes. Esto incluye: (a) suficiencia presupuestaria considerando costos unitarios realistas, (b) disponibilidad de personal calificado, (c) infraestructura y sistemas requeridos, (d) capacidades institucionales del ejecutor. Evaluación de diseño debe contrastar presupuesto propuesto con benchmarks de programas similares, verificar factibilidad de costos unitarios proyectados, identificar brechas de capacidades institucionales y evaluar si horizonte de implementación es realista. Programas ambiciosos con presupuestos insuficientes, plazos irrealistas o ejecutores sin capacidades demostradas enfrentan alta probabilidad de fracaso operacional. 5.3 Metodología de evaluación de diseño La evaluación de diseño combina múltiples métodos analíticos. Análisis documental examina documentos de diseño del programa (fundamentación, marcos lógicos, presupuestos), políticas sectoriales, estadísticas oficiales sobre el problema, evaluaciones de programas similares y literatura especializada sobre efectividad de intervenciones análogas. Entrevistas con stakeholders clave —gestores del programa, autoridades sectoriales, expertos temáticos, representantes de población objetivo— proporcionan perspectivas complementarias sobre viabilidad, pertinencia y potenciales obstáculos. Estas entrevistas no reemplazan análisis técnico sino que lo enriquecen con conocimiento experiencial. Benchmarking internacional compara el diseño propuesto con programas similares implementados en otros contextos, identificando mejores prácticas, errores frecuentes y factores críticos de éxito. Chile cuenta con ventaja comparativa en acceso a experiencias regionales documentadas por organismos como BID, CEPAL y Banco Mundial. Análisis de consistencia lógica verifica coherencia interna del diseño mediante herramientas como la Matriz de Marco Lógico y el árbol de problemas. Este análisis identifica inconsistencias, eslabones causales débiles, supuestos implausibles y brechas de diseño. Evaluación de viabilidad examina factibilidad política (¿existe respaldo político sostenido?), técnica (¿capacidades institucionales son suficientes?), administrativa (¿procedimientos son operacionalizables?) y financiera (¿recursos son realistas y sostenibles?). 5.4 Criterios de juicio en evaluación de diseño La evaluación de diseño emplea criterios específicos para emitir juicios fundamentados: Pertinencia: ¿Los objetivos corresponden al problema y necesidades de beneficiarios? ¿Se alinean con políticas superiores? Coherencia interna: ¿Existe lógica causal entre problema, objetivos, componentes y recursos? ¿Los componentes son complementarios? Coherencia externa: ¿El programa se articula apropiadamente con otras intervenciones? ¿Evita duplicaciones o contradicciones? Viabilidad política: ¿Existe respaldo político sostenido? ¿Los stakeholders relevantes apoyan el diseño? Viabilidad técnica: ¿Las intervenciones propuestas tienen respaldo en evidencia? ¿Son técnicamente factibles? Viabilidad administrativa: ¿El ejecutor cuenta con capacidades? ¿Los procedimientos son operacionalizables? Suficiencia de recursos: ¿El presupuesto es apropiado? ¿Los plazos son realistas? 5.5 Caso aplicado: Evaluación de diseño del Programa de Acompañamiento y Acceso Efectivo (PACE) El PACE, implementado desde 2014, busca restituir derecho a educación superior de estudiantes talentosos de establecimientos vulnerables mediante preparación académica y cupos garantizados en universidades. Su evaluación de diseño ex-ante habría examinado: Diagnóstico: Datos DEMRE mostraban que estudiantes de liceos vulnerables accedían marginalmente a educación superior incluso con rendimiento académico similar a pares de colegios privilegiados. Causas: menor preparación PSU, auto-exclusión por percepciones de no pertenencia, restricciones financieras. Pertinencia de objetivos: Objetivo de incrementar acceso y retención de estudiantes vulnerables en educación superior responde al problema diagnosticado y se alinea con compromisos de equidad educacional del gobierno. Población objetivo: Estudiantes del 15% de liceos de mayor vulnerabilidad que cumplan requisitos de participación. Cuantificación basada en matrículas administrativas (~ 25,000 estudiantes/cohorte). Alcanzabilidad mediante selección de establecimientos desde bases MINEDUC. Teoría de cambio: Preparación académica + cupo garantizado → competencias académicas + eliminación de barrera de acceso → ingreso y permanencia en educación superior → movilidad social. Supuestos: (a) preparación académica efectivamente desarrolla competencias, (b) cupo garantizado sin PSU no compromete desempeño universitario, (c) estudiantes cuentan con recursos para mantenerse una vez matriculados. Componentes: (a) Preparación académica desde 3° medio (talleres, tutorías), (b) apoyo psicoeducativo, (c) cupo garantizado en universidad adscrita, (d) acompañamiento en educación superior. Evaluación preguntaría: ¿Estos componentes abordan causas de inequidad de acceso? ¿Alguna causa relevante queda sin intervención? (Por ejemplo: restricciones financieras de mantención) Recursos y viabilidad: Presupuesto por estudiante, capacidades de liceos para implementar preparación, compromisos de universidades de sostener cupos, sistemas de seguimiento de trayectorias. Una evaluación de diseño robusta habría identificado fortalezas (articulación preparación-acceso-acompañamiento, compromiso institucional de universidades) y debilidades (dependencia crítica de calidad de implementación en liceos, riesgo de deserción por restricciones financieras no cubiertas, complejidad de coordinación con múltiples universidades). 5.6 Limitaciones y complementariedades La evaluación de diseño tiene limitaciones inherentes. Opera con supuestos y proyecciones, no con evidencia empírica de implementación efectiva. Un diseño técnicamente impecable puede fracasar por factores contextuales impredecibles, resistencias organizacionales o cambios en condiciones externas. Inversamente, programas con diseños deficientes ocasionalmente logran resultados por factores no anticipados. Estas limitaciones demandan complementariedad con evaluaciones de procesos (que examinan implementación efectiva), resultados (que miden logros observados) e impacto (que estiman efectos causales). El ciclo evaluativo completo requiere múltiples tipos de evaluación en diferentes momentos, no sustitución de evaluación ex-post por ex-ante. No obstante, en contextos de recursos limitados, evaluaciones de diseño bien conducidas previenen errores costosos y orientan inversiones hacia intervenciones con fundamentos sólidos. Como argumenta CNEP (2023), el sistema chileno ha privilegiado históricamente evaluación ex-post sobre ex-ante, generando aprendizajes tardíos cuando recursos ya se han comprometido. Fortalecer evaluación de diseño constituye prioridad para mejora del sistema. 5.7 Preguntas de reflexión Seleccione un programa social chileno reciente. Evalúe la coherencia entre su diagnóstico del problema y los objetivos propuestos. ¿Los objetivos responden a las causas fundamentales o solo a síntomas? Considere el programa Ingreso Ético Familiar. Construya su teoría de cambio identificando actividades, productos, resultados e impactos esperados. ¿Qué supuestos críticos debe cumplirse para que la cadena causal opere? ¿Por qué una intervención puede tener un diseño técnicamente sólido pero fracasar en implementación? Identifique 3 factores que podrían explicar esta brecha entre diseño e implementación. Compare las ventajas y limitaciones de invertir recursos en evaluación ex-ante (de diseño) versus evaluación ex-post (de impacto). ¿En qué contextos priorizaría cada una? 5.8 Referencias del capítulo DIPRES (2015). Evaluación Ex-Post: Conceptos y Metodologías. Ministerio de Hacienda. Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico para la planificación, el seguimiento y la evaluación de proyectos y programas. CEPAL. CNEP (2023). Actualización y reenfoque al sistema de evaluación de programas públicos. Comisión Nacional de Evaluación y Productividad. Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. "],["árbol-de-problemas-metodología-cepal.html", "Capítulo 6 Árbol de problemas: metodología CEPAL 6.1 Concepto y propósito 6.2 Pasos metodológicos 6.3 Errores comunes 6.4 Del árbol de problemas al árbol de objetivos 6.5 Caso aplicado: Programa Acompañamiento Psicosocial 6.6 Ejercicio práctico 6.7 Preguntas de reflexión 6.8 Referencias", " Capítulo 6 Árbol de problemas: metodología CEPAL 6.1 Concepto y propósito El árbol de problemas es una herramienta visual que estructura jerárquicamente un problema central, sus causas y sus efectos. Desarrollado por CEPAL (Ortegón, Pacheco &amp; Prieto, 2005), permite identificar relaciones causales y priorizar puntos de intervención. No se trata de simple listado de problemas, sino de análisis causal que distingue síntomas, problema central, causas directas, causas raíz y efectos de diverso nivel. 6.2 Pasos metodológicos 6.2.1 1. Identificar problema central Definir con precisión el problema que la política busca resolver. Debe ser específico, verificable, relevante socialmente. Formular como estado negativo actual, no como ausencia de solución. Ejemplo correcto: “Alta tasa de deserción escolar en educación media” Ejemplo incorrecto: “Falta de programas de retención escolar” 6.2.2 2. Identificar causas Preguntar sistemáticamente: ¿Por qué ocurre este problema? Distinguir: - Causas directas: factores inmediatos - Causas indirectas: factores estructurales subyacentes Organizar jerárquicamente. Verificar lógica causal: ¿la causa propuesta efectivamente genera el problema? 6.2.3 3. Identificar efectos Preguntar: ¿Qué consecuencias genera este problema? Distinguir: - Efectos directos: consecuencias inmediatas - Efectos indirectos: consecuencias de segundo y tercer orden 6.2.4 4. Diagramar el árbol Representar visualmente con problema central al centro, causas abajo (raíces), efectos arriba (ramas). Usar flechas para indicar dirección causal. 6.2.5 5. Validar la lógica causal Verificar que cada relación causal sea plausible. Eliminar pseudo-causas. Revisar completitud: ¿faltan causas o efectos relevantes? 6.3 Errores comunes Confundir causa con síntoma: “Bajo rendimiento escolar” puede ser síntoma de problema más profundo Confundir causa con ausencia de solución: “Falta de programas X” no es causa sino ausencia de intervención Incluir múltiples problemas centrales: El árbol debe enfocarse en un problema principal Lógica causal débil: Relaciones causales que no resisten escrutinio Exceso de complejidad: Árboles con 20+ causas pierden utilidad analítica 6.4 Del árbol de problemas al árbol de objetivos Convertir cada problema en su estado positivo correspondiente: - Problema: “Alta deserción escolar” → Objetivo: “Reducir deserción escolar” - Causas se transforman en medios - Efectos se transforman en fines Este árbol de objetivos fundamenta el diseño de componentes de intervención. 6.5 Caso aplicado: Programa Acompañamiento Psicosocial [DESARROLLAR: Árbol de problemas completo para familias en situación de vulnerabilidad. Mostrar problema central, 3-4 causas directas con causas raíz, 3-4 efectos directos con efectos indirectos. Incluir diagrama Mermaid] 6.6 Ejercicio práctico Construir árbol de problemas para: “Baja cobertura de educación parvularia en zonas rurales” Identificar 3 causas directas Para cada causa directa, identificar 2 causas raíz Identificar 3 efectos directos Para cada efecto directo, identificar 1 efecto indirecto Diagramar 6.7 Preguntas de reflexión ¿Por qué es importante distinguir entre causas y síntomas en el diagnóstico de problemas públicos? Piense en un programa que conozca. ¿Su diseño responde a causas del problema o solo a síntomas? ¿Qué limitaciones tiene el árbol de problemas para capturar problemas complejos con múltiples causas interrelacionadas? 6.8 Referencias Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico. CEPAL. "],["marco-lógico-instrumento-de-diseño-y-evaluación.html", "Capítulo 7 Marco lógico: instrumento de diseño y evaluación 7.1 Origen y propósito 7.2 Estructura de la matriz 7.3 Lógica vertical: Teoría del cambio 7.4 Lógica horizontal: Medición de logros 7.5 Indicadores en el marco lógico 7.6 Supuestos críticos 7.7 Construcción de un marco lógico 7.8 Caso aplicado: Programa Alimentación Escolar 7.9 Limitaciones del marco lógico 7.10 Preguntas de reflexión 7.11 Referencias", " Capítulo 7 Marco lógico: instrumento de diseño y evaluación 7.1 Origen y propósito La Matriz de Marco Lógico (MML) surge en años 1960s en USAID como herramienta de planificación y evaluación. CEPAL la adapta para América Latina (Ortegón, Pacheco &amp; Prieto, 2005), consolidándola como estándar regional. La MML sintetiza en formato tabular: objetivos jerárquicos, indicadores de logro, medios de verificación y supuestos críticos de una intervención. Su valor está en obligar explicitación de teoría de cambio y establecer base mensurable para evaluación. 7.2 Estructura de la matriz La MML organiza información en 4 filas (niveles de objetivos) y 4 columnas: 7.2.1 Filas: Jerarquía de objetivos Fin: Objetivo de desarrollo de largo plazo al que contribuye el programa Propósito: Efecto directo esperable si el programa se ejecuta exitosamente Componentes: Productos/servicios que el programa debe entregar Actividades: Acciones requeridas para generar cada componente 7.2.2 Columnas Resumen narrativo: Descripción de objetivos Indicadores: Medidas verificables de logro Medios de verificación: Fuentes de información para cada indicador Supuestos: Factores externos críticos para éxito 7.3 Lógica vertical: Teoría del cambio La MML expresa teoría causal ascendente: - SI se ejecutan actividades → ENTONCES se generan componentes - SI se entregan componentes → ENTONCES se logra propósito - SI se logra propósito → ENTONCES se contribuye al fin Cada relación está condicionada por supuestos: factores externos que deben cumplirse. 7.4 Lógica horizontal: Medición de logros Para cada nivel de objetivo: - Indicador especifica QUÉ se medirá como evidencia de logro - Medio de verificación establece DÓNDE se obtendrá esa información - Supuesto identifica QUÉ condiciones externas deben darse 7.5 Indicadores en el marco lógico Indicadores deben cumplir criterios SMART: - Specific (específicos) - Measurable (medibles) - Achievable (alcanzables) - Relevant (relevantes) - Time-bound (temporalmente definidos) Tipos de indicadores según nivel: - Fin: Indicadores de impacto (cambios de largo plazo) - Propósito: Indicadores de resultado/efecto - Componentes: Indicadores de producto - Actividades: Indicadores de proceso 7.6 Supuestos críticos Supuestos son factores externos al control del programa pero necesarios para éxito. Ejemplos: - Contexto macroeconómico estable - Continuidad de políticas complementarias - Disponibilidad de contrapartes institucionales - Participación de beneficiarios Identificar supuestos críticos permite: 1. Evaluar riesgos ex-ante 2. Diseñar estrategias de mitigación 3. Interpretar correctamente fracasos (¿falló programa o supuestos?) 7.7 Construcción de un marco lógico 7.7.1 Paso 1: Del árbol de problemas al árbol de objetivos Convertir cada elemento del árbol de problemas en estado positivo. 7.7.2 Paso 2: Identificar jerarquía de objetivos Fin: ¿A qué objetivo superior contribuye? Propósito: Reformulación positiva del problema central Componentes: Medios transformados en productos/servicios Actividades: Acciones para generar cada componente 7.7.3 Paso 3: Formular indicadores Para cada nivel, definir 2-3 indicadores SMART con línea base y meta. 7.7.4 Paso 4: Identificar medios de verificación ¿Dónde están disponibles los datos? Registros administrativos, encuestas, bases de datos oficiales. 7.7.5 Paso 5: Identificar supuestos Para cada nivel, preguntar: ¿Qué factores externos podrían impedir que el siguiente nivel se alcance? 7.8 Caso aplicado: Programa Alimentación Escolar [DESARROLLAR CASO COMPLETO: MML de programa PAE con todos sus componentes] Ejemplo estructura: | Resumen Narrativo | Indicadores | Medios Verificación | Supuestos | |——————-|————-|———————|———–| | FIN: Mejorar estado nutricional de niños vulnerables | % de niños con malnutrición | Encuesta MINSAL | Políticas de salud escolar continúan | | PROPÓSITO: Asegurar alimentación adecuada en horario escolar | [completar] | [completar] | [completar] | | etc. | | | | 7.9 Limitaciones del marco lógico Linealidad excesiva: Asume relaciones causales simples; dificultad con intervenciones complejas Rigidez: Una vez definido, incentiva resistencia a adaptaciones necesarias Foco en programas individuales: Dificultad para capturar sinergias entre programas Énfasis en lo medible: Riesgo de ignorar dimensiones cualitativas importantes 7.10 Preguntas de reflexión Seleccione un programa. Construya su marco lógico identificando fin, propósito, 3 componentes y actividades clave. ¿Por qué es importante distinguir entre propósito (resultado directo) y fin (impacto de largo plazo)? ¿Qué sucede si un programa cumple todos sus componentes pero no logra su propósito? ¿Qué podría explicarlo? 7.11 Referencias Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico. CEPAL. Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. "],["indicadores-de-desempeño-y-líneas-base.html", "Capítulo 8 Indicadores de desempeño y líneas base 8.1 Concepto de indicador 8.2 Tipología de indicadores 8.3 Criterios SMART 8.4 Línea base 8.5 Construcción de indicadores 8.6 Errores comunes 8.7 Caso: Sistema de indicadores para programa de capacitación laboral 8.8 Ejercicio práctico 8.9 Preguntas de reflexión 8.10 Referencias", " Capítulo 8 Indicadores de desempeño y líneas base 8.1 Concepto de indicador Un indicador es una medida que permite evaluar el desempeño de un programa en relación con objetivos. Según Bonnefoy y Armijo (2005), un buen indicador cumple funciones de: medición, comparación (con metas o benchmarks), y señalización (alertar sobre desviaciones). 8.2 Tipología de indicadores 8.2.1 Por dimensión evaluada Economía: Costo de recursos (ej: costo unitario por beneficiario) Eficiencia: Relación producto/insumo (ej: beneficiarios atendidos por $ millón) Eficacia: Grado de cumplimiento de metas (ej: % de meta de cobertura lograda) Calidad: Atributos de productos/servicios (ej: % de usuarios satisfechos) 8.2.2 Por nivel de objetivo (marco lógico) Insumos: Recursos asignados Procesos: Actividades ejecutadas Productos: Bienes/servicios entregados Resultados: Efectos en beneficiarios Impactos: Cambios de largo plazo 8.3 Criterios SMART Specific: Específico sobre qué mide Measurable: Cuantificable objetivamente Achievable: Alcanzable con recursos disponibles Relevant: Pertinente al objetivo que representa Time-bound: Con período de medición definido 8.4 Línea base La línea base es la medición inicial del indicador, previa a la intervención. Establece el punto de partida contra el cual se medirá cambio. Sin línea base confiable, es imposible atribuir cambios al programa versus tendencias preexistentes. 8.4.1 Fuentes para líneas base Registros administrativos del programa Encuestas nacionales (CASEN, ENE, etc.) Bases de datos sectoriales (MINSAL, MINEDUC, etc.) Estudios ad-hoc si datos secundarios no existen 8.5 Construcción de indicadores 8.5.1 Paso 1: Identificar el objetivo a medir ¿Qué aspecto específico del desempeño se busca evaluar? 8.5.2 Paso 2: Formular el indicador Definir fórmula clara. Ejemplos: - (Nº de beneficiarios atendidos / Nº de beneficiarios meta) * 100 - Costo total del programa / Nº de beneficiarios 8.5.3 Paso 3: Establecer línea base y meta Línea base: Valor inicial Meta: Valor esperado al término del período 8.5.4 Paso 4: Definir medio de verificación ¿De dónde provendrán los datos? ¿Con qué frecuencia se actualizarán? 8.5.5 Paso 5: Asignar responsable ¿Quién generará y reportará el indicador? 8.6 Errores comunes Indicadores no medibles: “Mejorar bienestar” sin especificar cómo se mide Confundir actividades con resultados: “N° de talleres realizados” no mide cambio en beneficiarios Metas irrealistas: Proponer mejoras del 100% en un año Falta de línea base: No se sabe desde dónde se parte Indicadores sin fuente verificable: No existe dato disponible 8.7 Caso: Sistema de indicadores para programa de capacitación laboral [DESARROLLAR TABLA: - 2 indicadores de producto - 2 indicadores de resultado - 1 indicador de eficiencia Con fórmulas, línea base, meta, medio de verificación] 8.8 Ejercicio práctico Para el Programa de Alimentación Escolar (PAE): 1. Formule 1 indicador de cobertura (producto) 2. Formule 1 indicador de resultado (nutricional) 3. Formule 1 indicador de eficiencia 4. Para cada uno, establezca: fórmula, línea base hipotética, meta, medio de verificación 8.9 Preguntas de reflexión ¿Por qué medir solo productos (ej: becas entregadas) es insuficiente para evaluar éxito de un programa? Un programa logra 100% de su meta de cobertura pero beneficiarios no mejoran. ¿Qué podría estar fallando? ¿Cuándo es aceptable no tener línea base para un indicador? 8.10 Referencias Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. DIPRES (2015). Evaluación Ex-Post: Conceptos y Metodologías. Ministerio de Hacienda. "],["sistemas-de-seguimiento-y-monitoreo.html", "Capítulo 9 Sistemas de seguimiento y monitoreo 9.1 Diferencia entre monitoreo y evaluación 9.2 Componentes de un sistema de monitoreo 9.3 Monitoreo en el ciclo de gestión 9.4 Caso: Sistema de monitoreo de programa social 9.5 Referencias", " Capítulo 9 Sistemas de seguimiento y monitoreo 9.1 Diferencia entre monitoreo y evaluación Monitoreo: Seguimiento continuo de actividades, productos y uso de recursos. Pregunta: ¿Se está ejecutando según planificado? Evaluación: Análisis periódico de resultados e impactos. Pregunta: ¿Se están logrando los objetivos? ¿La intervención es efectiva? Monitoreo es continuo y descriptivo. Evaluación es episódica y analítica. 9.2 Componentes de un sistema de monitoreo Indicadores de seguimiento: Cobertura, ejecución presupuestaria, actividades realizadas Fuentes de información: Registros administrativos, reportes de gestión Periodicidad: Mensual, trimestral, anual Responsables: Unidades de control de gestión Productos: Tableros de control, informes de avance Uso: Correcciones operacionales, alertas tempranas 9.3 Monitoreo en el ciclo de gestión Monitoreo alimenta evaluación al proveer información longitudinal sobre implementación. Programas sin sistemas de monitoreo robustos dificultan evaluaciones posteriores por carencia de datos de proceso. 9.4 Caso: Sistema de monitoreo de programa social [DESARROLLAR: Tablero de control con indicadores de proceso, producto, cobertura, presupuesto. Incluir alertas y umbrales] 9.5 Referencias Bonnefoy &amp; Armijo (2005); Ortegón, Pacheco &amp; Prieto (2005) "],["evaluación-de-procesos-y-gestión.html", "Capítulo 10 Evaluación de procesos y gestión 10.1 Concepto de evaluación de procesos 10.2 Preguntas típicas 10.3 Métodos de evaluación de procesos 10.4 Utilidad 10.5 Caso: Evaluación de procesos del Subsistema Chile Crece Contigo 10.6 Referencias", " Capítulo 10 Evaluación de procesos y gestión 10.1 Concepto de evaluación de procesos Examina CÓMO se implementa un programa: fidelidad al diseño, calidad de ejecución, eficiencia operacional, obstáculos y facilitadores. Identifica brechas entre implementación planificada y real. 10.2 Preguntas típicas ¿Las actividades se ejecutan según cronograma y calidad esperada? ¿Los beneficiarios reciben productos/servicios según diseño? ¿Existen cuellos de botella operacionales? ¿Qué factores facilitan u obstaculizan la implementación? ¿Cómo varía la implementación entre territorios o unidades ejecutoras? 10.3 Métodos de evaluación de procesos Análisis de registros administrativos: Cobertura real, tiempos de entrega, costos unitarios Entrevistas a implementadores: Gestores, operadores de ventanilla, proveedores Observación directa: Visitas a puntos de atención Grupos focales con beneficiarios: Experiencias de acceso y calidad Análisis de reclamos/sugerencias 10.4 Utilidad Evaluaciones de procesos son especialmente valiosas cuando: - Programas nuevos sin evidencia previa de implementación - Programas con resultados débiles (para identificar si el problema está en diseño o ejecución) - Programas descentralizados con variabilidad territorial - Preparación para escalamiento 10.5 Caso: Evaluación de procesos del Subsistema Chile Crece Contigo [DESARROLLAR: Análisis de tiempos de espera, cobertura efectiva vs nominal, calidad de atención según usuarios] 10.6 Referencias Pignatta (2015); DIPRES (2015) "],["evaluación-de-resultados-criterios-oecd-dac.html", "Capítulo 11 Evaluación de resultados: Criterios OECD-DAC 11.1 Los seis criterios OECD-DAC 11.2 Aplicación práctica de criterios 11.3 Caso: Aplicación de criterios OECD-DAC al programa PACE 11.4 Referencias", " Capítulo 11 Evaluación de resultados: Criterios OECD-DAC 11.1 Los seis criterios OECD-DAC Desarrollados por el Comité de Ayuda al Desarrollo de la OCDE para evaluación de cooperación internacional. Aplicables a políticas y programas públicos. 11.1.1 1. Pertinencia ¿Los objetivos del programa corresponden a las necesidades y prioridades de beneficiarios y contexto? Métodos: Análisis documental, comparación con diagnósticos sectoriales, consultas a beneficiarios y expertos. 11.1.2 2. Coherencia ¿El programa es compatible con otras intervenciones en el mismo contexto? ¿Se complementa o duplica con otros programas? ¿Está alineado con políticas sectoriales? 11.1.3 3. Eficacia ¿El programa logró sus objetivos? Considerar importancia relativa de distintos objetivos. Un programa puede cumplir metas secundarias pero fracasar en el objetivo principal. 11.1.4 4. Eficiencia ¿Los resultados se lograron con uso óptimo de recursos? Requiere benchmarking: comparar con programas similares, estándares técnicos, análisis de alternativas. 11.1.5 5. Impacto Efectos de largo plazo, amplios y potencialmente indirectos. Trasciende resultados inmediatos. Requiere establecer atribución causal distinguiendo efectos del programa de tendencias contextuales. 11.1.6 6. Sostenibilidad ¿Los beneficios perdurarán tras finalizar la intervención? ¿Se fortalecieron capacidades locales? ¿Existen recursos para continuar? 11.2 Aplicación práctica de criterios Cada criterio requiere: - Pregunta evaluativa específica - Indicadores apropiados - Fuentes de evidencia - Métodos de análisis 11.3 Caso: Aplicación de criterios OECD-DAC al programa PACE [DESARROLLAR: Evaluar PACE con los 6 criterios. Tabla con hallazgos por criterio] 11.4 Referencias OECD-DAC (2019); DIPRES (2015) "],["evaluación-de-impacto-fundamentos.html", "Capítulo 12 Evaluación de impacto: Fundamentos 12.1 Concepto de evaluación de impacto 12.2 El problema del contrafactual 12.3 Métodos experimentales 12.4 Métodos cuasi-experimentales 12.5 Requerimientos para evaluación de impacto 12.6 Caso: Evaluación de impacto del Subsidio Preferencial Escolar (SEP) 12.7 Referencias", " Capítulo 12 Evaluación de impacto: Fundamentos 12.1 Concepto de evaluación de impacto Busca determinar efectos causalmente atribuibles a una intervención. Pregunta clave: ¿Qué habría ocurrido a los beneficiarios sin el programa? (contrafactual) La diferencia entre el resultado observado y el contrafactual es el impacto del programa. 12.2 El problema del contrafactual No podemos observar simultáneamente al mismo individuo con y sin tratamiento. Solución: construir grupos de comparación que se asemejen a beneficiarios en características observables y no observables. 12.3 Métodos experimentales Experimentos controlados aleatorizados (RCT): Asignación aleatoria a tratamiento y control garantiza que grupos son estadísticamente idénticos pre-intervención. Diferencias post-intervención son atribuibles al programa. Ventajas: Estándar de oro para inferencia causal Limitaciones: Requisitos éticos, políticos, administrativos. Escalas mínimas. Costos. Validez externa limitada. 12.4 Métodos cuasi-experimentales Cuando aleatorización no es factible, métodos cuasi-experimentales construyen contrafactuales plausibles: 12.4.1 Diferencias en diferencias Compara cambios en grupo tratado vs grupo control antes y después de la intervención. Asume tendencias paralelas. 12.4.2 Regresión discontinua Explota umbrales de elegibilidad. Compara individuos justo por encima y debajo del umbral (similares excepto en elegibilidad). 12.4.3 Variables instrumentales Utiliza variación exógena en participación para estimar efectos causales. 12.4.4 Matching Construye grupo control emparejando individuos con características observables similares a tratados. 12.5 Requerimientos para evaluación de impacto Teoría de cambio clara: ¿Qué impactos se esperan? ¿En qué plazo? Datos longitudinales: Línea base + seguimiento(s) Variación en exposición al programa: Algunos reciben, otros no (al menos temporalmente) Tamaño muestral suficiente: Potencia estadística Tiempo suficiente: Impactos pueden tardar en materializarse 12.6 Caso: Evaluación de impacto del Subsidio Preferencial Escolar (SEP) [DESARROLLAR: Diseño cuasi-experimental usando variación en timing de implementación. Resultados en aprendizajes] 12.7 Referencias Banco Mundial (2005); DIPRES (2015); García Moreno &amp; García López (2014) "],["métodos-cuantitativos-de-evaluación.html", "Capítulo 13 Métodos cuantitativos de evaluación 13.1 Encuestas de evaluación 13.2 Análisis de registros administrativos 13.3 Técnicas econométricas básicas 13.4 Software estadístico 13.5 Limitaciones de métodos cuantitativos 13.6 Caso: Análisis de impacto del Programa Ingreso Ético Familiar 13.7 Referencias", " Capítulo 13 Métodos cuantitativos de evaluación 13.1 Encuestas de evaluación Diseñadas específicamente para medir resultados del programa en beneficiarios y contrafactuales. Incluyen: - Variables de resultado (outcomes) - Características sociodemográficas - Participación en el programa - Otras intervenciones recibidas 13.2 Análisis de registros administrativos Muchos programas mantienen bases de datos de beneficiarios con información longitudinal. Ventajas: bajo costo, cobertura completa. Limitaciones: variables limitadas a registros administrativos, calidad variable. 13.3 Técnicas econométricas básicas 13.3.1 Regresión lineal múltiple Estima efectos de tratamiento controlando por covariables observables. Asume selección en observables. 13.3.2 Propensity Score Matching Empareja tratados con controles de probabilidad similar de participar según características observables. 13.3.3 Diferencias en diferencias Y_impacto = (Y_tratados,post - Y_tratados,pre) - (Y_control,post - Y_control,pre) 13.3.4 Regresión discontinua Compara individuos inmediatamente por encima y debajo de umbral de elegibilidad. 13.4 Software estadístico R, Stata, Python son herramientas estándar. Requieren conocimientos estadísticos sólidos y comprensión de supuestos de cada técnica. 13.5 Limitaciones de métodos cuantitativos Reducen fenómenos complejos a variables medibles No capturan procesos ni mecanismos causales Requieren muestras grandes Dependientes de disponibilidad y calidad de datos Supuestos técnicos no siempre verificables 13.6 Caso: Análisis de impacto del Programa Ingreso Ético Familiar [DESARROLLAR: Diseño cuasi-experimental con PSM. Resultados en ingresos laborales y pobreza] 13.7 Referencias DIPRES (2015); Banco Mundial (2005) "],["análisis-de-efectos-estructurales-y-sinergias.html", "Capítulo 14 Análisis de efectos estructurales y sinergias 14.1 Efectos directos vs indirectos 14.2 Efectos estructurales 14.3 Sinergias entre programas 14.4 Metodologías para capturar efectos complejos 14.5 Caso: Efectos territoriales del Programa de Recuperación de Barrios 14.6 Referencias", " Capítulo 14 Análisis de efectos estructurales y sinergias 14.1 Efectos directos vs indirectos Efectos directos: Cambios en beneficiarios directos del programa (ej: niños que reciben alimentación escolar mejoran asistencia) Efectos indirectos: Cambios en no-beneficiarios por externalidades (ej: compañeros de beneficiarios también mejoran asistencia por efecto par) 14.2 Efectos estructurales Cambios en instituciones, mercados o prácticas que persisten más allá del programa individual. Ejemplos: - Programa de capacitación docente que transforma currículo de formación inicial - Subsidio de vivienda que modifica mercado inmobiliario local - Programa de evaluación que crea cultura de uso de evidencia 14.3 Sinergias entre programas Programas que operan complementariamente pueden generar efectos superiores a la suma de efectos individuales. Ejemplos: - Chile Crece Contigo + Programas de empleo femenino: Sinergia en desarrollo infantil y autonomía económica materna - Programas de salud + Programas educacionales: Niños saludables aprenden más 14.4 Metodologías para capturar efectos complejos Requieren diseños que trascienden comparación simple tratado/control: - Análisis de redes: Capturar efectos de contagio - Modelos multinivel: Efectos individuales, comunitarios, territoriales - Estudios longitudinales: Seguimiento de largo plazo - Métodos mixtos: Combinar medición cuantitativa con comprensión cualitativa de mecanismos 14.5 Caso: Efectos territoriales del Programa de Recuperación de Barrios [DESARROLLAR: Efectos en beneficiarios directos, en residentes no participantes, en valorización inmobiliaria, en capital social comunitario] 14.6 Referencias García Moreno &amp; García López (2014); Zapata &amp; Tejeda (2009) "],["casos-de-evaluación-en-chile.html", "Capítulo 15 Casos de evaluación en Chile 15.1 Caso 1: Evaluación del Subsistema de Protección Integral a la Infancia Chile Crece Contigo 15.2 Caso 2: Programa de Acompañamiento y Acceso Efectivo a la Educación Superior (PACE) 15.3 Caso 3: Pensión Garantizada Universal (PGU) 15.4 Preguntas de reflexión 15.5 Referencias", " Capítulo 15 Casos de evaluación en Chile 15.1 Caso 1: Evaluación del Subsistema de Protección Integral a la Infancia Chile Crece Contigo 15.1.1 Contexto y diseño Chile Crece Contigo (ChCC) constituye el subsistema de protección integral a la primera infancia, operando desde 2007. Integra prestaciones de salud, educación y protección social desde gestación hasta ingreso al sistema escolar. 15.1.2 Evaluación de diseño [DESARROLLAR: Análisis de teoría de cambio, coherencia entre componentes, articulación intersectorial] 15.1.3 Evaluación de resultados [DESARROLLAR: Cobertura, acceso a prestaciones, satisfacción usuarias] 15.1.4 Evaluación de impacto Estudios cuantitativos han documentado efectos en: desarrollo psicomotor infantil, lactancia materna, controles de salud. Metodología: diferencias en diferencias aprovechando rollout gradual. 15.1.5 Lecciones aprendidas [SINTETIZAR: Fortalezas del diseño integrado, desafíos de coordinación intersectorial, variabilidad en calidad de implementación territorial] 15.2 Caso 2: Programa de Acompañamiento y Acceso Efectivo a la Educación Superior (PACE) 15.2.1 Contexto PACE busca restituir derecho a educación superior de estudiantes de establecimientos vulnerables mediante preparación académica desde 3° medio y cupos garantizados en universidades adscritas. 15.2.2 Diseño [DESARROLLAR: Componentes de preparación + acompañamiento + cupo garantizado. Teoría de cambio] 15.2.3 Resultados preliminares [SINTETIZAR: Tasas de acceso y retención de beneficiarios PACE vs grupo comparación] 15.2.4 Desafíos evaluativos Selección compleja (no aleatoria), múltiples cohortes, efectos de mediano plazo, heterogeneidad entre liceos. 15.3 Caso 3: Pensión Garantizada Universal (PGU) 15.3.1 Contexto Reemplaza a Pensión Básica Solidaria y Aporte Previsional Solidario. Cobertura 90% de población mayor de 65 años con menores pensiones. 15.3.2 Evaluación ex-ante [DESARROLLAR: Proyecciones de cobertura, costo fiscal, efectos distributivos, riesgos de incentivos] 15.3.3 Primeros resultados [SINTETIZAR: Cobertura efectiva, problemas de focalización, efectos en pobreza de adultos mayores] 15.3.4 Debates pendientes Sostenibilidad fiscal, incentivos a cotizar, coordinación con sistema contributivo. 15.4 Preguntas de reflexión Compare fortalezas y debilidades evaluativas de los tres casos. ¿Cuál enfrenta mayores desafíos metodológicos? Seleccione uno e identifique preguntas evaluativas aún sin responder. ¿Qué rol juega el contexto político en la evaluación de cada programa? 15.5 Referencias Evaluaciones DIPRES de ChCC, PACE; Documentos CNEP sobre PGU "],["elaboración-de-informes-de-evaluación.html", "Capítulo 16 Elaboración de informes de evaluación 16.1 Estructura estándar de informe evaluativo 16.2 Principios de redacción 16.3 Visualización de datos evaluativos 16.4 Comunicación a diferentes audiencias 16.5 Caso: Análisis de informe de evaluación DIPRES 16.6 Referencias", " Capítulo 16 Elaboración de informes de evaluación 16.1 Estructura estándar de informe evaluativo 16.1.1 1. Resumen Ejecutivo (2-3 páginas) Contexto del programa Preguntas evaluativas Metodología sintética Hallazgos principales (priorizados) Recomendaciones clave 16.1.2 2. Introducción Antecedentes del programa Propósito de la evaluación Usuarios previstos del informe Estructura del documento 16.1.3 3. Descripción del programa Contexto y problema que aborda Objetivos y teoría de cambio Componentes y prestaciones Población objetivo y cobertura Presupuesto y estructura institucional 16.1.4 4. Metodología de evaluación Preguntas evaluativas Diseño metodológico Fuentes de información Técnicas de análisis Limitaciones metodológicas 16.1.5 5. Hallazgos Organizar por preguntas evaluativas o criterios (pertinencia, eficacia, etc.). Cada hallazgo debe: - Estar respaldado por evidencia específica - Distinguir entre hechos e interpretaciones - Considerar explicaciones alternativas - Citar fuentes de información 16.1.6 6. Conclusiones Síntesis de hallazgos principales respondiendo preguntas evaluativas. Evitar introducir nueva evidencia. 16.1.7 7. Recomendaciones Accionables, específicas, priorizadas, viables. Cada recomendación debe: - Vincularse a hallazgo específico - Especificar a quién va dirigida - Indicar urgencia relativa - Considerar viabilidad política/administrativa 16.1.8 8. Anexos Metodología detallada, instrumentos de recolección, tablas estadísticas, bibliografía. 16.2 Principios de redacción Claridad: Lenguaje directo, párrafos concisos, estructura lógica Precisión: Evitar ambigüedades, cuantificar cuando posible Objetividad: Distinguir hechos de juicios, fundamentar interpretaciones Accesibilidad: Explicar jerga técnica, usar visualizaciones Utilidad: Foco en información accionable para decisiones 16.3 Visualización de datos evaluativos Gráficos efectivos: - Barras comparativas: Comparar resultados entre grupos o períodos - Series de tiempo: Evolución de indicadores - Mapas: Variación territorial - Tablas resumidas: Hallazgos multidimensionales Principios: Simplicidad, títulos claros, ejes etiquetados, fuentes citadas. 16.4 Comunicación a diferentes audiencias Tomadores de decisión: Resumen ejecutivo con implicancias claras Gestores: Análisis operacional detallado con recomendaciones específicas Ciudadanía: Síntesis accesible, visualizaciones, lenguaje no técnico Comunidad académica/técnica: Anexo metodológico completo 16.5 Caso: Análisis de informe de evaluación DIPRES [EJERCICIO: Revisar un informe DIPRES. Evaluar si cumple con estructura estándar. Identificar fortalezas y debilidades de redacción y visualización] 16.6 Referencias DIPRES (2015); Guías metodológicas de evaluación "],["ética-y-buenas-prácticas-en-evaluación.html", "Capítulo 17 Ética y buenas prácticas en evaluación 17.1 Principios éticos fundamentales 17.2 Dilemas éticos comunes 17.3 Estándares profesionales 17.4 Comités de ética 17.5 Caso: Dilemas éticos en evaluación de programa de salud mental 17.6 Preguntas de reflexión 17.7 Referencias", " Capítulo 17 Ética y buenas prácticas en evaluación 17.1 Principios éticos fundamentales 17.1.1 1. Respeto a personas Consentimiento informado de participantes Protección de poblaciones vulnerables Confidencialidad de información personal Derecho a no participar sin consecuencias 17.1.2 2. Beneficencia y no maleficencia Maximizar beneficios, minimizar daños Evaluar riesgos de la evaluación misma Considerar efectos no anticipados Proteger a beneficiarios en evaluaciones experimentales 17.1.3 3. Justicia Distribución equitativa de beneficios y cargas de evaluación No excluir sistemáticamente a grupos de beneficios probados Considerar implicancias distributivas de hallazgos 17.1.4 4. Integridad Honestidad en reporte de hallazgos Transparencia metodológica Reconocimiento de limitaciones Evitar conflictos de interés 17.2 Dilemas éticos comunes 17.2.1 ¿Negar tratamiento a grupos de control? Evaluaciones experimentales asignan aleatoriamente algunos elegibles a control, negándoles temporalmente beneficios. ¿Es esto ético? Depende de: - ¿Existe evidencia previa de efectividad? - ¿Los recursos alcanzarían para todos de todos modos? - ¿El diseño minimiza tiempo sin tratamiento? - ¿Se ofrece tratamiento al grupo control posteriormente? 17.2.2 ¿Usar datos administrativos sin consentimiento? Registros administrativos contienen información personal. ¿Es legítimo usarlos sin consentimiento individual explícito? Requiere: - Anonimización rigurosa - Propósitos claramente de interés público - Resguardos de seguridad de datos - Marcos legales apropiados (Ley de Protección de Datos) 17.2.3 ¿Reportar hallazgos negativos que amenacen programas? Evaluaciones pueden encontrar que programas políticamente valorados son inefectivos. ¿El evaluador debe reportar honestamente incluso si esto compromete el programa? Principio: La integridad técnica requiere honestidad. Pero comunicación debe considerar: - Solidez de evidencia - Explicaciones alternativas - Contexto de toma de decisiones - Responsabilidad con beneficiarios que dependen del programa 17.2.4 ¿Quién es el cliente del evaluador? Evaluador responde simultáneamente a múltiples audiencias: quién paga la evaluación, gestores del programa, beneficiarios, ciudadanía. Pueden existir tensiones entre sus intereses. Resguardos: Independencia técnica, transparencia de financiamiento, publicación de hallazgos. 17.3 Estándares profesionales Organizaciones como American Evaluation Association han desarrollado códigos éticos y estándares de calidad. Principios clave: - Competencia técnica - Integridad - Respeto por las personas - Responsabilidad por el bien común - Transparencia 17.4 Comités de ética Evaluaciones que involucran recolección primaria de datos con seres humanos deben someterse a revisión ética. Comités evalúan: - Riesgos para participantes - Procedimientos de consentimiento - Protección de confidencialidad - Balance riesgo/beneficio - Calificaciones de evaluadores 17.5 Caso: Dilemas éticos en evaluación de programa de salud mental [DESARROLLAR: Escenario con información sensible, poblaciones vulnerables, uso de grupos control. Discutir cómo balancear rigor evaluativo con protección de participantes] 17.6 Preguntas de reflexión ¿En qué circunstancias sería éticamente aceptable negar temporalmente beneficios a un grupo control? Un evaluador descubre que un programa popular es inefectivo. ¿Cómo debe comunicar esto sin dañar innecesariamente a beneficiarios que dependen del programa? ¿Qué resguardos implementaría para proteger confidencialidad en una evaluación que utiliza datos administrativos sensibles? 17.7 Referencias American Evaluation Association (2018). Guiding Principles for Evaluators Nagel, S.S. (2002). Handbook of public policy evaluation. Sage. Zapata, G., &amp; Tejeda, I. (2009). Consideraciones éticas en evaluación "],["referencias-12.html", "Referencias", " Referencias Banco Mundial (2005). Chile: estudio de evaluación de impacto del Programa de Evaluación de Programas. Unidad de Reducción de la Pobreza y Gestión Económica América Latina y el Caribe. Banco Mundial (2010). La formulación de políticas en la OCDE: Ideas para América Latina. Banco Mundial LAC. Bonnefoy, J.C., &amp; Armijo, M. (2005). Indicadores de desempeño en el sector público. CEPAL, ILPES. CEP (2017). Un Estado para la ciudadanía. Informe de la Comisión de Modernización del Estado. Centro de Estudios Públicos. CNEP (2023). Actualización y reenfoque al sistema de evaluación de programas públicos: una propuesta para un Estado más moderno y efectivo. Comisión Nacional de Evaluación y Productividad. DIPRES (2015). Evaluación Ex-Post: Conceptos y Metodologías. Ministerio de Hacienda. García Moreno, M., &amp; García López, R. (2014). La gestión para resultados en el desarrollo: avances y desafíos en América Latina y el Caribe. Banco Interamericano de Desarrollo. Irarrázaval, I., Larrañaga, O., Rodríguez, J., &amp; Valdés, R. (2020). Propuestas para una mejor calidad del gasto y las políticas públicas en Chile. Centro de Políticas Públicas, Pontificia Universidad Católica de Chile. Nagel, S.S. (2002). Handbook of public policy evaluation. Sage Publications. Ortegón, E., Pacheco, J.F., &amp; Prieto, A. (2005). Metodología del marco lógico para la planificación, el seguimiento y la evaluación de proyectos y programas. CEPAL. Pérez, G., &amp; Maldonado, C. (Eds.) (2015). Panorama de los sistemas nacionales de monitoreo y evaluación en América Latina. CIDE/Clear LAC. Pignatta, M.A. (2015). Monitoreo y evaluación de políticas públicas en América Latina: Brechas por cerrar. Perspectivas de Políticas Públicas. Zapata, G., &amp; Tejeda, I. (2009). Impactos del aseguramiento de la calidad y acreditación de la educación superior. Consideraciones y proposiciones. Calidad en la Educación. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
