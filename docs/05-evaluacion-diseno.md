# (PART) Herramientas de Diagnóstico y Diseño {-}

# Evaluación de diseño de políticas y programas

## Concepto y propósito de la evaluación de diseño

La evaluación de diseño constituye una valoración sistemática de la arquitectura conceptual y operacional de una intervención, realizada típicamente en fase ex-ante —previa a implementación completa— aunque también aplicable a programas en operación para identificar debilidades estructurales que explican desempeño deficiente. Como establece DIPRES (2015), su propósito es determinar si el programa cuenta con fundamentos técnicos sólidos que hagan razonable esperar que, bajo implementación apropiada, logre sus objetivos.

A diferencia de evaluaciones de resultados o impacto que interrogan desempeño observado empíricamente, la evaluación de diseño opera mediante análisis lógico de coherencia, consistencia y viabilidad. No pregunta "¿funcionó el programa?" sino "¿debería funcionar si se implementa adecuadamente?". Esta distinción resulta crucial: programas con diseños defectuosos fracasarán independientemente de calidad de gestión, mientras programas bien diseñados pueden fracasar por deficiencias de implementación.

Ortegón, Pacheco y Prieto (2005) argumentan que evaluaciones ex-ante rigurosas previenen desperdicio de recursos en intervenciones cuyo fracaso era predecible desde el diseño. En contextos de restricción fiscal, identificar tempranamente programas con fundamentos técnicos débiles —antes de invertir recursos sustantivos en implementación— constituye contribución mayor a eficiencia del gasto público que evaluar ex-post programas ya ejecutados.

La evaluación de diseño cumple múltiples funciones. Para tomadores de decisión, proporciona evidencia técnica para decidir si proceder con implementación, modificar el diseño o abandonar la iniciativa. Para gestores, identifica ajustes necesarios antes que deficiencias se materialicen en fracasos operacionales. Para beneficiarios potenciales, resguarda que intervenciones efectivamente respondan a sus necesidades. Para el sistema público, fortalece cultura de planificación basada en evidencia versus improvisación.

## Componentes esenciales de la evaluación de diseño

### 1. Diagnóstico del problema público

El primer componente examina si el problema que motiva la intervención está correctamente identificado, caracterizado y cuantificado. Un diagnóstico robusto especifica: (a) manifestaciones observables del problema, (b) población afectada con estimaciones cuantitativas, (c) distribución territorial y temporal, (d) causas fundamentales versus síntomas, (e) tendencias históricas y proyecciones, (f) experiencias previas de abordaje.

Errores comunes incluyen confundir síntomas con problemas (tratar "bajo rendimiento escolar" sin identificar sus causas), definir problemas como ausencia de soluciones ("falta de programa X"), diagnósticos sin respaldo empírico que reproducen supuestos no contrastados, y sub-estimación o sobre-estimación de magnitud que genera programas desproporcionados.

Un buen diagnóstico debe ser contrastable: afirmaciones sobre magnitud, distribución y causas del problema deben respaldar en datos verificables de fuentes confiables. El estándar chileno privilegia uso de encuestas nacionales (CASEN, ENE, CENSO), registros administrativos sectoriales (MINSAL, MINEDUC, Superintendencias) y estudios académicos publicados, evitando diagnósticos basados exclusivamente en percepciones de expertos o narrativas políticas sin sustento empírico.

### 2. Pertinencia de objetivos

Este componente evalúa si los objetivos del programa responden efectivamente al problema diagnosticado y se alinean con políticas superiores. Pertinencia implica correspondencia lógica: objetivos deben apuntar a resolver el problema o sus causas fundamentales, no a síntomas secundarios. Un programa cuyo diagnóstico identifica "deserción escolar por embarazo adolescente y trabajo infantil" pero cuyos objetivos se limitan a "mejorar infraestructura de establecimientos" sufre de incoherencia problema-objetivos.

Adicionalmente, objetivos deben alinearse con políticas sectoriales, compromisos internacionales y prioridades gubernamentales. Un programa de primera infancia que ignore compromisos del Subsistema Chile Crece Contigo o estándares de la Convención de Derechos del Niño enfrenta cuestionamientos de coherencia sistémica.

Los objetivos deben ser SMART: específicos (no ambiguos), medibles (verificables), alcanzables (realistas dado recursos y horizonte temporal), relevantes (importantes para resolver el problema) y temporizados (con plazos definidos). Objetivos vagos como "mejorar la calidad de vida" sin especificar dimensiones, magnitud y plazo resultan técnicamente deficientes.

### 3. Población objetivo y focalización

La evaluación de diseño interroga si la población objetivo está correctamente definida, cuantificada y es alcanzable. Definición precisa especifica criterios de elegibilidad verificables objetivamente. Cuantificación estima tamaño de población elegible mediante fuentes estadísticas confiables. Alcanzabilidad considera si el programa cuenta con mecanismos efectivos para identificar y contactar a potenciales beneficiarios.

Problemas frecuentes incluyen: (a) definiciones difusas que generan ambigüedad sobre quién califica ("familias vulnerables" sin especificar umbral operacional), (b) estimaciones de población objetivo desactualizadas o metodológicamente débiles, (c) focalización que excluye incorrectamente a población elegible o incluye a no-elegibles, (d) ausencia de mecanismos de identificación cuando población objetivo es "invisible" para sistemas administrativos.

El caso chileno muestra heterogeneidad. Programas que utilizan el Registro Social de Hogares para focalización cuentan con identificación robusta de población vulnerable. Programas dirigidos a poblaciones informales, migrantes irregulares o grupos con barreras de acceso enfrentan desafíos mayores de alcanzabilidad que frecuentemente no se resuelven en el diseño.

### 4. Teoría de cambio y cadena causal

El componente central de evaluación de diseño examina la teoría de cambio subyacente: el modelo causal que postula cómo las actividades del programa generarán productos, los productos inducirán resultados en beneficiarios, y los resultados contribuirán a impactos de largo plazo. Como establece la metodología CEPAL, esta cadena debe ser explícita, plausible y estar respaldada por evidencia o literatura especializada.

Una teoría de cambio robusta especifica: (a) la secuencia lógica actividades → productos → resultados → impactos, (b) los mecanismos causales que vinculan cada eslabón, (c) los supuestos críticos que deben cumplirse para que la cadena opere, (d) los horizontes temporales en que se esperan efectos, (e) las condiciones contextuales que moderan efectividad.

Por ejemplo, un programa de capacitación laboral podría postular: capacitaciones (actividad) → competencias técnicas adquiridas (producto) → mayor empleabilidad (resultado) → salida de pobreza (impacto). Esta cadena asume: (a) que las competencias enseñadas corresponden a demandas del mercado laboral local, (b) que beneficiarios pueden aplicar competencias sin barreras adicionales (cuidado infantil, transporte, discriminación), (c) que existe demanda laboral suficiente, (d) que empleos accesibles generan ingresos por sobre línea de pobreza.

La evaluación de diseño interroga cada eslabón y supuesto. ¿Las actividades efectivamente generan los productos previstos? ¿Existe evidencia de que esos productos inducen los resultados esperados? ¿Los supuestos son plausibles dado el contexto? Teorías de cambio con eslabones débiles o supuestos implausibles predicen fracaso incluso con implementación impecable.

### 5. Componentes e intervenciones

Este análisis examina si las prestaciones, servicios o transferencias que el programa proveerá son suficientes, coherentes y complementarias para lograr objetivos. Suficiencia implica que componentes cubren las causas relevantes del problema. Coherencia significa que componentes no se contradicen entre sí. Complementariedad indica que componentes se refuerzan mutuamente.

Un programa para reducir deserción escolar podría incluir: (a) becas monetarias, (b) apoyo psicosocial, (c) nivelación académica, (d) orientación vocacional. Evaluación de diseño pregunta: ¿Estos componentes abordan las causas identificadas de deserción? ¿Alguna causa importante queda sin intervención? ¿Los componentes se refuerzan o generan tensiones? ¿La intensidad de cada componente es apropiada?

Riesgos incluyen sobrediseño (componentes excesivos que exceden capacidad de gestión o presupuesto), subdiseño (intervención insuficiente que no modificará factores causales), incoherencia (componentes que operan en direcciones contradictorias) y desbalance (sobre-inversión en algunos componentes, sub-inversión en otros críticos).

### 6. Recursos y factibilidad operacional

El componente final evalúa si recursos financieros, humanos, tecnológicos e institucionales son apropiados a los objetivos y componentes. Esto incluye: (a) suficiencia presupuestaria considerando costos unitarios realistas, (b) disponibilidad de personal calificado, (c) infraestructura y sistemas requeridos, (d) capacidades institucionales del ejecutor.

Evaluación de diseño debe contrastar presupuesto propuesto con benchmarks de programas similares, verificar factibilidad de costos unitarios proyectados, identificar brechas de capacidades institucionales y evaluar si horizonte de implementación es realista. Programas ambiciosos con presupuestos insuficientes, plazos irrealistas o ejecutores sin capacidades demostradas enfrentan alta probabilidad de fracaso operacional.

## Metodología de evaluación de diseño

La evaluación de diseño combina múltiples métodos analíticos. **Análisis documental** examina documentos de diseño del programa (fundamentación, marcos lógicos, presupuestos), políticas sectoriales, estadísticas oficiales sobre el problema, evaluaciones de programas similares y literatura especializada sobre efectividad de intervenciones análogas.

**Entrevistas con stakeholders clave** —gestores del programa, autoridades sectoriales, expertos temáticos, representantes de población objetivo— proporcionan perspectivas complementarias sobre viabilidad, pertinencia y potenciales obstáculos. Estas entrevistas no reemplazan análisis técnico sino que lo enriquecen con conocimiento experiencial.

**Benchmarking internacional** compara el diseño propuesto con programas similares implementados en otros contextos, identificando mejores prácticas, errores frecuentes y factores críticos de éxito. Chile cuenta con ventaja comparativa en acceso a experiencias regionales documentadas por organismos como BID, CEPAL y Banco Mundial.

**Análisis de consistencia lógica** verifica coherencia interna del diseño mediante herramientas como la Matriz de Marco Lógico y el árbol de problemas. Este análisis identifica inconsistencias, eslabones causales débiles, supuestos implausibles y brechas de diseño.

**Evaluación de viabilidad** examina factibilidad política (¿existe respaldo político sostenido?), técnica (¿capacidades institucionales son suficientes?), administrativa (¿procedimientos son operacionalizables?) y financiera (¿recursos son realistas y sostenibles?).

## Criterios de juicio en evaluación de diseño

La evaluación de diseño emplea criterios específicos para emitir juicios fundamentados:

**Pertinencia**: ¿Los objetivos corresponden al problema y necesidades de beneficiarios? ¿Se alinean con políticas superiores?

**Coherencia interna**: ¿Existe lógica causal entre problema, objetivos, componentes y recursos? ¿Los componentes son complementarios?

**Coherencia externa**: ¿El programa se articula apropiadamente con otras intervenciones? ¿Evita duplicaciones o contradicciones?

**Viabilidad política**: ¿Existe respaldo político sostenido? ¿Los stakeholders relevantes apoyan el diseño?

**Viabilidad técnica**: ¿Las intervenciones propuestas tienen respaldo en evidencia? ¿Son técnicamente factibles?

**Viabilidad administrativa**: ¿El ejecutor cuenta con capacidades? ¿Los procedimientos son operacionalizables?

**Suficiencia de recursos**: ¿El presupuesto es apropiado? ¿Los plazos son realistas?

## Caso aplicado: Evaluación de diseño del Programa de Acompañamiento y Acceso Efectivo (PACE)

El PACE, implementado desde 2014, busca restituir derecho a educación superior de estudiantes talentosos de establecimientos vulnerables mediante preparación académica y cupos garantizados en universidades. Su evaluación de diseño ex-ante habría examinado:

**Diagnóstico**: Datos DEMRE mostraban que estudiantes de liceos vulnerables accedían marginalmente a educación superior incluso con rendimiento académico similar a pares de colegios privilegiados. Causas: menor preparación PSU, auto-exclusión por percepciones de no pertenencia, restricciones financieras.

**Pertinencia de objetivos**: Objetivo de incrementar acceso y retención de estudiantes vulnerables en educación superior responde al problema diagnosticado y se alinea con compromisos de equidad educacional del gobierno.

**Población objetivo**: Estudiantes del 15% de liceos de mayor vulnerabilidad que cumplan requisitos de participación. Cuantificación basada en matrículas administrativas (~ 25,000 estudiantes/cohorte). Alcanzabilidad mediante selección de establecimientos desde bases MINEDUC.

**Teoría de cambio**: Preparación académica + cupo garantizado → competencias académicas + eliminación de barrera de acceso → ingreso y permanencia en educación superior → movilidad social. Supuestos: (a) preparación académica efectivamente desarrolla competencias, (b) cupo garantizado sin PSU no compromete desempeño universitario, (c) estudiantes cuentan con recursos para mantenerse una vez matriculados.

**Componentes**: (a) Preparación académica desde 3° medio (talleres, tutorías), (b) apoyo psicoeducativo, (c) cupo garantizado en universidad adscrita, (d) acompañamiento en educación superior. Evaluación preguntaría: ¿Estos componentes abordan causas de inequidad de acceso? ¿Alguna causa relevante queda sin intervención? (Por ejemplo: restricciones financieras de mantención)

**Recursos y viabilidad**: Presupuesto por estudiante, capacidades de liceos para implementar preparación, compromisos de universidades de sostener cupos, sistemas de seguimiento de trayectorias.

Una evaluación de diseño robusta habría identificado fortalezas (articulación preparación-acceso-acompañamiento, compromiso institucional de universidades) y debilidades (dependencia crítica de calidad de implementación en liceos, riesgo de deserción por restricciones financieras no cubiertas, complejidad de coordinación con múltiples universidades).

## Limitaciones y complementariedades

La evaluación de diseño tiene limitaciones inherentes. Opera con supuestos y proyecciones, no con evidencia empírica de implementación efectiva. Un diseño técnicamente impecable puede fracasar por factores contextuales impredecibles, resistencias organizacionales o cambios en condiciones externas. Inversamente, programas con diseños deficientes ocasionalmente logran resultados por factores no anticipados.

Estas limitaciones demandan complementariedad con evaluaciones de procesos (que examinan implementación efectiva), resultados (que miden logros observados) e impacto (que estiman efectos causales). El ciclo evaluativo completo requiere múltiples tipos de evaluación en diferentes momentos, no sustitución de evaluación ex-post por ex-ante.

No obstante, en contextos de recursos limitados, evaluaciones de diseño bien conducidas previenen errores costosos y orientan inversiones hacia intervenciones con fundamentos sólidos. Como argumenta CNEP (2023), el sistema chileno ha privilegiado históricamente evaluación ex-post sobre ex-ante, generando aprendizajes tardíos cuando recursos ya se han comprometido. Fortalecer evaluación de diseño constituye prioridad para mejora del sistema.

## Preguntas de reflexión

1. Seleccione un programa social chileno reciente. Evalúe la coherencia entre su diagnóstico del problema y los objetivos propuestos. ¿Los objetivos responden a las causas fundamentales o solo a síntomas?

2. Considere el programa Ingreso Ético Familiar. Construya su teoría de cambio identificando actividades, productos, resultados e impactos esperados. ¿Qué supuestos críticos debe cumplirse para que la cadena causal opere?

3. ¿Por qué una intervención puede tener un diseño técnicamente sólido pero fracasar en implementación? Identifique 3 factores que podrían explicar esta brecha entre diseño e implementación.

4. Compare las ventajas y limitaciones de invertir recursos en evaluación ex-ante (de diseño) versus evaluación ex-post (de impacto). ¿En qué contextos priorizaría cada una?

## Referencias del capítulo

- DIPRES (2015). *Evaluación Ex-Post: Conceptos y Metodologías*. Ministerio de Hacienda.
- Ortegón, E., Pacheco, J.F., & Prieto, A. (2005). *Metodología del marco lógico para la planificación, el seguimiento y la evaluación de proyectos y programas*. CEPAL.
- CNEP (2023). *Actualización y reenfoque al sistema de evaluación de programas públicos*. Comisión Nacional de Evaluación y Productividad.
- Bonnefoy, J.C., & Armijo, M. (2005). *Indicadores de desempeño en el sector público*. CEPAL, ILPES.
